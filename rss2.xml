<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>枫叶冢</title>
    <link>http://example.com/</link>
    
    <atom:link href="http://example.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>只有努力不会辜负你</description>
    <pubDate>Mon, 28 Nov 2022 12:04:39 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>mapreduce</title>
      <link>http://example.com/2022/11/28/11-28/</link>
      <guid>http://example.com/2022/11/28/11-28/</guid>
      <pubDate>Mon, 28 Nov 2022 00:34:31 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;yarn&quot;&gt;&lt;a href=&quot;#yarn&quot; class=&quot;headerlink&quot; title=&quot;yarn&quot;&gt;&lt;/a&gt;yarn&lt;/h1&gt;&lt;h2 id=&quot;架构&quot;&gt;&lt;a href=&quot;#架构&quot; class=&quot;headerlink&quot; title=&quot;架构&quot;&gt;&lt;/a&gt;架构&lt;/h</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h1><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>主从架构 ： resourcemanager(资源的分配) : nodemanager(资源的供给与隔离)</p><h3 id="资源调度"><a href="#资源调度" class="headerlink" title="资源调度"></a>资源调度</h3><p>通过rm把nm的资源分配给我们的 task上</p><h3 id="资源隔离"><a href="#资源隔离" class="headerlink" title="资源隔离"></a>资源隔离</h3><p>nm按照要求给task提供资源，保证提供的资源具有独占性</p><h3 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h3><p>nm指挥分配的资源</p><p>一个task对应一个container ： cpu&#x2F;mem（cpu和内存）</p><p>每个container之间是相互隔离的</p><h2 id="yarn的架构设计"><a href="#yarn的架构设计" class="headerlink" title="yarn的架构设计"></a>yarn的架构设计</h2><p>作业提交的流程：<br>client &#x3D;&gt; 给rm 的apps 发送请求 去运行 jar （app master）</p><p>apps 分配一个container 去运行 app master</p><p>app master 会向apps manager 去注册我的作业</p><p>app master 向resource scheduler 申请资源去运行 我的代码</p><p>nodemanager 会开启资源 container 去运行map task 以及reduce task</p><p>tsak 会向 app master 汇报代码与运行情况 </p><p>当代码运行完成 app master 会给apps 发送请求 ，通知我的作业完成了</p><p>apps manager 收到请求之后会通知你的客户端 ，告诉已经运行完成</p><p>输入阶段 ： map tsak 的个数 &#x3D;》container 的申请个数 redurce task 同理</p><h2 id="面试会问"><a href="#面试会问" class="headerlink" title="面试会问"></a>面试会问</h2><p>yarn的架构设计 &#x3D;&#x3D;mr作业提交流程</p><h2 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h2><p>FIFO ：</p><ul><li>先进先出</li><li>单队列</li></ul><p>Capacity ：容量调度器</p><ul><li>多队列</li><li>先进先出（针对一个队列）</li><li>每个队列之间互不影响</li><li>每个队列之间是事先定好的</li></ul><p>Fair：公平调度器</p><ul><li>多队列</li><li>每个队列之间的每个job是有影响的 不是先进先出</li><li>哪一个job的优先级高就执行哪一个</li><li>如果相同优先级，则是顺序</li></ul><p>主流的中小企业：Capacity</p><p>大公司会用：fair</p><h3 id="默认调度器"><a href="#默认调度器" class="headerlink" title="默认调度器"></a>默认调度器</h3><p>容量调度器</p><p>通过调度器进行其作业调度</p><h2 id="yarn的web界面的简介"><a href="#yarn的web界面的简介" class="headerlink" title="yarn的web界面的简介"></a>yarn的web界面的简介</h2><p>左侧侧边栏 ： 有几个选项卡 ： 分别是 </p><h2 id="yarn资源的调优"><a href="#yarn资源的调优" class="headerlink" title="yarn资源的调优"></a>yarn资源的调优</h2><p>container? </p><p>一定比例的cpu和mem</p><p>刀片服务器的配置 ： 128G 16 core :假设一个机器的配置</p><p>刀片服务器 装完系统 消耗内存 1G </p><p>系统预留 ： 预留 20%左右 包含装完成系统 消耗的1G</p><p>原因 ：给未来部署组件预留空间，防止全部使用 ： 会导致系统夯住 就是卡住 ，oom机制【linux系统】：系统会自己杀死进程当内存不足的时候</p><p>预留空间 ： 128 * 0.2 &#x3D; 26G</p><p>其余空间用于大数据 102G</p><p>hadoop ： </p><ul><li>datanode  进程内存 ： 默认 1G &#x3D;》 生产上 2G</li><li>nodemanager 进程内存 ： ，默认 1G &#x3D;》 生产上 4G</li></ul><p>接下来还有96G全部给我们的yarn资源 ： 96G</p><p>container的资源分配 ： </p><p>内存</p><p>cpu</p><p>相比：cpu更重要一些</p><p>container的内存划分 ：默认是86G</p><p>其最小是 1G（默认）</p><p>最大是 8G（默认） 但是可以设置</p><p>注意 container的内存会自动增加 默认以1G递增</p><p>container cpu ： 是虚拟核 &#x3D;》 考虑初衷是不同节点的cpu性能不同</p><p>比如 ： 一个cpu是另外一个cpu的2倍</p><p>第一机器 ： pcore ：vcore &#x3D; 1：2 相当于1个物理核当成两个虚拟核用</p><p>给container的核数 ： 默认是8core</p><p>总数 ： </p><p>最小:1c  (默认)</p><p>最大:4c（默认）</p><p>实际开发角度 ：</p><p>mem</p><p>cpu ： cloudera的公司推荐一个container的core最好不要超过5</p><p>配置core  ：在yarn-site.xml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">cpu： </span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.pcores-vcores-multiplier&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="mapreduce"><a href="#mapreduce" class="headerlink" title="mapreduce"></a>mapreduce</h2><p>map task :mem  : 默认是1024m ，一个map task 申请的资源是1024m ， 但是如果实际使用的资源的内存量超过这个值，你的task会强制被杀死 ，reduce task 也一样</p><p>map task :vcore ： 默认是1</p><p>reduce task :mem : 默认是1024m</p><p>reduce task :vcore : 默认是1</p><p>mr作业是进程级别 &#x3D;》 jvm</p><p>map task</p><p>reduce task </p><p>jvm参数调优 ： </p><p>存储 hdfs</p><p>存储文件</p><p>压缩</p><ul><li>为什么使用压缩？</li><li>节省空间</li><li>节省时间<ul><li>网络io核磁盘io会减少</li><li>指的是mapreduce数据计算过程中</li><li>经过网络传输的数据会变少，</li><li>同样到磁盘上的时候，数据量少也会减少磁盘的io</li></ul></li><li>使用场景<ul><li>采用压缩 ， 对机器的cpu要求比较高</li><li>存储数据的空间不够了，才会用压缩</li></ul></li><li>两面性</li><li>采用压缩的确会让空间和时间减少</li><li>cpu消耗 cpu利用率高 &#x3D;》会导致处理的时间变长一点</li><li>如何使用压缩？</li></ul><p>常见的压缩格式</p><ul><li>gzip</li><li>bzip2</li><li>Lzo</li><li>Snappy</li><li>LZ4</li></ul><p>常见的压缩注意点：</p><p>压缩比 Bzip2 30%  GZIP    snappy、lzo 50%</p><p>解压速度 ： snappy、lzo GZIP   Bzip2</p><p>压缩文件可不可以一被切分 </p><p>假设一个 5G文件 不能被切分 split 意味着 只能使用一个map task去处理</p><p>map task  5G</p><p>假设一个 5G文件 能被切片  splits 10map task 去并行处理</p><p>5*1024 &#x2F;10 &#x3D; 一个map task 处理的数据</p><p>能否被切分 决定了 你的 一个map task处理的数据量有多少</p><p>压缩后的文件是否支持分割？<br>            gzip  不可分割<br>            bzip2  可分割<br>            lzo   带索引的可以分割 (默认是不支持分割的)<br>            snappy 不可分割的</p><p>mapreduce 每个阶段该如何采用这些算法？</p><p>input &#x3D;》 maps &#x3D;》 reduce &#x3D;》 output</p><p>input：<br>    1.Bzip2 ：支持分割 多个map task 进行出</p><p>map out：</p><ul><li>snappy 、lzo</li><li>shuffle 过程 要选择一个解压 速度快的压缩格式</li></ul><p>reduce out ：</p><ul><li>1.高的压缩比 + 支持分片  &#x3D;》 节省空间</li><li>2.bzip2  、lzo带索引的</li></ul><p>reduce out 数据 作为下一个<br>map 的输入咋办？<br>建议使用bzip2【如果采用压缩的话】</p><p>进行压缩配置的两种方式 ： </p><p>job &#x3D;》 code 针对少数job生产生效</p><p>集群所有的job 在配置文件里配置</p><p>对mapreduce 进行配置 </p><ul><li><p>配置压缩的codec</p></li><li><p>map reduce 输出配置</p><ul><li>先打开压缩的开关</li><li>配置codec即可</li></ul></li><li><p>hadoop是不是支持哪些压缩 ，通过命令 ：或者配置文件</p></li><li><p>core-site.xm</p><ul><li>配置支持的压缩有什么</li><li>原生的hadoop默认不支持lzo的算法，因为lzo要把整个hadooop重新编译一遍才可以重新实行</li><li>&#96;&#96;&#96;<property>        <name>io.compression.codecs</name>        <value>org.apache.hadoop.io.compress.BZip2Codec,        org.apache.hadoop.io.compress.SnappyCodec,        org.apache.hadoop.io.compress.GzipCodec,        org.apache.hadoop.io.compress.DefaultCodec        </value></property><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mapred-site.xml:</span><br><span class="line"></span><br><span class="line">* 1.先打开压缩的开关</span><br><span class="line">* 2.map reduce 输出 压缩算法</span><br><span class="line"></span><br><span class="line">reduce： 开关</span><br><span class="line"></span><br><span class="line">* mapreduce.output.fileoutputformat.compress</span><br><span class="line">* ```</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">          &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;</span><br><span class="line">          &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>&#96;&#96;&#96;</p><property>        <name>mapreduce.output.fileoutputformat.compress.codec</name>        <value>org.apache.hadoop.io.compress.BZip2Codec</value></property><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">各个datanode数据节点的平衡</span><br><span class="line"></span><br><span class="line">* DN1 存储空间 90%</span><br><span class="line">* DN2 存储空间 60%</span><br><span class="line">* DN3 存储空间 80%</span><br><span class="line"></span><br><span class="line">如何做呢？</span><br><span class="line"></span><br><span class="line">* sbin/start-balancer.sh</span><br><span class="line"></span><br><span class="line">parameters = Balancer.BalancerParameters</span><br><span class="line"></span><br><span class="line">[BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5</span><br><span class="line"></span><br><span class="line">数据平衡的默认阈值：threshold = 10.0</span><br><span class="line"></span><br><span class="line">每个节点的磁盘使用率 - 平均的磁盘使用率 &lt;10%</span><br><span class="line"></span><br><span class="line">DN1 存储空间 90%  -76% = 14% 说明这个节点数据多 往别的节点迁移数据 出</span><br><span class="line">DN2 存储空间 60%  -76% = -12% 说明这个节点数据少 别的节点迁移数据 进</span><br><span class="line">DN3 存储空间 80%  -76% = 4% 说明这个节点数据多 往别的节点迁移数据</span><br><span class="line"></span><br><span class="line">avg=90 + 80 +60 /3 = 76%</span><br><span class="line"></span><br><span class="line">生产上 从现在开始 start-balancer.sh -threshold 10  每天要定时做的</span><br><span class="line"></span><br><span class="line">放到业务低谷期去做 数据平衡操作</span><br><span class="line"></span><br><span class="line">注意： 不要在业务高峰期做</span><br><span class="line"></span><br><span class="line">1.数据平衡 数据传输  带宽有关</span><br><span class="line"></span><br><span class="line">调优参数 ：平衡的网络带宽  w</span><br><span class="line"></span><br><span class="line">dfs.datanode.balance.bandwidthPerSec 100m 【2.x 默认是10m】</span><br><span class="line"></span><br><span class="line">每个节点数据几十T  需要数据平衡的数据 几十T  可以申请维护窗口时间 ：</span><br><span class="line"></span><br><span class="line">dfs.datanode.balance.bandwidthPerSec 临时调大 200M</span><br><span class="line"></span><br><span class="line">单个DN节点 多块磁盘的数据平衡</span><br><span class="line"></span><br><span class="line">投产前规划：</span><br><span class="line">DN 机器   10块 2T 【不做raid】  =》 20T   副本的</span><br><span class="line"></span><br><span class="line">1.dn 配置多个磁盘</span><br><span class="line"></span><br></pre></td></tr></table></figure><property>      <name>dfs.datanode.data.dir</name>      <value>/data01,/data02,/data03</value></property></li></ul><pre><code>2.为什么要使用多块物理磁盘？1.存储2.因为多个磁盘的io也是叠加的每块磁盘 磁盘io 每秒 100m三块磁盘 1s 能 300m文件内容一块磁盘 1s 100m3.可以让服务一直运行，加入一个磁盘挂了，服务不会减少做多个磁盘数据均衡    dfs.disk.balancer.enabled  true 【3.x有这个功能 cdh 2.x 也有】 apache 2.x 没有这个功能得通过命令去解决磁盘数据均衡？hdfs diskbalancer1.步骤    hdfs diskbalancer -plan  bigdata32  =&gt; 生成一个  bigdata32.plan.json 文件        hdfs diskbalancer -execute bigdata32.plan.json =》 执行disk 数据均衡计划文件        hdfs diskbalancer -query bigdata32生产上 当你发现 磁盘写入不均衡 可以做一下 【一般 一个月 半个月 做一次即可】</code></pre>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/11/28/11-28/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>mapreduce</title>
      <link>http://example.com/2022/11/24/11-24/</link>
      <guid>http://example.com/2022/11/24/11-24/</guid>
      <pubDate>Thu, 24 Nov 2022 00:22:22 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;typeScript&quot;&gt;&lt;a href=&quot;#typeScript&quot; class=&quot;headerlink&quot; title=&quot;typeScript&quot;&gt;&lt;/a&gt;typeScript&lt;/h1&gt;&lt;h2 id=&quot;简介：&quot;&gt;&lt;a href=&quot;#简介：&quot; class=&quot;header</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="typeScript"><a href="#typeScript" class="headerlink" title="typeScript"></a>typeScript</h1><h2 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h2><p>ts：typescript ： 简单来说就是js的超集，可以向下兼容js，所以我们使用ts不用对js进行更改，ts会自动转换成js</p><p>因为现在的浏览器不支持ts的语法 ： ts的编译环境是node.js</p><p>安装运行 ： npm install -g typescript</p><p>安装之后用tsv -v 查看</p><p>与逆行的时候下载一个json</p><p>命令行 : tsc -init</p><p>然后运行命令 ： ts-node .&#x2F;文件名</p><p>下面是关于ts的简单介绍</p><p>首先ts是静态类型，js是动态类型</p><h3 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h3><p>静态类型是编译阶段就会发现问题，而动态类型则是直到运行阶段才会问问题的</p><p>所以相比较而言，静态类型，比动态类型更加可控</p><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><p>ts的类型 ：</p><p>包括 : 布尔 ， 数值，字符串， null , symbol , BigInt,undefined</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> isDone :<span class="built_in">boolean</span> = <span class="literal">false</span></span><br><span class="line"><span class="keyword">let</span> <span class="attr">s</span>: <span class="built_in">number</span> = <span class="number">6</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">a</span>: <span class="built_in">number</span> = <span class="title class_">NaN</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">b</span>: <span class="built_in">number</span> = <span class="title class_">Infinity</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">my</span>: <span class="built_in">string</span> = <span class="string">&#x27;yon&#x27;</span></span><br><span class="line"><span class="keyword">let</span> o : <span class="literal">undefined</span></span><br><span class="line"><span class="keyword">let</span> <span class="attr">nme</span>: <span class="literal">null</span>;</span><br><span class="line"><span class="keyword">function</span> <span class="title function_">alertName</span>(<span class="params"></span>) : <span class="built_in">void</span> &#123;</span><br><span class="line">    <span class="title function_">alert</span>(<span class="string">&#x27;my first name&#x27;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">arr</span>: <span class="built_in">any</span>[] = [<span class="string">&quot;hello&quot;</span> , <span class="number">1</span> , <span class="literal">true</span>]</span><br><span class="line"><span class="keyword">let</span> <span class="attr">ids</span>:<span class="built_in">number</span>[] = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>];</span><br><span class="line"><span class="keyword">let</span> <span class="attr">names</span>: <span class="title class_">String</span>[] = [<span class="string">&quot;xxx&quot;</span> , <span class="string">&quot;yyy&quot;</span> , <span class="string">&quot;zzzz&quot;</span>];</span><br><span class="line"><span class="keyword">let</span> <span class="attr">book</span>: <span class="title class_">Object</span>[] = [&#123;<span class="attr">name1</span>:<span class="string">&#x27;tom&#x27;</span> , <span class="attr">animal</span>:<span class="string">&#x27;cont&#x27;</span>&#125;,&#123;<span class="attr">name1</span>:<span class="string">&#x27;gggg&#x27;</span>,<span class="attr">animal</span>:<span class="string">&#x27;hhhh&#x27;</span>&#125;];</span><br><span class="line">ids.<span class="title function_">push</span>(<span class="number">6</span>);</span><br><span class="line"><span class="keyword">let</span> <span class="attr">person</span>:(<span class="built_in">string</span>|<span class="built_in">number</span>|<span class="built_in">boolean</span>)[]=[<span class="string">&#x27;aslkdjla&#x27;</span>,<span class="number">1</span>,<span class="literal">true</span>]; <span class="comment">// 这个是三个值</span></span><br><span class="line"><span class="keyword">let</span> <span class="attr">person1</span>:[<span class="built_in">string</span>,<span class="built_in">number</span>,<span class="built_in">boolean</span>]=[<span class="string">&#x27;sdfjkl&#x27;</span>,<span class="number">1</span>,<span class="literal">true</span>]; <span class="comment">// 这个是一个值</span></span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">info</span>(isDone)<span class="keyword">let</span> isDone :<span class="built_in">boolean</span> = <span class="literal">false</span></span><br><span class="line"><span class="keyword">let</span> <span class="attr">s</span>: <span class="built_in">number</span> = <span class="number">6</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">a</span>: <span class="built_in">number</span> = <span class="title class_">NaN</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">b</span>: <span class="built_in">number</span> = <span class="title class_">Infinity</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">my</span>: <span class="built_in">string</span> = <span class="string">&#x27;yon&#x27;</span></span><br><span class="line"><span class="keyword">let</span> o : <span class="literal">undefined</span></span><br><span class="line"><span class="keyword">let</span> <span class="attr">nme</span>: <span class="literal">null</span>;</span><br><span class="line"><span class="keyword">function</span> <span class="title function_">alertName</span>(<span class="params"></span>) : <span class="built_in">void</span> &#123;</span><br><span class="line">    <span class="title function_">alert</span>(<span class="string">&#x27;my first name&#x27;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">info</span>(isDone)</span><br></pre></td></tr></table></figure><h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2><p>我们定义接口的方式如下：</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">interface</span> a&#123;</span><br><span class="line">    <span class="attr">name</span>:<span class="built_in">string</span>;</span><br><span class="line">    <span class="attr">age</span>:<span class="built_in">number</span>;</span><br><span class="line">    <span class="attr">isP</span>:<span class="built_in">boolean</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>和上述数组的作用一样，不过这样定义可以增强复用性，随便让一个东西继承一下他，就可以了</p><p>比如：</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">interface</span> a&#123;</span><br><span class="line">    <span class="attr">name</span>:<span class="built_in">string</span>;</span><br><span class="line">    <span class="attr">age</span>:<span class="built_in">number</span>;</span><br><span class="line">    <span class="attr">isP</span>:<span class="built_in">boolean</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="attr">p</span>:a=&#123;</span><br><span class="line">    <span class="attr">name</span>:<span class="string">&#x27;kjds&#x27;</span>,</span><br><span class="line">    <span class="attr">age</span>:<span class="number">11</span>,</span><br><span class="line">    <span class="attr">isP</span>:<span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><p>函数合上述的接口很像，他是在接口创建了几个方法，如下</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">interface</span> s&#123;</span><br><span class="line">    <span class="attr">name</span>:<span class="built_in">string</span>;</span><br><span class="line">    <span class="title function_">eat</span>(<span class="attr">name</span>:<span class="built_in">string</span>):<span class="built_in">string</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="attr">j</span>:s=&#123;</span><br><span class="line">    <span class="attr">name</span>:<span class="string">&#x27;lkasdj&#x27;</span>,</span><br><span class="line">    <span class="attr">eat</span>:<span class="keyword">function</span>(<span class="params">name:<span class="built_in">string</span></span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;eat$&#123;name&#125;&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="title function_">add</span> = (<span class="params">a:<span class="built_in">number</span>,b:<span class="built_in">number</span>,c?:<span class="built_in">number</span>|<span class="built_in">string</span></span>)=&gt;&#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(c)</span><br><span class="line">    <span class="keyword">return</span> a+b</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>问号代表从后面的类型里选数据，这里是number和string</p><p>上面的是lambad多</p><p>把ts转换成js的语法是any</p><h2 id="Dom和类型转换"><a href="#Dom和类型转换" class="headerlink" title="Dom和类型转换"></a>Dom和类型转换</h2><p>ts无法像js一样进行访问dom这个就意味着ts无法确定这些dom是不是存在，所以使用非空断言，我们可用明确的告诉他，这个表达式是null&#x2F;undefined</p><p>非空断言 X! 代表把x中的null和undefined给去除了</p><p>例如 ：下面是关于ts和获取dom的</p><p><code>const link = document.querySelector(&#39;a&#39;);</code></p><p>当我们要通过class来选择一个dom元素的时候，可以进行一层转化</p><p>例如 ： <code>const from = document.getElementById(&#39;form&#39;) as HTMLAnchorElement</code></p><h2 id="类"><a href="#类" class="headerlink" title="类"></a>类</h2><p>和java中的类异常类似</p><p>比如 ： </p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Preson</span> &#123; </span><br><span class="line">    <span class="attr">name</span>:<span class="built_in">string</span>;</span><br><span class="line">    <span class="attr">iscool</span>:<span class="built_in">boolean</span>;</span><br><span class="line">    <span class="attr">age</span>:<span class="built_in">number</span>;</span><br><span class="line">    <span class="title function_">constructor</span>(<span class="params">n:<span class="built_in">string</span>,c:<span class="built_in">boolean</span>,a:<span class="built_in">number</span></span>)&#123;</span><br><span class="line">        <span class="variable language_">this</span>.<span class="property">name</span>=n</span><br><span class="line">        <span class="variable language_">this</span>.<span class="property">iscool</span>=c</span><br><span class="line">        <span class="variable language_">this</span>.<span class="property">age</span>=a</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="title function_">sayhai</span>(<span class="params"></span>)&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;hi,wosshi$&#123;this.name&#125;,jinnnan$&#123;this.age&#125;&#x27;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>类的修饰符</p><p>readonly,privatee,protected,public</p><p>简单的介绍就到这里为止，具体以后再慢慢补充</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%9D%82%E8%B4%A7%E6%8A%80%E6%9C%AF%E6%A0%88/">杂货技术栈</category>
      
      
      
      <comments>http://example.com/2022/11/24/11-24/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>mapreduce</title>
      <link>http://example.com/2022/11/21/11-21/</link>
      <guid>http://example.com/2022/11/21/11-21/</guid>
      <pubDate>Mon, 21 Nov 2022 00:47:09 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;续讲Hadoop&quot;&gt;&lt;a href=&quot;#续讲Hadoop&quot; class=&quot;headerlink&quot; title=&quot;续讲Hadoop&quot;&gt;&lt;/a&gt;续讲Hadoop&lt;/h1&gt;&lt;h2 id=&quot;模板模式&quot;&gt;&lt;a href=&quot;#模板模式&quot; class=&quot;headerlink&quot; </description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="续讲Hadoop"><a href="#续讲Hadoop" class="headerlink" title="续讲Hadoop"></a>续讲Hadoop</h1><h2 id="模板模式"><a href="#模板模式" class="headerlink" title="模板模式"></a>模板模式</h2><p>关于mapreduce的操作模式就是模板模式，加上自定义的变量</p><h3 id="模板模式-1"><a href="#模板模式-1" class="headerlink" title="模板模式"></a>模板模式</h3><p>关于模板模式，就是三步走</p><p>开始阶段 ： map阶段</p><p>处理阶段 ： reduce阶段</p><p>结束阶段 ： 关闭流并输出</p><h3 id="自定义的变量"><a href="#自定义的变量" class="headerlink" title="自定义的变量"></a>自定义的变量</h3><p>BooleanWriteable : 布尔类型</p><p>ByteWriteable : Byte类型</p><p>DoubleWriteable : double类型</p><p>FloatWriteable ：float类型</p><p>intWriteable ： int类型</p><p>…（等等）</p><h3 id="mapreduce的核心方法"><a href="#mapreduce的核心方法" class="headerlink" title="mapreduce的核心方法"></a>mapreduce的核心方法</h3><ul><li>input : InputFormat &#x3D;&gt; 如何加载数据</li><li>查看源码发现有DBInputFormat 和 FileInputFormat，代表我们可以从db（数据库中），或者文件中加载数据（上述两个是抽象类，我们实例化的时候其实是实例化其子类）</li><li>简单来说就是读取数据的方法</li><li>默认实例化的时候是实例化 TextInputFormat</li><li>关于TextInputFormat ： 其有两个参数<ul><li>key 和 value<ul><li>key 是 读取文件的位置</li><li>value 是 一行一行的内容<ul><li>在value中有个属性 判断文件可不可以被切分 ： isSpiltable<ul><li>map task 数据 是由 input 的切片数量决定的 ， 当不可被切片的时候对应的数量就是1，对应一个map task</li></ul></li></ul></li></ul></li></ul></li></ul><p>creatRecordReader</p><h2 id="一个文件加载的时候会被切分成几个切片"><a href="#一个文件加载的时候会被切分成几个切片" class="headerlink" title="一个文件加载的时候会被切分成几个切片"></a>一个文件加载的时候会被切分成几个切片</h2><p>前提 ： 文件可以被切分</p><p>当一个文件 在hdfs上的时候是按我们的大小进行切分的 以128m为基础单位</p><p>而我们的map的切片是按照blocksize进行切分的简单来说就是以blocksize的大小进行切分</p><p>总结  ：</p><ul><li>文件大小小于128m<ul><li>那么切分成1片</li></ul></li><li>如果文件大于128m<ul><li>filesize&#x2F;splitesize &#x3D; num 切片数</li><li>filesize剩余的部分 ： 和splitesize的10%比较<ul><li>如果大，则开启一个切片文件</li><li>如果小，则是和上面最后一个合并到一起</li></ul></li></ul></li></ul><p>不能被切分的文件： 某些被压缩文件</p><ul><li>关于这个切片大小的标准 ：<ul><li>在hdfs上是128m</li><li>但是在idea上是32m（在源码里能看）</li></ul></li></ul><h2 id="redurce-task-个数是由上面决定"><a href="#redurce-task-个数是由上面决定" class="headerlink" title="redurce task 个数是由上面决定?"></a>redurce task 个数是由上面决定?</h2><p>默认是 1</p><p>如果要更改要手动更改</p><p>setNumReduceTasks(数量)</p><p>如果 reduce阶段数量变多</p><p>则会把相同的文件拉到一起，就是按照一个规则进行的分区</p><p>默认是走的hashcode ： 这个按照规则来分的就是分区</p><p>一般来说 ： 分区的结果是 suffler 的输出结果是 reduce的输入</p><h3 id="简单需求"><a href="#简单需求" class="headerlink" title="简单需求"></a>简单需求</h3><p>基于phone的存储数据 ，进行分文件存储</p><p>比如</p><p>13开头的存储在一个上</p><p>15开头的在一个文件上</p><p>这就要自定义分区</p><p>通过继承Partitioner类实现方法然后导入方法</p><p>实现getPartition方法进行分区 ： 以下是一个简单的分区函数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">myPartionit</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text , IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text text, IntWritable intWritable, <span class="type">int</span> i)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (text.toString().equals(<span class="string">&quot;ANALYST&quot;</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (text.toString().equals(<span class="string">&quot;CLERK&quot;</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (text.toString().equals(<span class="string">&quot;MANAGER&quot;</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (text.toString().equals(<span class="string">&quot;PRESIDENT&quot;</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">3</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (text.toString().equals(<span class="string">&quot;SALESMAN&quot;</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">4</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">5</span>;</span><br><span class="line">&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后我们要在我们的主程序中调用这个类 ， test类：就是我们的主类，如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> sxwang</span></span><br><span class="line"><span class="comment"> * 11 18 14:00</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">test</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * driver</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (args.length &lt; <span class="number">2</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;出现问题最少两个参数&quot;</span>);</span><br><span class="line">System.exit(<span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line">        String input=args[<span class="number">0</span>];</span><br><span class="line">String output=args[<span class="number">1</span>];</span><br><span class="line"><span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"><span class="comment">//0.todo... 删除目标路径</span></span><br><span class="line">FileUtils.deletePath(conf,output);</span><br><span class="line"></span><br><span class="line"><span class="comment">//1.设置 作业名称</span></span><br><span class="line"><span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">&quot;WCAPP&quot;</span>);</span><br><span class="line"><span class="comment">//2.设置map reduce 执行代码的主类</span></span><br><span class="line">job.setJarByClass(test.class);</span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"><span class="comment">//3.指定 oupput kv类型</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(IntWritable.class);</span><br><span class="line">job.setNumReduceTasks(<span class="number">6</span>);</span><br><span class="line">job.setPartitionerClass(myPartionit.class);</span><br><span class="line"><span class="comment">//4. 设置数据源路径 数据输出路径</span></span><br><span class="line">FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(input));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(output));</span><br><span class="line"><span class="comment">//5. 提交mr yarn</span></span><br><span class="line">System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * mapper</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MyMapper</span></span><br><span class="line">            <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * 1.按照分隔符 进行拆分 每个单词 ，每个单词赋值为1</span></span><br><span class="line"><span class="comment">             * (word ,1)</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line"></span><br><span class="line">String[] words = value.toString().split(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                String[] split = word.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">context.write(<span class="keyword">new</span> <span class="title class_">Text</span>(split[<span class="number">2</span>]) ,<span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * reducer</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MyReducer</span></span><br><span class="line">            <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text,IntWritable,Text,IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *  (word ,1)</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         *  (word,&lt;1,1,1,1&gt;)</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         *  1.聚合value</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         *  2.写出去</span></span><br><span class="line"><span class="comment">*  (word ,3)</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            <span class="type">int</span> sum=<span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">                sum +=Integer.parseInt(value.toString());</span><br><span class="line">&#125;</span><br><span class="line">            context.write(key,<span class="keyword">new</span> <span class="title class_">IntWritable</span>(sum));</span><br><span class="line">&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="sql-vs-mr-mapreduce"><a href="#sql-vs-mr-mapreduce" class="headerlink" title="sql vs mr(mapreduce)"></a>sql vs mr(mapreduce)</h2><ul><li>sql  :<ul><li>group by</li><li>distinct</li><li>join</li><li>order by</li><li>union<ul><li><p>group by 用mr实现</p></li><li><p>就是再map阶段进行处理reduce阶段进行合并</p></li><li><p>distinct</p></li><li></li><li><p>去重 ： sql ：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span>(ename) <span class="keyword">from</span> emp;</span><br><span class="line">#或者可以通过分组进行去重</span><br><span class="line"><span class="keyword">select</span> name <span class="keyword">from</span> <span class="keyword">group</span> <span class="keyword">by</span> name;</span><br></pre></td></tr></table></figure></li><li><p>通过上述我们可知group by 也可以进行去重 ，所以后面我们能用group by 就要用group by</p></li><li><p>因为distinct只有一个task进行处理</p></li><li><p>而group by 则是多个task 进行处理 ， 所以效率会比较高</p></li><li><p>接下来是在mr里实现</p></li><li><p>order by</p></li><li><p>mr :</p></li><li><p>全局排序 ： reduce task 是 1</p></li><li><p>分区排序 ： reduce task 是 多个</p></li><li><p>mr : 实现</p></li><li></li></ul></li></ul></li></ul><h2 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h2><p>如果你的reduce task 大于分区数 ： 会有空白文件</p><p>如果 reduce task 小于分区数 且大于1 ： 则会报错</p><p>如果 reduce task 且分区数 等于 1 ： 则会把东西整合到一个文件</p><h2 id="关于mapreduce在linux上运行的学习，官方提供了源码库"><a href="#关于mapreduce在linux上运行的学习，官方提供了源码库" class="headerlink" title="关于mapreduce在linux上运行的学习，官方提供了源码库"></a>关于mapreduce在linux上运行的学习，官方提供了源码库</h2><p>地址在github上：</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/11/21/11-21/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>把jar包打包成exe</title>
      <link>http://example.com/2022/11/19/jar%E5%88%B0exe/</link>
      <guid>http://example.com/2022/11/19/jar%E5%88%B0exe/</guid>
      <pubDate>Sat, 19 Nov 2022 03:40:21 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;使用工具&quot;&gt;&lt;a href=&quot;#使用工具&quot; class=&quot;headerlink&quot; title=&quot;使用工具&quot;&gt;&lt;/a&gt;使用工具&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;我们所使用的工具是exe4j 和 inno setup&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;exe4j&quot;&gt;&lt;a</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="使用工具"><a href="#使用工具" class="headerlink" title="使用工具"></a>使用工具</h1><ul><li>我们所使用的工具是exe4j 和 inno setup</li></ul><h2 id="exe4j"><a href="#exe4j" class="headerlink" title="exe4j"></a>exe4j</h2><ul><li>简单来说这个就是个工具，可以把java的jar包打包成exe的</li><li>可以让其他由java运行环境的机器运行这个jar</li><li>而且还可以稍微做到保密的程度</li><li>exe4j 是一个帮助你集成 Java 应用程序到 Windows 操作环境的 java 可执行文件生成工具，无论这些应用是用于服务器，还是图形用户界面（GUI）或命令行的应用程序。如果你想在任务管理器中及 Windows XP 分组的用户友好任务栏里以你的进程名取代 java.exe 的出现，那么 exe4j 可以完成这个工作。exe4j 帮助你以一种安全的方式启动你的 java 应用程序，来显示本地启动画面，检测及发布合适的 JRE 和 JDK，以及进行启动时所发生的错误处理等，以至于更多。</li><li>关于exe4j的破解我就不多赘述了，懂的都懂</li><li>接下来是实际操作</li><li>下面是开始界面</li><li><img src="https://pic.imgdb.cn/item/6378540416f2c2beb192c11f.jpg" alt="开始界面"></li><li>然后接下来</li><li>选择jar到exe的模式</li><li><img src="https://pic.imgdb.cn/item/6378546916f2c2beb1932dfa.jpg"></li><li>然后接下来要选择你的输出路径</li><li><img src="https://pic.imgdb.cn/item/637854cd16f2c2beb193c56c.jpg"></li><li>接下来选择我们的启动方式，我选择的是控制台启动</li><li><img src="https://pic.imgdb.cn/item/6378550616f2c2beb1942a62.jpg"></li><li>接下来选择可以在32位和64位机器上都可以运行</li><li><img src="https://pic.imgdb.cn/item/6378553316f2c2beb194a3be.jpg"></li><li><img src="https://pic.imgdb.cn/item/6378556516f2c2beb1952fa9.jpg"></li><li>接下来选择我们exe要执行的主类</li><li>我这个项目是boot项目，所以选择的是这个</li><li><img src="https://pic.imgdb.cn/item/6378558416f2c2beb195719f.jpg"></li><li>然后设置我们的jdk版本</li><li><img src="https://pic.imgdb.cn/item/637855bd16f2c2beb195f1d6.jpg"></li><li><img src="https://pic.imgdb.cn/item/637855dc16f2c2beb1962843.jpg"></li><li>最后一直点击next就好了</li><li><img src="https://pic.imgdb.cn/item/6378564016f2c2beb196ce41.jpg"></li><li>这样我们就打包完成了</li><li>但是这样的打包只能再有jdk的环境中运行</li><li>接下来我们要为这个添加依赖</li></ul><h2 id="inno-setup"><a href="#inno-setup" class="headerlink" title="inno setup"></a>inno setup</h2><ul><li>简单来说就是个绑定依赖的程序</li><li>打开</li><li><img src="https://pic.imgdb.cn/item/6378572316f2c2beb198e2a2.jpg"></li><li><img src="https://pic.imgdb.cn/item/6378577616f2c2beb199aecb.jpg"></li><li><img src="https://pic.imgdb.cn/item/6378579d16f2c2beb19a1b5f.jpg"></li><li>然后配置我们的名称和版本号</li><li>然后选择我们，刚刚弄个出来的exe程序</li><li><img src="https://pic.imgdb.cn/item/637858cd16f2c2beb19b780b.jpg"></li><li>然后就一直傻瓜式next</li><li><img src="https://pic.imgdb.cn/item/6378593316f2c2beb19be141.jpg"></li><li>设置输出文件夹</li><li>图标</li><li>以及运行时的密码</li><li>然后一直next就好</li><li>然后会跳出两个对话框</li><li>全部选择是</li><li>最后会出现这个东西</li><li><img src="https://pic.imgdb.cn/item/637859e916f2c2beb19d21f9.jpg"></li><li>就代表导入依赖了</li><li>接下来我们要对他进行修改</li><li><img src="https://pic.imgdb.cn/item/63785a3d16f2c2beb19d8050.jpg"></li><li>这个是自己电脑本机的jdk路径</li><li>然后还有要在这里加上MYjrename</li><li><img src="https://pic.imgdb.cn/item/63785a8116f2c2beb19dd2fa.jpg"></li><li>最后运行就成功了</li><li>他会给你成一个setup文件，通过这个文件安装的exe</li><li>就会自己带环境了</li><li></li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%9D%82%E8%B4%A7%E6%8A%80%E6%9C%AF%E6%A0%88/">杂货技术栈</category>
      
      
      
      <comments>http://example.com/2022/11/19/jar%E5%88%B0exe/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>内网穿透</title>
      <link>http://example.com/2022/11/19/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/</link>
      <guid>http://example.com/2022/11/19/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/</guid>
      <pubDate>Sat, 19 Nov 2022 03:18:15 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;内网穿透&quot;&gt;&lt;a href=&quot;#内网穿透&quot; class=&quot;headerlink&quot; title=&quot;内网穿透&quot;&gt;&lt;/a&gt;内网穿透&lt;/h1&gt;&lt;h2 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="内网穿透"><a href="#内网穿透" class="headerlink" title="内网穿透"></a>内网穿透</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><ul><li>内网穿透简单来说就是让我们处于局域网的机器或者本地的项目，可以在外部的公网访问</li></ul><h3 id="常见的内网穿透工具"><a href="#常见的内网穿透工具" class="headerlink" title="常见的内网穿透工具"></a>常见的内网穿透工具</h3><ul><li>natapp网站</li><li>frp</li><li>ngrok</li><li>蜻蜓映射</li><li>ssh命令</li></ul><h3 id="natapp"><a href="#natapp" class="headerlink" title="natapp"></a>natapp</h3><ul><li>natapp网站 ： 注册账号之后会免费给你送一条隧道，不过临时用可以，不能长久用，</li><li>因为隧道名字会被强制更换</li><li>而vip隧道就不会强制更换</li></ul><h3 id="frp"><a href="#frp" class="headerlink" title="frp"></a>frp</h3><ul><li>frp是专业的反向代理工具</li><li><code>https:github.com/fatedier/frp</code></li><li>其中部署也非常简单 ： </li><li>frps在云服务器上</li><li>而要进行内网穿透的机器运行frpc</li><li>再改改配置就ok了</li></ul><h3 id="ngrok"><a href="#ngrok" class="headerlink" title="ngrok"></a>ngrok</h3><ul><li>属于专门做内网穿透的平台，以前是免费的不过最近好像收费了</li><li>注册之后我们就可以根据平台上的帮助文档进行配置，</li></ul><h3 id="蜻蜓映射"><a href="#蜻蜓映射" class="headerlink" title="蜻蜓映射"></a>蜻蜓映射</h3><ul><li>同上</li></ul><h3 id="ssh命令"><a href="#ssh命令" class="headerlink" title="ssh命令"></a>ssh命令</h3><ul><li>我们可以通过<code>ssh -R 80:localhost:80 xxxxx@localhost.run</code></li><li>进行反向映射端口</li><li>xxxx是远程云主机</li><li>前面的80是云主机的端口</li><li>后面的是本地的端口</li><li>这个命令是ssh自带的</li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%9D%82%E8%B4%A7%E6%8A%80%E6%9C%AF%E6%A0%88/">杂货技术栈</category>
      
      
      
      <comments>http://example.com/2022/11/19/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>11-19作业</title>
      <link>http://example.com/2022/11/19/%E4%BD%9C%E4%B8%9A11-19/</link>
      <guid>http://example.com/2022/11/19/%E4%BD%9C%E4%B8%9A11-19/</guid>
      <pubDate>Sat, 19 Nov 2022 03:13:45 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;通过mapreduce的api统计emp表里的每个工作的人数&quot;&gt;&lt;a href=&quot;#通过mapreduce的api统计emp表里的每个工作的人数&quot; class=&quot;headerlink&quot; title=&quot;通过mapreduce的api统计emp表里的每个工作的人数&quot;&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="通过mapreduce的api统计emp表里的每个工作的人数"><a href="#通过mapreduce的api统计emp表里的每个工作的人数" class="headerlink" title="通过mapreduce的api统计emp表里的每个工作的人数"></a>通过mapreduce的api统计emp表里的每个工作的人数</h1><ul><li>数据源</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">7369,SMITH,CLERK,7902,1980-12-17 14:00:00,800.00,,20</span><br><span class="line">7499,ALLEN,SALESMAN,7698,1981-02-20 14:00:00,1600.00,300.00,30</span><br><span class="line">7521,WARD,SALESMAN,7698,1981-02-22 14:00:00,1250.00,500.00,30</span><br><span class="line">7566,JONES,MANAGER,7839,1981-04-02 14:00:00,2975.00,,20</span><br><span class="line">7654,MARTIN,SALESMAN,7698,1981-09-28 13:00:00,1250.00,1400.00,30</span><br><span class="line">7698,BLAKE,MANAGER,7839,1981-05-01 13:00:00,2850.00,,30</span><br><span class="line">7782,CLARK,MANAGER,7839,1981-06-09 13:00:00,2450.00,,10</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7839,KING,PRESIDENT,,1981-11-17 14:00:00,5000.00,,10</span><br><span class="line">7844,TURNER,SALESMAN,7698,1981-09-08 13:00:00,1500.00,0.00,30</span><br><span class="line">7876,ADAMS,CLERK,7788,1983-01-12 14:00:00,1100.00,,20</span><br><span class="line">7900,lebulang,CLERK,7698,1981-12-03 14:00:00,950.00,,30</span><br><span class="line">7902,FORD,ANALYST,7566,1981-12-03 14:00:00,3000.00,,20</span><br><span class="line">7934,MILLER,CLERK,7782,1982-01-23 14:00:00,1300.00,,10</span><br><span class="line">7839,KING,PRESIDENT,,1981-11-17 14:00:00,5000.00,,10</span><br><span class="line">7654,MARTIN,SALESMAN,7698,1981-09-28 13:00:00,3200.00,1400.00,30</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><p>其中第二列就是工作</p></li><li><p>代码如下 ：</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">package org.example;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @author sxwang</span><br><span class="line"> * 11 18 14:00</span><br><span class="line"> */</span><br><span class="line">public class test &#123;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * driver</span><br><span class="line">     * @param args</span><br><span class="line">     */</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        String input=&quot;D://emp.txt&quot;;</span><br><span class="line">        String output=&quot;out&quot;;</span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        //0.todo... 删除目标路径</span><br><span class="line">        FileUtils.deletePath(conf,output);</span><br><span class="line"></span><br><span class="line">        //1.设置 作业名称</span><br><span class="line">        Job job = Job.getInstance(conf, &quot;WCAPP&quot;);</span><br><span class="line">        //2.设置map reduce 执行代码的主类</span><br><span class="line">        job.setJarByClass(test.class);</span><br><span class="line">        job.setMapperClass(MyMapper.class);</span><br><span class="line">        job.setReducerClass(MyReducer.class);</span><br><span class="line">        //3.指定 oupput kv类型</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        //4. 设置数据源路径 数据输出路径</span><br><span class="line">        FileInputFormat.addInputPath(job, new Path(input));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(output));</span><br><span class="line">        //5. 提交mr yarn</span><br><span class="line">        System.exit(job.waitForCompletion(true) ? 0 : 1);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * mapper</span><br><span class="line">     */</span><br><span class="line">    public static class MyMapper</span><br><span class="line">            extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        protected void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            /**</span><br><span class="line">             * 1.按照分隔符 进行拆分 每个单词 ，每个单词赋值为1</span><br><span class="line">             * (word ,1)</span><br><span class="line">             */</span><br><span class="line"></span><br><span class="line">            String[] words = value.toString().split(&quot;\n&quot;);</span><br><span class="line"></span><br><span class="line">            for (String word : words) &#123;</span><br><span class="line">                String[] split = word.split(&quot;,&quot;);</span><br><span class="line">                context.write(new Text(split[2]) ,new IntWritable(1));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * reducer</span><br><span class="line">     */</span><br><span class="line">    public static class MyReducer</span><br><span class="line">            extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">        /**</span><br><span class="line">         *  (word ,1)</span><br><span class="line">         *</span><br><span class="line">         *  (word,&lt;1,1,1,1&gt;)</span><br><span class="line">         *</span><br><span class="line">         *  1.聚合value</span><br><span class="line">         *</span><br><span class="line">         *  2.写出去</span><br><span class="line">         *  (word ,3)</span><br><span class="line">         */</span><br><span class="line">        @Override</span><br><span class="line">        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            int sum=0;</span><br><span class="line">            for (IntWritable value : values) &#123;</span><br><span class="line">                sum +=Integer.parseInt(value.toString());</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key,new IntWritable(sum));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>基本概念 ： 把数据先通过map进行etl，然后通过redurce进行数据的整合之类的</li><li>最后输出</li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E4%BD%9C%E4%B8%9A/">作业</category>
      
      
      
      <comments>http://example.com/2022/11/19/%E4%BD%9C%E4%B8%9A11-19/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>踩过的坑</title>
      <link>http://example.com/2022/11/17/%E5%9D%91/</link>
      <guid>http://example.com/2022/11/17/%E5%9D%91/</guid>
      <pubDate>Thu, 17 Nov 2022 11:47:34 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;修改windows下的hosts文件不生效&quot;&gt;&lt;a href=&quot;#修改windows下的hosts文件不生效&quot; class=&quot;headerlink&quot; title=&quot;修改windows下的hosts文件不生效&quot;&gt;&lt;/a&gt;修改windows下的hosts文件不生效&lt;/</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="修改windows下的hosts文件不生效"><a href="#修改windows下的hosts文件不生效" class="headerlink" title="修改windows下的hosts文件不生效"></a>修改windows下的hosts文件不生效</h1><ul><li>这个坑一直卡了我三四天</li><li>配置完hadoop集群，进行web访问的时候出现了这个问题</li><li>因为windows的主机映射不好使，所以我无法在网页端查看数据</li></ul><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><ul><li>我这里解决这个的办法是通过</li><li>把中间空格放大的方法</li><li>就是一个小空格加一个tab键</li><li>然后进入 <code>edge://net-internals</code>或者 <code>chrome://net-internals</code></li><li>这取决于你的浏览器</li><li>然后 <code>win + x</code> 查看windows powershell（管理员）打开之后执行 <code> ipconfig /flushdns</code></li><li>刷新之后就可以通过映射访问了</li></ul><h1 id="有关于idea关于Hadoop的api的copyToLocalfile不生效"><a href="#有关于idea关于Hadoop的api的copyToLocalfile不生效" class="headerlink" title="有关于idea关于Hadoop的api的copyToLocalfile不生效"></a>有关于idea关于Hadoop的api的copyToLocalfile不生效</h1><ul><li>报错信息 <code>java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.</code></li><li>这个是代表你windows里没有配置hadoop环境</li><li>我们可以不用他提供的api</li><li>通过</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">FSDataInputStream fis = fs.open(new Path(&quot;/date&quot;)); // hdfs上的文件</span><br><span class="line">OutputStream outputStream = new FileOutputStream( new File(&quot;D:\\ bg1.txt&quot;) , false); // 本地的存储地点</span><br><span class="line">IOUtils.copyBytes(fis,outputStream,4096 , true);</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>就可以实现下载</li></ul><h1 id="关于linux里的mysql会自动识别大小写表名的事情"><a href="#关于linux里的mysql会自动识别大小写表名的事情" class="headerlink" title="关于linux里的mysql会自动识别大小写表名的事情"></a>关于linux里的mysql会自动识别大小写表名的事情</h1><p>linux里的mysql会自动对表名进行大小写区分，这可能导致一些boot项目启动的时候找不到表名</p><p>如何更改如下</p><p>首先查看自己的表名大小写权限是不是打开了</p><p>先在mysql里执行</p><p><code>show variables like &quot;%case%&quot;;</code></p><p>查看一下我们的权限打没打开</p><p>一般默认的时候会有个off的选项</p><p>这个是默认的情况</p><p>接下来我们对其进行修改</p><p>我们首先退出mysql</p><p>然后关闭mysql服务输入 ： <code>mysqladmin -uroot -p shutdown //以安全模式关闭数据库</code></p><p>接下来输入密码就关闭了</p><p>然后编辑mysql的配置文件</p><p>输入 ： <code>vim /etc/my.cnf</code></p><p>在mysqld下添加一行</p><p><code>lower_case_table_names=1</code></p><p>然后退出保存即可</p><p>最后要重新启动mysql输入 ： <code>systemctl resatrt mysqld</code></p><p>就可以了</p><h1 id="mysql误删了配置文件-performance-schema的恢复"><a href="#mysql误删了配置文件-performance-schema的恢复" class="headerlink" title="mysql误删了配置文件 performance_schema的恢复"></a>mysql误删了配置文件 performance_schema的恢复</h1><p>首先我们退出mysql</p><p>然后输入 <code>mysql_upgrade -u root -p </code></p><p>输入我们的密码之后</p><p>然后重新登录我们的mysql就会出现了</p><h1 id="npm怎么清除缓存"><a href="#npm怎么清除缓存" class="headerlink" title="npm怎么清除缓存"></a>npm怎么清除缓存</h1><p>首先关于npm进行清除缓存</p><p>先明确自己的npm的版本</p><p>如果npm version &lt; 7.0.0</p><p><code>$ npm cache clean -f</code></p><p>npm version ≧ 7.0.0 会报以下错误</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ npm cache cleannpm ERR! As of npm@5, the npm cache self-heals from corruption issues</span><br><span class="line">npm ERR! by treating integrity mismatches as cache misses.  As a result,</span><br><span class="line">npm ERR! data extracted from the cache is guaranteed to be valid.  If you</span><br><span class="line">npm ERR! want to make sure everything is consistent, use npm cache verify</span><br><span class="line">npm ERR! instead.  Deleting the cache can only make npm go slower, and is</span><br><span class="line">npm ERR! not likely to correct any problems you may be encountering!</span><br><span class="line">npm ERR!</span><br><span class="line">npm ERR! On the other hand, <span class="keyword">if</span> you<span class="string">&#x27;re debugging an issue with the installer,</span></span><br><span class="line"><span class="string">npm ERR! or race conditions that depend on the timing of writing to an empty</span></span><br><span class="line"><span class="string">npm ERR! cache, you can use npm install --cache /tmp/empty-cache to use a</span></span><br><span class="line"><span class="string">npm ERR! temporary cache instead of nuking the actual one.</span></span><br><span class="line"><span class="string">npm ERR!</span></span><br><span class="line"><span class="string">npm ERR! If you&#x27;</span>re sure you want to delete the entire cache, rerun this <span class="built_in">command</span></span><br><span class="line">npm ERR! with --force.npm ERR! A complete <span class="built_in">log</span> of this run can be found <span class="keyword">in</span>:</span><br><span class="line">npm ERR!     /Users/xxxxxx/.npm/_logs/2021-02-04T06_35_38_043Z-debug.log</span><br></pre></td></tr></table></figure><p>如果想强制清除缓存就要用–force</p><h1 id="关于npm设置镜像源"><a href="#关于npm设置镜像源" class="headerlink" title="关于npm设置镜像源"></a>关于npm设置镜像源</h1><p><code>npm config set registry http://registry.npm.taobao.org</code></p><p>然后就把淘宝镜像源加进入了</p><h1 id="Git之本地分支和远程分支建立追踪关系的几种方式"><a href="#Git之本地分支和远程分支建立追踪关系的几种方式" class="headerlink" title="Git之本地分支和远程分支建立追踪关系的几种方式"></a>Git之本地分支和远程分支建立追踪关系的几种方式</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git branch --set-upstream-to=origin/remote_branch your_branch</span><br><span class="line">origin/remote_branch是你本地分支对应的远程分支；your_branch是你当前的本地分支。</span><br></pre></td></tr></table></figure><p>或者之间checkout -b 根据远程建立本地分支</p><h1 id="git-删除远程分支"><a href="#git-删除远程分支" class="headerlink" title="git 删除远程分支"></a>git 删除远程分支</h1><p><code> git push origin --delete [branch_name]</code></p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/11/17/%E5%9D%91/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hadoop面试会问</title>
      <link>http://example.com/2022/11/17/11-17/</link>
      <guid>http://example.com/2022/11/17/11-17/</guid>
      <pubDate>Thu, 17 Nov 2022 11:47:10 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;hdfs的写流程&quot;&gt;&lt;a href=&quot;#hdfs的写流程&quot; class=&quot;headerlink&quot; title=&quot;hdfs的写流程&quot;&gt;&lt;/a&gt;hdfs的写流程&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;面试|必备&lt;/li&gt;
&lt;li&gt;通过客户端，写文件&lt;/li&gt;
&lt;li&gt;首先客户端会</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="hdfs的写流程"><a href="#hdfs的写流程" class="headerlink" title="hdfs的写流程"></a>hdfs的写流程</h1><ul><li>面试|必备</li><li>通过客户端，写文件</li><li>首先客户端会调用 FileSystem 的creat</li><li>去和namenode进行通信 ， 是通过rpc通信</li><li>然后namenode会去检查这个路径的文件是不是存在并且检查你的权限</li><li>假如都ok ， 则执行你的操作</li><li>但是这个时候还没有写数据</li><li>是不关联任何block的</li><li>而namenode根据上传文件的大小，和块的大小，以及副本数</li><li>计算要上传多少块以及后续要在datanode上的什么位置</li><li>最后把信息返回到客户端</li><li>就是 ： fsdataoutputstream</li><li>Client 调用 fdsdataoutputstream</li><li>将第一个块的第一个副本数写第一个dn节点</li><li>写完去写第二个</li><li>然后是第三个</li><li>当三个都写完了 ， 会返回一个ack package</li><li>把确认包给dn2</li><li>当dn2确认并加上自己的</li><li>然后给dn3，同上</li><li>最后将ack package 返回给 fdsdataoutputstream</li><li>上面只是对于第一个块</li><li>当所有的块全部写完的时候，我们会调用</li><li>这个流对象的close方法</li><li>告诉namenode ，文件写入成功</li></ul><h1 id="hdfs读操作"><a href="#hdfs读操作" class="headerlink" title="hdfs读操作"></a>hdfs读操作</h1><ul><li>面试|必备</li><li>Cilent调用FS.open(filepath)</li><li>nn和pc进行通信，返回该文件的部分或者全部的block列表</li><li>以fdsdatainputstream的形式</li><li>Client的FSDDataInputstream的read方法</li><li>会去和最近的dn进行匹配，读取完成</li><li>会check，假如ok，就关闭通信</li><li>假如读取失败</li><li>会记录dn+block信息</li><li>下次就也不会从这个节点读取，转而从其他的节点读取</li><li>然后会去和第二块的最近的dn进行读取，以此类推</li><li>当整个block全部读取完之后，文件还没读取完，就会继续调用fs的方法进行下一次block的读取</li><li>以此类推</li><li>最后用close方法关闭</li></ul><h1 id="hdfs的副本放置策略"><a href="#hdfs的副本放置策略" class="headerlink" title="hdfs的副本放置策略"></a>hdfs的副本放置策略</h1><ul><li>机架</li><li>生产上读写操作，选择dn节点进行读取</li><li>第一副本 ： </li><li>放置在client上传的dn节点</li><li>client 不在dn节点 ，就随机选择一个dn</li><li>第二副本 ： </li><li>放置在第一个副本不同的机架上的一个dn节点</li><li>第三副本 ： </li><li>和第二副本相同机架但是不同节点</li><li>补充 ；</li><li>副本数是三</li><li>上述都是以副本数为3的情况</li><li>如果是4就多几个随机</li></ul><h1 id="hdfs的安全模式"><a href="#hdfs的安全模式" class="headerlink" title="hdfs的安全模式"></a>hdfs的安全模式</h1><ul><li>什么时候会发生安全模式</li><li>刚启动hadoop的时候</li><li>hsdf发生故障的时候 ： 集群文件大量会丢失</li><li>人为进入</li><li>命令 ： <code>hdfs dfsadmin -safemode &lt;enter | leave | get | wait | forceExit&gt;</code></li></ul><h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><ul><li>kill 命令 不只有一个-9</li><li>还有很多 ： 目前我所看见的至少有64个</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">1) SIGHUP     2) SIGINT     3) SIGQUIT     4) SIGILL     5) SIGTRAP</span><br><span class="line">6) SIGABRT     7) SIGBUS     8) SIGFPE     9) SIGKILL    10) SIGUSR1</span><br><span class="line">11) SIGSEGV    12) SIGUSR2    13) SIGPIPE    14) SIGALRM    15) SIGTERM</span><br><span class="line">16) SIGSTKFLT    17) SIGCHLD    18) SIGCONT    19) SIGSTOP    20) SIGTSTP</span><br><span class="line">21) SIGTTIN    22) SIGTTOU    23) SIGURG    24) SIGXCPU    25) SIGXFSZ</span><br><span class="line">26) SIGVTALRM    27) SIGPROF    28) SIGWINCH    29) SIGIO    30) SIGPWR</span><br><span class="line">31) SIGSYS    34) SIGRTMIN    35) SIGRTMIN+1    36) SIGRTMIN+2    37) SIGRTMIN+3</span><br><span class="line">38) SIGRTMIN+4    39) SIGRTMIN+5    40) SIGRTMIN+6    41) SIGRTMIN+7    42) SIGRTMIN+8</span><br><span class="line">43) SIGRTMIN+9    44) SIGRTMIN+10    45) SIGRTMIN+11    46) SIGRTMIN+12    47) SIGRTMIN+13</span><br><span class="line">48) SIGRTMIN+14    49) SIGRTMIN+15    50) SIGRTMAX-14    51) SIGRTMAX-13    52) SIGRTMAX-12</span><br><span class="line">53) SIGRTMAX-11    54) SIGRTMAX-10    55) SIGRTMAX-9    56) SIGRTMAX-8    57) SIGRTMAX-7</span><br><span class="line">58) SIGRTMAX-6    59) SIGRTMAX-5    60) SIGRTMAX-4    61) SIGRTMAX-3    62) SIGRTMAX-2</span><br><span class="line">63) SIGRTMAX-1    64) SIGRTMAX</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>这个简单介绍几个 ： </li><li>1 ： 重新加载合格进程</li><li>9 ： 杀死一个进程</li><li>15 ： 正常停止一个进程</li></ul><h1 id="公司内部的逻辑"><a href="#公司内部的逻辑" class="headerlink" title="公司内部的逻辑"></a>公司内部的逻辑</h1><ul><li>一般一个组件都会有一个用户进行管理</li><li>比如 ： mysql ： 可以创建一个mysql用户</li><li>等</li><li>创建数据库的流程 ： </li><li><code>create dataase dl2262;</code></li><li><code>grant all priviledges on dl2262 identified by &quot;1234546;&quot;</code></li><li><code>flush priviledges;</code></li></ul><h1 id="hdfs生产最佳实践"><a href="#hdfs生产最佳实践" class="headerlink" title="hdfs生产最佳实践"></a>hdfs生产最佳实践</h1><ul><li>如何查看hdfs的文件块是不是丢失<br>-<code>hdfs fsck&lt;path&gt;</code></li></ul><h2 id="hdfs-的block块丢失了如何解决"><a href="#hdfs-的block块丢失了如何解决" class="headerlink" title="hdfs 的block块丢失了如何解决"></a>hdfs 的block块丢失了如何解决</h2><ul><li>第一种 ： 不用解决 ，也因为hdfs会自动回复，一般是6h ， 就是dn上传的时候</li><li>第二种 ： 手动进行回复</li><li><code>hdfs debug recoverLease -path &lt;filename&gt; retries num</code> </li><li>后面的是重试次数</li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/11/17/11-17/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hadoop</title>
      <link>http://example.com/2022/11/11/11-10/</link>
      <guid>http://example.com/2022/11/11/11-10/</guid>
      <pubDate>Fri, 11 Nov 2022 02:07:37 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;hadoop简介&quot;&gt;&lt;a href=&quot;#hadoop简介&quot; class=&quot;headerlink&quot; title=&quot;hadoop简介&quot;&gt;&lt;/a&gt;hadoop简介&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;以阿帕奇软件 ，hadoop为主的生态圈 &lt;/li&gt;
&lt;li&gt;狭义就是Hadoo</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="hadoop简介"><a href="#hadoop简介" class="headerlink" title="hadoop简介"></a>hadoop简介</h1><ul><li>以阿帕奇软件 ，hadoop为主的生态圈 </li><li>狭义就是Hadoop</li></ul><h2 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h2><ul><li>hdfs ： 存储海量的数据</li><li>mapreduce ： 计算分析</li><li>yarn ： 资源和作业的调度</li></ul><h3 id="观点"><a href="#观点" class="headerlink" title="观点"></a>观点</h3><ul><li>存储是第一位的</li><li>计算是第二位的</li></ul><h2 id="学习介绍"><a href="#学习介绍" class="headerlink" title="学习介绍"></a>学习介绍</h2><ul><li>官网进行学习</li><li><code>hadoop.apache.org</code></li><li>但是后面的其余框架对应的官网就是 把hadoop 改掉</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="hadoop模块的简介"><a href="#hadoop模块的简介" class="headerlink" title="hadoop模块的简介"></a>hadoop模块的简介</h3><ul><li>hdfs ：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">A distributed file system that provides high-throughput access to application data.</span><br></pre></td></tr></table></figure><ul><li>mapredurce :</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">A YARN-based system for parallel processing of large data sets</span><br></pre></td></tr></table></figure><ul><li>yarm :</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">A framework for job scheduling and cluster resource management</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="版本以及主流"><a href="#版本以及主流" class="headerlink" title="版本以及主流"></a>版本以及主流</h3><ul><li>版本 ： 1.x ， 2.x ， 3.x</li><li>主流 ： 2.x -》 3.x</li><li>公司所用 ： </li><li>apache 原生</li><li>cdh ： 5.x ， 6.x 从6.3之后开始收费</li></ul><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><ul><li>安装的是什么？</li><li>Hadoop有什么？</li><li>我们只用部署 yarn hdfs 因为mapredurce是java代码人员给我们的 ：都是主从架构的</li><li>hdfs ： </li><li>namenode : 老大 负责指挥数据的存储</li><li>datanode ： 主要负责数据的存储</li><li>seconderynamenode ： 负责辅助namenode的</li><li>yarn :</li><li>resourcemanager : 老大 负责资源分配</li><li>nodemanager ： 小弟负责资源分配给xxx</li></ul><h3 id="部署模式"><a href="#部署模式" class="headerlink" title="部署模式"></a>部署模式</h3><ul><li>单点模式 ：所有都在一台机器上</li><li>完全分布式模式 ：分布在多台机器上</li></ul><h2 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h2><ul><li>部署平台 ： windows 和linux</li><li>一般linux用的多 ，而且在linux上最少2000台</li><li>jdk ： 安装java的要求</li><li>3.3-目前 ： java8 - 11</li><li>3.0-3.2 ： java8</li><li>2.7-2.10 ： java8</li><li>但是有些的java8 的版本也不行 ，详细参考官网</li><li>要下载补丁安装就好</li><li>ssh ： 默认centos是安装的，但是ubantu是没安装的</li><li>个人要求 ：创建hadoop用户 ，以后我们都用那个用户开发</li><li>而且创建 几个文件夹进行规范</li><li>app ：app</li><li>data ：数据 </li><li>log ：监控日志</li><li>project ：项目 </li><li>shell ： shell脚本</li><li>software ： 安装包</li><li>以前版本的apche 版本框架 在</li><li><code>archive.apache.org/dist</code></li><li>部署jdk</li><li>hadoop ： 无要求</li></ul><h3 id="配置开始"><a href="#配置开始" class="headerlink" title="配置开始"></a>配置开始</h3><ul><li>把文件解压</li><li>配置环境变量</li><li>个人 ： 修改 .&#x2F;bashrc </li><li>添加 exprot JAVA_HOME&#x3D; xxxx</li><li>export PATH&#x3D;${JAVA_HOME}&#x2F;bin:$PATH</li><li>export HADOOP_HOME&#x3D;mmm</li><li>export PATH&#x3D;${HADOOP_HOME}&#x2F;bin:${HADOOP_HOME}&#x2F;sbin:$PATH</li><li>上面xxxx和mmm</li><li>分别代表我们的java的和hadoop的解压目录</li><li>然后我们导入个人环境变量 ， </li><li>source ~&#x2F;.bashrc</li><li>然后验证安装 java -version 和 hadoop version</li><li>如果成功就代表安装成功了</li><li>接下来我们更改hadoop的配置文件</li><li>进入到hadoop的etc文件夹里</li><li>编辑 core-site.xml</li><li>在两个标签之间输入</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hdfs://你的机器名:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>编辑hdfs-site.xml</li><li>在两个标签之间输入</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><ul><li>接下来我们进行开放访问 ， 我们先执行</li><li><code>ssh user@hostname [com]</code> 进行登录</li><li>然后通过输入密码登陆一次</li><li>上一条的的命令如果加上com则代表登录并执行这个命令</li><li>然后我们要设置免密登录</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">ssh-keygen -t rsa -P &#x27;&#x27; -f ~/.ssh/id_rsa</span><br><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">chmod 0600 ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>然后继续上述的命令：看看能不能免密登录</li><li>成功之后，我们要更改文件格式 ： 相当于初始化操作</li><li>然后我们启动dhfs</li><li><code>start-dfs.sh</code></li><li>然后我们访问 <code>http://你的虚拟机ip:9870/</code></li><li>成功看见hadoop的web页面就好了</li><li>然后可以更简便的方法</li><li>在c盘找到 windows 然后进去system32</li><li>然后进去driver</li><li>然后进入etc文件</li><li>然后在hosts文件最后的加上我们在linux里的主机和ip映射</li><li>就可以通过 <code>http://你的虚拟机名称:9870/</code>访问我们的hadoop了</li></ul><h3 id="部署yarn"><a href="#部署yarn" class="headerlink" title="部署yarn"></a>部署yarn</h3><ul><li>对于部署单点的yarn</li><li>我们可以通过配置yarn的配置文件</li><li><code>mapred-site.xml</code> 和<code>yarn-site.xml</code></li></ul><h4 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a>mapred-site.xml</h4><ul><li>对于这个文件我们这样更改</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h4><ul><li>对于这个文件我们进行</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">#设置web访问的端口</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;你的主机名称:9999&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">#设置运行在那个虚拟机上</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;你的主机名&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#下面两个要基本配置</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>配置完成之后我们要进行<code>start-yarn.sh</code></li><li>然后开启这个服务 ： 我们可以通过访问浏览器的 <code>xxx你的ip:9999</code> ,访问这个服务</li><li>这样我们的单点就算配置完成了</li></ul><h2 id="关于分布式的配置"><a href="#关于分布式的配置" class="headerlink" title="关于分布式的配置"></a>关于分布式的配置</h2><ul><li>分布式的配置就是把单点的配置分到多台机器上</li><li>比如 ： 把hdfs的namenode 和 datanode 和secondnamenode分到三台机器上</li><li>把yarn的部署也分到三台机器上</li></ul><h3 id="开始分布式"><a href="#开始分布式" class="headerlink" title="开始分布式"></a>开始分布式</h3><ul><li>首先我们要明确一点 ： namenode是老大 ，只能有一个</li><li>datanode是小弟 ：  可以有多个</li><li>secondnamenode ：是秘书只能有一个</li><li>对于yarn ： resourcemanager只能有一个</li><li>而 ：nodemanagers ：可有多个</li></ul><h4 id="开始之前的配置"><a href="#开始之前的配置" class="headerlink" title="开始之前的配置"></a>开始之前的配置</h4><ul><li>关于hdfs和上面部署的一样</li><li>只不过在配置免密登录的时候不同</li><li>因为我们现在要做三台机器互相免密</li><li>所以我们要用</li><li><code>ssh-keygen -t rsa</code> 然后三次回车生成公钥和私钥</li><li>然后三台机器都要用一遍</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">ssh-copy-id 第一台机器名</span><br><span class="line">ssh-copy-id 第二台机器名</span><br><span class="line">ssh-copy-id 第三台机器名</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>然后分别对三台机器用</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">ssh hadoop@第一台机器名</span><br><span class="line">ssh hadoop@第二台机器名</span><br><span class="line">ssh hadoop@第三台机器名</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>最后分别ssh一下然后如果能成功就代表成功了</li></ul><h4 id="hdfs"><a href="#hdfs" class="headerlink" title="hdfs"></a>hdfs</h4><ul><li><code>core-site.xml</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">#设置存储位置</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/data/hadoopdate&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line">#设置namenode在哪一台机器上运行</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://主机器的名称:9000&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>-<code>workers</code> </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">第一台机器的名称</span><br><span class="line">第二台机器的名称</span><br><span class="line">第三台机器的名称</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><code>hdfs-site.xml</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"> #下面的values代表是几台机器，我这个是三台机器</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  #设置secondarynamenode的端口和在哪一台机器上</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata4:9868&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">#同上一个</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata4:9869&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>然后三台机器都要配置相同的环境变量</li></ul><h4 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h4><ul><li><code>mapred-site.xml</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><code>yarn-site.xml</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">#设置resourcemanager其所占用的端口</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata5:9999&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">#设置resourcemanager其所在的机器</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata5&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">#下面两个是正常yarn的配置文件</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li>同样三台机器也要配置相同的文件</li><li>然后我们要在我们的namenode机器上</li><li>开始初始化： <code>hdfs namenode -format</code></li><li>然后我们在namenode上开始运行<code>start-dfs.sh</code></li><li>然后我们在resourcemanager上开始运行<code>start-yarn.sh</code></li><li>然后我们可以用jps查看每个机器的进程</li><li>查看是不是符合我们的想法</li></ul><h3 id="关于datanode缺失"><a href="#关于datanode缺失" class="headerlink" title="关于datanode缺失"></a>关于datanode缺失</h3><ul><li>因为默认hadoop有一个id文件是在<code>/tmp/hadoop-hadoop/dfs</code>下的</li><li>我们默认启动的时候可能有多个原因 ，造成生成的id不一致</li><li>比如 ； 我们忘记关闭hadoop服务之类的，或者卸载的时候忘记删掉他了</li><li>这些都会造成id不一致的问题</li><li>解决方法： </li><li>如果dfs文件夹中没有重要的数据，那么删除dfs文件夹，再重新运行下列指令（格式化指令）</li><li>如果dfs文件中有重要的数据，那么在dfs&#x2F;name目录下找到一个current&#x2F;VERSION文件，记录clusterID并复制。然后dfs&#x2F;data目录下找到一个current&#x2F;VERSION文件，将其中clustreID的值替换成刚刚复制的clusterID的值即可</li></ul><h2 id="hdfs的命令"><a href="#hdfs的命令" class="headerlink" title="hdfs的命令"></a>hdfs的命令</h2><ul><li>创建文件夹 <code>hdfs dfs -mkdir xxx</code> : 创建xxx文件夹，可以多层创建文件夹</li><li>创建文件<code>hdfs dfs -touchz path</code></li><li>复制文件<code>hdfs dfs -cp 源目录 目标路径</code> ：这个是把整个文件夹结构都cp过去：属于hdfs的内部操作不是上传下载</li><li>移动文件<code>hdfs dfs -mv 源目录 目标目录</code></li><li>赋予权限<code>hdfs dfs -chmod 权限参数 </code></li><li>上传文件<code>hdfs dfs -put 源文件夹路径 目标文件夹路径</code></li><li>上传文件<code>hdfs dfs -copyFromLocal 源文件夹 目标文件夹</code></li><li>上传文件且删除本地文件<code>hdfs dfs -moveFromLocal 源文件 目标文件</code></li><li>下载文件<code>hdfs dfs -get 源文件夹路径 目标文件夹路径</code></li><li>查看文件内容 <code>hdfs dfs -cat path</code>从头看这个文件</li><li>查看文件内容 <code>hdfs dfs -tail path</code>查看这个文件的最后1k</li><li>删除文件<code>hdfs dfs -rm 文件路径</code></li><li>删除文件夹<code>hdfs dfs -rm -R 文件夹</code></li></ul><h2 id="javaapi的方式操作hdfs"><a href="#javaapi的方式操作hdfs" class="headerlink" title="javaapi的方式操作hdfs"></a>javaapi的方式操作hdfs</h2><ul><li><p>单元测试 ： 代表我们可以 ， 单独运行某个方法</p></li><li><p>进行部份调试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">package org.example;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.junit.Assert;</span><br><span class="line">import org.junit.Test;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.net.URI;</span><br><span class="line">import java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public class hdfsapi &#123;</span><br><span class="line">    @Test</span><br><span class="line">    public  void  mkdir() throws IOException, URISyntaxException, InterruptedException &#123;</span><br><span class="line">        // 获取程序入口</span><br><span class="line">        Configuration conf = new org.apache.hadoop.conf.Configuration(); //  配置参数</span><br><span class="line">        URI uri = new URI(&quot;hdfs://192.168.41.132:9000&quot;); //创建uri作为要连接的地址和端口</span><br><span class="line">        FileSystem fs = FileSystem.get( uri , conf , &quot;hadoop&quot;); // 开始链接 ，三个参数分别是其所在地地方，配置参数 ，用户名</span><br><span class="line">        Path path = new Path(&quot;/ggd&quot;); // 路径</span><br><span class="line">        boolean mkdir = fs.mkdirs(path); // 判断是不是执行成功</span><br><span class="line">        Assert.assertEquals(true , mkdir);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>实现创建文件夹并移动且改名</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">package org.example;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">public class hdfsapi &#123;</span><br><span class="line"></span><br><span class="line">    static FileSystem  fs;</span><br><span class="line"></span><br><span class="line">    static &#123;</span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        conf.set(&quot;fs.defaultFS&quot; , &quot;hdfs://192.168.41.132:9000&quot;);</span><br><span class="line">        System.setProperty(&quot;HADOOP_USER_NAME&quot; ,&quot;hadoop&quot;);</span><br><span class="line">        try &#123;</span><br><span class="line">            fs = FileSystem.get(conf);</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        // 移动之前的文件存储路径</span><br><span class="line">        String url = args[1];</span><br><span class="line">        // 移动之后的文件存储路径</span><br><span class="line">        String det = args[2];</span><br><span class="line"></span><br><span class="line">        String hozhui = args[0];</span><br><span class="line"></span><br><span class="line">        // 要上传的文件目录</span><br><span class="line">        String pathlocalfile = args[3];</span><br><span class="line"></span><br><span class="line">        Integer date = Integer.parseInt(args[4]);</span><br><span class="line"></span><br><span class="line">        Integer up = Integer.parseInt(args[5]);</span><br><span class="line"></span><br><span class="line">        Integer down = Integer.parseInt(args[6]);</span><br><span class="line"></span><br><span class="line">        String[] string = new String[args.length - 7];</span><br><span class="line"></span><br><span class="line">        System.arraycopy(args , 7 , string , 0 , args.length - 7);</span><br><span class="line"></span><br><span class="line">        makeream(hozhui,url,det,pathlocalfile,date , up ,down , string);</span><br><span class="line"></span><br><span class="line">      //rm(20221115 , 1 , 1);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void rm(int m , int up ,int down ) &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        // 通过递归调用本身函数，进行递归调用，删除不同日期的文件夹以及</span><br><span class="line">        while (up &gt; 0)&#123;</span><br><span class="line">            rm(m+1 , --up , 0);</span><br><span class="line">        &#125;</span><br><span class="line">        // 同上</span><br><span class="line">        while (down &gt; 0)&#123;</span><br><span class="line">            rm(m-1 , 0 ,--down);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 把数字改变成字符串</span><br><span class="line">        String uri = String.valueOf(m);</span><br><span class="line"></span><br><span class="line">        // 基础路径</span><br><span class="line">        Path path = new Path(&quot;/data/dt=&quot; + uri);</span><br><span class="line">        Path path1 = new Path(&quot;/data/hive&quot; + &quot;/&quot; + uri + &quot;-01.data&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            fs.delete(path);</span><br><span class="line">            fs.delete(path1);</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    private static void makeream(String hozhui, String url , String det ,String pathlocalfile , int nowday , int up , int down , String ...args) &#123;</span><br><span class="line">        // 基礎路徑</span><br><span class="line">        String dataurl = null;</span><br><span class="line">        if (args.length &gt; 0)&#123;</span><br><span class="line">            String[] arg1 = new String[args.length-1];</span><br><span class="line">            System.arraycopy(args , 1 , arg1 ,0 ,args.length-1);</span><br><span class="line">            dataurl = args[0];</span><br><span class="line">            makeream(hozhui,url,det,pathlocalfile,nowday,up,down,arg1);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            dataurl = String.valueOf(nowday);</span><br><span class="line">            // 通过递归的方式 ， 进行创建文件夹等操作</span><br><span class="line">            while (up &gt; 0)&#123;</span><br><span class="line">                makeream(hozhui,url,det,pathlocalfile,nowday+1 , --up , 0);</span><br><span class="line">            &#125;</span><br><span class="line">            while (down &gt; 0)&#123;</span><br><span class="line">                makeream(hozhui,url,det,pathlocalfile,nowday-1 , 0 ,--down);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        // 获取文件名字</span><br><span class="line">        String[] split = pathlocalfile.split(&quot;/&quot;);</span><br><span class="line">        String filname = split[split.length-1];</span><br><span class="line">        // 上传的文件路径</span><br><span class="line">        Path pathfile = new Path(pathlocalfile);</span><br><span class="line">        // 最后的文件的名字</span><br><span class="line">        String settotal = det+ &quot;/&quot; + dataurl + hozhui;</span><br><span class="line">        // 刚开始创建的文件目录</span><br><span class="line">        String urltotal = url + dataurl;</span><br><span class="line">        // 创建文件夹的Path</span><br><span class="line">        Path path = new Path(urltotal);</span><br><span class="line">        // 移动前的文件路径</span><br><span class="line">        Path pathfilehdfs = new Path( path + &quot;/&quot; + filname);</span><br><span class="line">        // 新的名字文件路径</span><br><span class="line">        Path newname = new Path(settotal);</span><br><span class="line"></span><br><span class="line">//        // 要移动到的路径</span><br><span class="line">//        Path detPath = new Path(url1);</span><br><span class="line"></span><br><span class="line">        // 后来的文件目录</span><br><span class="line">        // Path detpathfile = new Path(det + &quot;/&quot; + filname);</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            if(!fs.exists(new Path(det)))&#123;</span><br><span class="line">                fs.mkdirs(new Path(det));</span><br><span class="line">            &#125;</span><br><span class="line">            fs.mkdirs(path);</span><br><span class="line">            fs.copyFromLocalFile(pathfile , path);</span><br><span class="line">            // fs.rename(pathfilehdfs , detPath);</span><br><span class="line">            fs.rename(pathfilehdfs , newname);</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>单词统计</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">package org.example;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IOUtils;</span><br><span class="line"></span><br><span class="line">import java.io.*;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line">import java.util.Map;</span><br><span class="line"></span><br><span class="line">public class wordcount &#123;</span><br><span class="line">    static FileSystem fs;</span><br><span class="line"></span><br><span class="line">    static &#123;</span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        conf.set(&quot;fs.defaultFS&quot; , &quot;hdfs://192.168.41.132:9000&quot;);</span><br><span class="line">        System.setProperty(&quot;HADOOP_USER_NAME&quot; ,&quot;hadoop&quot;);</span><br><span class="line">        try &#123;</span><br><span class="line">            fs = FileSystem.get(conf);</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">        wordcounts(&quot;/2.log&quot; , &quot;,&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void wordcounts( String path ,  String regx ) &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        String basic = null;</span><br><span class="line"></span><br><span class="line">        try( FSDataInputStream fis = fs.open(new Path(path));</span><br><span class="line">             OutputStream outputStream = new FileOutputStream( new File(&quot;D:\\ bg1.txt&quot;) , false);</span><br><span class="line">             InputStream inputStream = new FileInputStream(new File(&quot;D:\\ bg1.txt&quot;));</span><br><span class="line">             )</span><br><span class="line">        &#123;</span><br><span class="line"></span><br><span class="line">            IOUtils.copyBytes(fis,outputStream,4096 , true);</span><br><span class="line">            byte[] buffer = new byte[1024];</span><br><span class="line">            int len = 0;</span><br><span class="line">            while((len = inputStream.read(buffer)) != -1)&#123;</span><br><span class="line">                basic = new String(buffer, 0, len);</span><br><span class="line">                System.out.println(basic);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            String[] split = basic.split(regx);</span><br><span class="line"></span><br><span class="line">            Map&lt;String , Integer&gt; result = new HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            for (int i=0;i&lt;split.length;i++)&#123;</span><br><span class="line">                result.put(split[i] , result.getOrDefault(split[i] , 0 )+1);</span><br><span class="line">            &#125;</span><br><span class="line">                result.forEach((k,v)-&gt;System.out.println(k+&quot;,&quot;+v));</span><br><span class="line"></span><br><span class="line">            int[] flag = new int[split.length];</span><br><span class="line">            for (int i =0; i&lt; split.length; i++)&#123;</span><br><span class="line">                for (int j = 0; j&lt; split.length;j++)&#123;</span><br><span class="line">                    if (split[i].equals(split[j]))&#123;</span><br><span class="line">                        flag[i]++;</span><br><span class="line">                    &#125;else&#123;</span><br><span class="line"></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            System.out.println(&quot;--------------------------------------------------------&quot;);</span><br><span class="line">            for (int i =0; i&lt;split.length-1;i++)&#123;</span><br><span class="line"></span><br><span class="line">                if (!(split[i].equals(split[i+1])))&#123;</span><br><span class="line">                    System.out.println(split[i] + &quot;\t&quot; + flag[i]);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">//            System.out.print(&quot;\n&quot;);</span><br><span class="line"></span><br><span class="line">//            for (int i =0; i&lt; flag.length;i++)&#123;</span><br><span class="line">//                System.out.print(flag[i]);</span><br><span class="line">//                System.out.print(&quot;\t&quot;);</span><br><span class="line">//            &#125;</span><br><span class="line"></span><br><span class="line">            System.out.println(&quot;--------------------------------------&quot;);</span><br><span class="line"></span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="关于文件的存储"><a href="#关于文件的存储" class="headerlink" title="关于文件的存储"></a>关于文件的存储</h3><ul><li>架构设计 ： 面试必问</li><li>namenode —–nn</li><li>负责对外的访问接口</li><li>负责块的映射</li><li>元数据 ； 描述数据的数据</li><li>文件名称</li><li>文件的目录</li><li>文件的属性，权限，创建时间，副本数据</li><li>blockmap ： 块映射</li><li>一个 文件被分割成多个数据块，</li><li>块映射不会永久化这个存储</li><li>是通过集群运行的时候dn定期发送blockreport给nn进行维护</li><li>控制其数据块在哪一个节点上的</li><li>nn作用 ： 管理文件的命名空间 ，其实就算维护文件系统树的文件以及文件夹</li><li>是以两种的方式永久的存储在磁盘</li><li>镜像文件 ： fsimage</li><li>编辑日志文件 editlogs</li><li>seconderynamenode ——snn</li><li>去老大的节点上拿镜像文件和日志文件，进行合并和备份，然后换给nn</li><li>datanode —–dn</li><li>每个节点都会有这个进程</li><li>负责关于客户端的文件的读写</li><li>负责存储数据</li><li>存储数据块，以及对于数据块的校验</li><li>每隔3秒发送一次心跳给namenode ，告诉你我还在</li><li>每个一定时间（6h）发送一次块报告,这个报告，是扫描磁盘和内存之中的数据一不一样</li><li>目的 ： 生产上 ： 可能会发生文件块丢失或者损坏</li><li></li></ul><h2 id="mapreduce"><a href="#mapreduce" class="headerlink" title="mapreduce"></a>mapreduce</h2><ul><li>简介 ： 其设计理念是计算向数据靠拢，采用分而治之的策略，将庞大的数据分为很多个很多个小切片，并且为每个小切片单独的启动一个map任务</li><li>适合mapredurce处理的数据集要满足一个前提：待处理的数据集可以分解成许多更小的数据集，且每一个更小的数据集都可以并行的处理</li><li>其采用的是主从架构（master&#x2F;Slave），就是一个主服务器多个从服务器（salve），master上运行jobTracker，slaver运行TaskTracker</li></ul><h3 id="mapreduce体系架构"><a href="#mapreduce体系架构" class="headerlink" title="mapreduce体系架构"></a>mapreduce体系架构</h3><ul><li>其主要是由四个部分组成 ： 分别是Client , JobTracker,TaskTracker以及Task</li><li>Client ： 用户编写的Mapredurce程序通过Client提交到jobTracker</li><li>jobTracker(运行在主服务器上) : 负责监控和资源调度</li><li>监控所有的TaskTracker与job的健康情况，一旦发现失败，就把相应的任务转移到其他的节点</li><li>其会跟踪任务的执行调度，资源使用量，并将这些信息告诉任务调度器（TaskScheduler），而调度器会在资源调度器出现空闲时，选择合适的任务去使用这些资源</li><li>jobTracker（运行在服务器） </li><li>TaskTracker会周期性的通过心跳将本节点上资源的使用情况和任务运行进度汇报给jobTracker，同时接受jobTracker发送过来的命令并执行相应的操作（如 ： 启动新任务 ，杀死任务等）</li><li>TaskTracker使用“slot”等量划分本节点上的资源量（CPU ，内存）。一个Task获取到一个slot后才有机会运行，而hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot分为Map slot和Reduce slot两种，分别提供Map Task和Redurce Task使用</li><li>Task</li><li>Map Task和Redurce Task 均由TaskTracker</li></ul><h3 id="mapreduce的工作流程"><a href="#mapreduce的工作流程" class="headerlink" title="mapreduce的工作流程"></a>mapreduce的工作流程</h3><ul><li>一个大的mapredurce任务，首先会被分为为若干个Map任务在多台机器上执行运行（map任务通常运行在存储节点上），每一个map任务会输出一个&lt;key , value&gt;形式的中间结果，一个map任务只有全部执行完成之后才会进行reduce任务，map的输出结果&lt;key, value&gt;（存储在本地磁盘） ， 具有相同的key会被发送到同一个reduce任务</li><li>注意 ：不同的map之间不会有通信</li><li>不同的reduce之间也不会</li><li>用户不能显式的从一台机器向另外一台机器发送消息，所有的信息交换都是通过Mapredurce框架自身去实现</li><li>map的输入文件，redurce任务的输出结果都是保存在hdfs分布式文件系统中，map的输出结果保存在本地磁盘文件中</li><li>当一个map任务处理全部结束之后，reduce任务才能开始去取相应的数据</li><li>只有map任务需要考虑数据局限性，实现计算向数据靠拢，reduce无需考虑数据局限性</li></ul><h4 id="各阶段执行内容"><a href="#各阶段执行内容" class="headerlink" title="各阶段执行内容"></a>各阶段执行内容</h4><ul><li>分为 ：5个阶段 预处理，map，shuffle，reduce，输出</li><li>预处理 ： 由inputFormat 进行格式验证以及逻辑上的分区inputSplit ， inputSplit经过redodrdreader</li><li>根据inputSplit的信息来处理inputSplit中的具体记录，转换为键值对 ，输入给map</li><li>map ： 接受来自RR键值对，进行分区，排序，合并，归并，得到&lt;key , value-list&gt;形式的中间结果，输入给reduce，此处包括map端的shuffle和reduce端的shuffle</li><li>reduce ： 接受shuffle输出的&lt;key , value&gt;,执行用户子自定义，输出给outputFromat</li><li>输出 ： outputFromat，模块会验证输出目录是不是已经存在以及输出结果是不是符合配置文件中的配置类型，如果满足，就输出reduce的结果到分布式文件系统</li></ul><h4 id="map的shuffle"><a href="#map的shuffle" class="headerlink" title="map的shuffle"></a>map的shuffle</h4><ul><li>过程有四部</li><li>输入数据并执行map任务</li><li>map任务结果输出，写入缓存</li><li>溢写 ： 如果输出结果超过了一定的比例，每一次溢写会在磁盘上生成一个磁盘文件，写入之前进行会分区，，排序，如果指定了commbiner还可以进行合并，这样经过溢写的磁盘文件就包含了多个分区，且分区内部都是经过排序的</li><li>merge ： 随着map任务的进行，当有多个溢写文件时（就是大于等于2的适合），就会发现merge，生成一个更大的磁盘文件，这个大的溢写文件也是经过排序和分区的，默认情况下每10个溢写会变成一个大文件，通常在merge过程中，如果文件数量大于3则进行combine操作，从而减少磁盘的数据量，如果只有一两个溢写，合并操作得不偿失</li></ul><h4 id="reduce的shuttle"><a href="#reduce的shuttle" class="headerlink" title="reduce的shuttle"></a>reduce的shuttle</h4><ul><li>领取数据 ： 经过map的suffle后，map的输出结果保存在磁盘，此时，需要将磁盘数据取回到reduce机器，此时如果线程被占满，同样会和map端一样开启溢写操作，reduce通过RPC询问JobTracker是不是还拥有未完成的map任务，如果有，将数据提取到reduce机器上，此时实际上是多个reduce机器，同时多线程从map机器领回数据</li><li>merge ： 一个map的shuffle结果因为拥有多个分区，所以会有不同的reduce机器取回自己的数据，而每一个reduce也从不同的map机器取回数据，如果每个reduce机器内存达到阈值，就进行溢写操作，溢写的时候一般有很多键值对可以进行merge，如果定义了combiner还可以进行合并，进行溢写操作的过程中还可以进行combine。并非是一个reduce程序从map机器上取回数据就生成一个溢写文件，而是缓存不够用，则发生溢写，如果缓存够用，则是直接在内存里进行操作</li><li>把数据输出给map任务（对一个reduce而言）：执行用户自定义的逻辑，最终输出</li></ul><h2 id="mapredurce里的数据类型"><a href="#mapredurce里的数据类型" class="headerlink" title="mapredurce里的数据类型"></a>mapredurce里的数据类型</h2><ul><li>intWritable</li><li>longWritable</li><li>等等，</li><li>就是基本数据类型加上Writable</li><li>在redurce里数据类型也是一样的</li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/11/11/11-10/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>git</title>
      <link>http://example.com/2022/11/08/11-8/</link>
      <guid>http://example.com/2022/11/08/11-8/</guid>
      <pubDate>Tue, 08 Nov 2022 02:13:43 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Git是一款免费、开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。&lt;/li&gt;
&lt;li&gt;Git是一个开源的分布式版</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><ul><li>Git是一款免费、开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。</li><li>Git是一个开源的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。Git 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。</li><li>记录文件变化的 ，之后可以指定版本恢复</li></ul><h2 id="版本控制"><a href="#版本控制" class="headerlink" title="版本控制"></a>版本控制</h2><ul><li>本地 ： 常用 ：简单 ， 缺点 ： 容易出错 ， 集成效率低下</li><li>集中 ： svn ： 缺点 ： 中央服务器出现故障就全完了</li><li>分布（目前的主流）： 解决单点故障的问题 ： git</li><li>git 可以分支 ，，且支持文件备份 ， 多个工作流</li><li></li></ul><h1 id="Git工作流程"><a href="#Git工作流程" class="headerlink" title="Git工作流程"></a>Git工作流程</h1><ul><li>从远程仓库中克隆 Git 资源作为本地仓库</li><li>从本地仓库中checkout代码然后进行代码修改</li><li>在提交本地仓库前先将代码提交到暂存区</li><li>提交修改，提交到本地仓库；本地仓库中保存修改的各个历史版本</li><li>在需要和团队成员共享代码时，可以将修改代码push到远程仓库</li><li>git 的核心概念 ： 工作区、暂存区、版本库、远程仓库</li><li>Workspace： 工作区，就是你平时存放项目代码的地方</li><li>Index &#x2F; Stage： 暂存区，用于临时存放你的改动，事实上它只是一个文件，保存即将提交到文件列表信息</li><li>Repository： 仓库区（或版本库），就是安全存放数据的位置，这里面有你提交到所有版本的数据。其中HEAD指向最新放入仓库的版本</li><li>Remote： 远程仓库，托管代码的服务器，可以简单的认为是你项目组中的一台电脑用于远程数据交换</li></ul><h1 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h1><ul><li>每次的提交Git都把它们串成一条时间线，这条时间线就是一个分支。截止到目前，只有一条时间线，在Git里这个分支叫主分支，即master分支。HEAD指针严格来说不是指向提交，而是指向master，master才是指向提交的。</li></ul><h1 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h1><ul><li>远程仓库分为公有远程仓库和私有远程仓库。</li></ul><h1 id="公有远程仓库"><a href="#公有远程仓库" class="headerlink" title="公有远程仓库"></a>公有远程仓库</h1><ul><li>本质和本地仓库无异，只是这个仓库①不在本地②大家可能都知道③需要将代码共享到远程仓库④可以被其他人克隆同步代码等。</li><li>一般情况下在企业中会有一个搭建在公司的远程仓库，可以让本公司内部的开发人员同步开发。而业界最富盛名的远程仓库则为github；它上面存放了非常多的开源组织、个人、企业等的开放源码库，任何都可以从上面获取源码。</li></ul><h1 id="私有远程仓库"><a href="#私有远程仓库" class="headerlink" title="私有远程仓库"></a>私有远程仓库</h1><ul><li>远程仓库实际上和本地仓库一样，纯粹为了7x24小时开机并交换大家的修改。GitHub就是一个免费托管开源代码的远程仓库。但是对于某些视源代码如生命的商业公司来说，既不想公开源代码，又舍不得给GitHub交保护费，那就只能自己搭建一台Git服务器作为私有仓库使用。</li><li>在本地创建了一个Git仓库，又想让其他人来协作开发，此时就可以把本地仓库同步到远程仓库，同时还增加了本地仓库的一个备份。</li><li>常用的远程仓库就是github：<a href="https://github.com/">https://github.com/</a></li><li>Github支持两种同步方式“https”和“ssh”。如果使用https很简单基本不需要配置就可以使用，但是每次提交代码和下载代码时都需要输入用户名和密码。而且如果是公司配置的私有git服务器一般不提供https方式访问。</li></ul><h1 id="忽略文件"><a href="#忽略文件" class="headerlink" title="忽略文件"></a>忽略文件</h1><ul><li>在工程中，并不是所有文件都需要保存到版本库中的，例如“target”目录及目录下的文件就可以忽略。Git忽略文件详解可参考Git忽略文件.gitignore详解</li></ul><h1 id="常用Git命令"><a href="#常用Git命令" class="headerlink" title="常用Git命令"></a>常用Git命令</h1><ul><li>经常使用 Git ，但是很多命令还是记不住。但要熟练掌握，恐怕要记住40~60个命令，所以整理了一份常用Git命令清单。可以参考常用Git命令</li></ul><h2 id="常用的git"><a href="#常用的git" class="headerlink" title="常用的git"></a>常用的git</h2><h3 id="配置用户名和邮箱"><a href="#配置用户名和邮箱" class="headerlink" title="配置用户名和邮箱"></a>配置用户名和邮箱</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">$ git --version   # 查看git的版本信息</span><br><span class="line">$ git config --global user.name   # 获取当前登录的用户</span><br><span class="line">$ git config --global user.email  # 获取当前登录用户的邮箱</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="登录git"><a href="#登录git" class="headerlink" title="登录git"></a>登录git</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 如果刚没有获取到用户配置，则只能拉取代码，不能修改  要是使用git，你要告诉git是谁在使用</span><br><span class="line">$ git config --global user.name &#x27;userName&#x27;    # 设置git账户，userName为你的git账号，</span><br><span class="line">$ git config --global user.email &#x27;email&#x27;</span><br><span class="line"># 获取Git配置信息，执行以下命令：</span><br><span class="line">$ git config –list</span><br></pre></td></tr></table></figure><h3 id="配置https和ssh推送时保存用户名和密码"><a href="#配置https和ssh推送时保存用户名和密码" class="headerlink" title="配置https和ssh推送时保存用户名和密码"></a>配置https和ssh推送时保存用户名和密码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"># https提交保存用户名和密码</span><br><span class="line">$ git config --global credential.helper store</span><br><span class="line"># 生成公钥私钥，将公钥配置到GitHub，ssh提交就可以免输入用户名密码</span><br><span class="line"># 三次回车即可生成 ssh key</span><br><span class="line">$ ssh-keygen -t rsa</span><br><span class="line"># 查看已生成的公钥</span><br><span class="line">$ cat ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure><h3 id="推送到远程仓库正确流程"><a href="#推送到远程仓库正确流程" class="headerlink" title="推送到远程仓库正确流程"></a>推送到远程仓库正确流程</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">git init # 初始化仓库</span><br><span class="line">git add .(文件name) # 添加文件到暂存区</span><br><span class="line">git commit -m &quot;first commit&quot; # 添加文件到本地仓库并提交描述信息</span><br><span class="line">git remote add origin 远程仓库地址 # 链接远程仓库，创建主分支</span><br><span class="line">git pull origin master --allow-unrelated-histories # 把本地仓库的变化连接到远程仓库主分支</span><br><span class="line">git push -u origin master # 把本地仓库的文件推送到远程仓库</span><br></pre></td></tr></table></figure><h3 id="新建本地仓库"><a href="#新建本地仓库" class="headerlink" title="新建本地仓库"></a>新建本地仓库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 创建一个文件夹</span><br><span class="line">$ mkdir GitRepositories    # 创建文件夹GitRepositories</span><br><span class="line">$ cd GitRepositories       # 切换到GitRepositories目录下</span><br><span class="line"># 在当前目录新建一个Git代码库</span><br><span class="line">$ git init</span><br><span class="line"># 新建一个目录，将其初始化为Git代码库</span><br><span class="line">$ git init [project-name]</span><br><span class="line"># 下载一个项目和它的整个代码历史</span><br><span class="line">$ git clone [url]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="配置-全局和项目"><a href="#配置-全局和项目" class="headerlink" title="配置(全局和项目)"></a>配置(全局和项目)</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。</span><br><span class="line"># 显示当前的Git配置</span><br><span class="line">$ git config --list</span><br><span class="line"># 编辑Git配置文件</span><br><span class="line">$ git config -e [--global]</span><br><span class="line"># 设置提交代码时的用户信息</span><br><span class="line">$ git config [--global] user.name &quot;[name]&quot;</span><br><span class="line">$ git config [--global] user.email &quot;[email address]&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="增加-x2F-删除文件"><a href="#增加-x2F-删除文件" class="headerlink" title="增加&#x2F;删除文件"></a>增加&#x2F;删除文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"># 添加指定文件到暂存区</span><br><span class="line">$ git add [file1][file2] ...</span><br><span class="line"># 添加指定目录到暂存区，包括子目录</span><br><span class="line">$ git add [dir]</span><br><span class="line"># 添加当前目录的所有文件到暂存区</span><br><span class="line">$ git add .</span><br><span class="line"># 添加每个变化前，都会要求确认</span><br><span class="line"># 对于同一个文件的多处变化，可以实现分次提交</span><br><span class="line">$ git add -p</span><br><span class="line"># 删除工作区文件，并且将这次删除放入暂存区</span><br><span class="line">$ git rm [file1] [file2] ...</span><br><span class="line"># 停止追踪指定文件，但该文件会保留在工作区</span><br><span class="line">$ git rm --cached [file]</span><br><span class="line"># 改名文件，并且将这个改名放入暂存区</span><br><span class="line">$ git mv [file-original] [file-renamed]</span><br></pre></td></tr></table></figure><h3 id="代码提交"><a href="#代码提交" class="headerlink" title="代码提交"></a>代码提交</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 提交暂存区到仓库区</span><br><span class="line">$ git commit -m [message]</span><br><span class="line"># 提交暂存区的指定文件到仓库区</span><br><span class="line">$ git commit [file1] [file2] ... -m [message]</span><br><span class="line"># 提交工作区自上次commit之后的变化，直接到仓库区</span><br><span class="line">$ git commit -a</span><br><span class="line"># 提交时显示所有diff信息</span><br><span class="line">$ git commit -v</span><br><span class="line"># 使用一次新的commit，替代上一次提交</span><br><span class="line"># 如果代码没有任何新变化，则用来改写上一次commit的提交信息</span><br><span class="line">$ git commit --amend -m [message]</span><br><span class="line"># 重做上一次commit，并包括指定文件的新变化</span><br><span class="line">$ git commit --amend [file1] [file2] ...</span><br></pre></td></tr></table></figure><h3 id="分支-1"><a href="#分支-1" class="headerlink" title="分支"></a>分支</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"># 列出所有本地分支</span><br><span class="line">$ git branch</span><br><span class="line"># 列出所有远程分支</span><br><span class="line">$ git branch -r</span><br><span class="line"># 列出所有本地分支和远程分支</span><br><span class="line">$ git branch -a</span><br><span class="line"># 新建一个分支，但依然停留在当前分支</span><br><span class="line">$ git branch [branch-name]</span><br><span class="line"># 新建一个分支，并切换到该分支</span><br><span class="line">$ git checkout -b [branch]</span><br><span class="line"># 新建一个分支，指向指定commit</span><br><span class="line">$ git branch [branch] [commit]</span><br><span class="line"># 新建一个分支，与指定的远程分支建立追踪关系</span><br><span class="line">$ git branch --track [branch] [remote-branch]</span><br><span class="line"># 切换到指定分支，并更新工作区</span><br><span class="line">$ git checkout [branch-name]</span><br><span class="line"># 切换到上一个分支</span><br><span class="line">$ git checkout -</span><br><span class="line"># 建立追踪关系，在现有分支与指定的远程分支之间</span><br><span class="line">$ git branch --set-upstream [branch] [remote-branch]</span><br><span class="line"># 合并指定分支到当前分支</span><br><span class="line">$ git merge [branch]</span><br><span class="line"># 选择一个commit，合并进当前分支</span><br><span class="line">$ git cherry-pick [commit]</span><br><span class="line"># 删除分支</span><br><span class="line">$ git branch -d [branch-name]</span><br><span class="line"># 删除远程分支</span><br><span class="line">$ git push origin --delete [branch-name]</span><br><span class="line">$ git branch -dr [remote/branch]</span><br><span class="line">#强制删除分支</span><br><span class="line">$ git branch -D [branch-name]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 列出所有tag</span><br><span class="line">$ git tag</span><br><span class="line"># 新建一个tag在当前commit</span><br><span class="line">$ git tag [tag]</span><br><span class="line"># 新建一个tag在指定commit</span><br><span class="line">$ git tag [tag] [commit]</span><br><span class="line"># 删除本地tag</span><br><span class="line">$ git tag -d [tag]</span><br><span class="line"># 删除远程tag</span><br><span class="line">$ git push origin :refs/tags/[tagName]</span><br><span class="line"># 查看tag信息</span><br><span class="line">$ git show [tag]</span><br><span class="line"># 提交指定tag</span><br><span class="line">$ git push [remote] [tag]</span><br><span class="line"># 提交所有tag</span><br><span class="line">$ git push [remote] --tags</span><br><span class="line"># 新建一个分支，指向某个tag</span><br><span class="line">$ git checkout -b [branch] [tag]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="查看信息"><a href="#查看信息" class="headerlink" title="查看信息"></a>查看信息</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"># 查看目录</span><br><span class="line">$ ls -al或者$ ll</span><br><span class="line"># 查看仓库状态，显示有变更的文件</span><br><span class="line">$ git status</span><br><span class="line"># 显示当前分支的版本历史</span><br><span class="line">$ git log</span><br><span class="line"># 显示commit历史，以及每次commit发生变更的文件</span><br><span class="line">$ git log --stat</span><br><span class="line"># 搜索提交历史，根据关键词</span><br><span class="line">$ git log -S [keyword]</span><br><span class="line"># 显示某个commit之后的所有变动，每个commit占据一行</span><br><span class="line">$ git log [tag] HEAD --pretty=format:%s</span><br><span class="line"># 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件</span><br><span class="line">$ git log [tag] HEAD --grep feature</span><br><span class="line"># 显示某个文件的版本历史，包括文件改名</span><br><span class="line">$ git log --follow [file]</span><br><span class="line">$ git whatchanged [file]</span><br><span class="line"># 显示指定文件相关的每一次diff</span><br><span class="line">$ git log -p [file]</span><br><span class="line"># 显示过去5次提交</span><br><span class="line">$ git log -5 --pretty --oneline</span><br><span class="line"># 显示所有提交过的用户，按提交次数排序</span><br><span class="line">$ git shortlog -sn</span><br><span class="line"># 显示指定文件是什么人在什么时间修改过</span><br><span class="line">$ git blame [file]</span><br><span class="line"># 显示暂存区和工作区的差异</span><br><span class="line">$ git diff</span><br><span class="line"># 显示暂存区和上一个commit的差异</span><br><span class="line">$ git diff --cached [file]</span><br><span class="line"># 显示工作区与当前分支最新commit之间的差异</span><br><span class="line">$ git diff HEAD</span><br><span class="line"># 显示两次提交之间的差异</span><br><span class="line">$ git diff [first-branch]...[second-branch]</span><br><span class="line"># 显示今天你写了多少行代码</span><br><span class="line">$ git diff --shortstat &quot;@&#123;0 day ago&#125;&quot;</span><br><span class="line"># 显示某次提交的元数据和内容变化</span><br><span class="line">$ git show [commit]</span><br><span class="line"># 显示某次提交发生变化的文件</span><br><span class="line">$ git show --name-only [commit]</span><br><span class="line"># 显示某次提交时，某个文件的内容</span><br><span class="line">$ git show [commit]:[filename]</span><br><span class="line"># 显示当前分支的最近几次提交</span><br><span class="line">$ git reflog</span><br><span class="line"># 以图形化界面展示 分支的commit 历史</span><br><span class="line">$ git log --oneline --gragh</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="远程同步"><a href="#远程同步" class="headerlink" title="远程同步"></a>远程同步</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 下载远程仓库的所有变动</span><br><span class="line">$ git fetch [remote]</span><br><span class="line"># 显示所有远程仓库</span><br><span class="line">$ git remote -v</span><br><span class="line"># 显示某个远程仓库的信息</span><br><span class="line">$ git remote show [remote]</span><br><span class="line"># 增加一个新的远程仓库，并命名</span><br><span class="line">$ git remote add [shortname] [url]</span><br><span class="line"># 取回远程仓库的变化，并与本地分支合并</span><br><span class="line">$ git pull [remote] [branch]</span><br><span class="line">$ git fetch 也同上</span><br><span class="line">$ git pull 相当 fetch ＋ merge</span><br><span class="line"># 上传本地指定分支到远程仓库</span><br><span class="line">$ git push [remote] [branch]</span><br><span class="line"># 强行推送当前分支到远程仓库，即使有冲突</span><br><span class="line">$ git push [remote] --force</span><br><span class="line"># 推送所有分支到远程仓库</span><br><span class="line">$ git push [remote] --all</span><br><span class="line">$ push推的是分支不是代码</span><br><span class="line"># 可以选择那个远程仓库那个分支</span><br><span class="line">$ git push [&lt;repository&gt; [&lt;分支的名字&gt;]]</span><br><span class="line">$ git fetch</span><br><span class="line">$ git fetch --all</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="撤销"><a href="#撤销" class="headerlink" title="撤销"></a>撤销</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"># 恢复暂存区的指定文件到工作区</span><br><span class="line">$ git checkout [file]</span><br><span class="line"># 从工作区撤回文件</span><br><span class="line">$ git restore --staged xxxx xxx xxx... </span><br><span class="line">#从暂存区到工作区撤回</span><br><span class="line">$ git restore xxx xxx xxx..</span><br><span class="line"># 恢复某个commit的指定文件到暂存区和工作区</span><br><span class="line">$ git checkout [commit] [file]</span><br><span class="line"># 恢复暂存区的所有文件到工作区</span><br><span class="line">$ git checkout .</span><br><span class="line"># 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变</span><br><span class="line">$ git reset [file]</span><br><span class="line"># 重置暂存区与工作区，与上一次commit保持一致</span><br><span class="line">$ git reset --hard</span><br><span class="line"># 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变</span><br><span class="line">$ git reset [commit]</span><br><span class="line"># 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致</span><br><span class="line">$ git reset --hard [commit]</span><br><span class="line"># 重置当前HEAD为指定commit，但保持暂存区和工作区不变</span><br><span class="line">$ git reset --keep [commit]</span><br><span class="line"># 新建一个commit，用来撤销指定commit</span><br><span class="line"># 后者的所有变化都将被前者抵消，并且应用到当前分支</span><br><span class="line">$ git revert [commit]</span><br><span class="line"># 暂时将未提交的变化移除，稍后再移入</span><br><span class="line">$ git stash</span><br><span class="line">$ git stash pop</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 从当前目录的所有文件中查找文本内容：</span><br><span class="line">$ git grep &quot;Hello&quot;</span><br><span class="line"># 在某一版本中搜索文本：</span><br><span class="line">$ git grep &quot;Hello&quot; v2.5</span><br><span class="line"># 生成一个可供发布的压缩包</span><br><span class="line">$ git archive</span><br></pre></td></tr></table></figure><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><ul><li>当工作区和暂存区都有的时候</li><li>直接更改工作区的文件 通过</li><li>commit -am “xx”</li><li>可以直接把修改了的文件传到本地仓库</li><li>merge : 经过merge的文件 ， 相当于把其他地方的分支给回收回来 ，</li></ul><h3 id="git文件夹的目录介绍"><a href="#git文件夹的目录介绍" class="headerlink" title=".git文件夹的目录介绍"></a>.git文件夹的目录介绍</h3><ul><li>HEAD ： 当前项目正在工作的分支</li><li>config ： 我们当前项目的一些配置信息（我们本地的配置信息）</li><li>可以通过 <code>git config --local user.name &quot;xxx&quot;</code> 来进行设置</li><li>或者直接在这个文件按照格式改也一样</li><li>同样 ， 有set就有get</li><li>refs ： 指向的是那个分支的指向</li><li>commit 的hash值对应的是什么内容的</li><li>object : git真正的存储对象 </li><li><code>git cat-file</code> </li><li><code>-t</code> : 文件类型</li><li><code>-p</code> : 文件内容</li><li><code>tree</code> ： 代表是文件夹</li><li><code>blob</code> ： 代表文件内容</li><li>git 存储文件高效的原因 ： 不同的文件只要有相同的地方就是一个blob</li></ul><h3 id="查看git文件存储的方法"><a href="#查看git文件存储的方法" class="headerlink" title="查看git文件存储的方法"></a>查看git文件存储的方法</h3><ul><li>我们先进入到.git的object文件夹里 ，因为object文件夹是git存储的首要位置</li><li>接下来 ，我们找到refs的文件 ，然后进入到heads里</li><li>找到文件 ，里面有我们的commit的哈希值</li><li>通过哈希值的前两个字母</li><li>我们在object文件夹下 ，找到这两个字符的文件夹</li><li>点进去查看文件名字</li><li>然后我们在命令行</li><li>输入<code>git cat-file -p</code></li><li>和<code>git cat-file -t</code></li><li>就可以查看到文件的类型和文件的内容</li><li>对于文件的内容，我们可以对于上次-p出来的文件再次进行 -p操作 哈希值 ，</li><li>就可以看见了</li><li>修改之前的某个commit的message</li><li>可以用 <code>git rebase</code></li><li>git rebase -i &lt;after - this - commit&gt;: 交互式 : 基于前一个commit 进行修改当前的commit ，这样修改了之后所有的hash值会改变，因为变头了 ，但是如果先进行了 ， <code>git cherry-pick</code>操作,则会把吗，master和以前的分支都改变（hash），但是一般是不带master等其他分支的</li><li>常用的操作</li><li>-p ： </li><li>-r ： 重写一个commit的message</li><li>-e ： </li><li>-s ： 合并多个commit的message</li><li>输入上面的那个命令之后我们要进入到一个界面</li><li>把要修改的前面改成 r</li><li>然后保存退出</li><li>然后就会跳转到另外可以编辑这个要编辑的conmmit的地方</li><li>就改掉message就好</li><li>合并本地message 要在本地合并 ，不能在远程合并</li><li>把多个commit合并成一个commit的操作和上面一样 ，只不过是把r改成s了</li><li>但是这样合并之后，你相当于把这些信息合并到一起 ，就把最上面的那个看成是一个集合</li><li>这样之后一般会放到 .git文件之下的 ， rebase</li></ul><h2 id="忽略文件-1"><a href="#忽略文件-1" class="headerlink" title="忽略文件"></a>忽略文件</h2><ul><li><code>.gitignore</code>文件就是忽略文件</li></ul><h2 id="仓库备份"><a href="#仓库备份" class="headerlink" title="仓库备份"></a>仓库备份</h2><ul><li>git clone ： 就是克隆&#x2F;备份</li><li>克隆一个仓库到一个新的文件夹</li><li>git clone &lt;仓库&gt; &lt;名字&gt;</li><li>这个仓库是本地或者远程仓库</li><li>仓库 ： 可以接url，或者本地的.git文件</li><li>克隆到名字的文件夹里 ，如果么没有则创建</li><li>本地备份本地不常用</li><li>一般都是备份远程仓库</li><li>对于远程仓库的地址 ，则是为我们通过查看远程仓库的地址进行</li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/11/08/11-8/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
