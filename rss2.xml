<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>枫叶冢</title>
    <link>http://example.com/</link>
    
    <atom:link href="http://example.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>只有努力不会辜负你</description>
    <pubDate>Sun, 11 Dec 2022 00:59:56 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>云原生教学视频</title>
      <link>http://example.com/2022/12/10/12-08/</link>
      <guid>http://example.com/2022/12/10/12-08/</guid>
      <pubDate>Sat, 10 Dec 2022 03:38:03 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;云原生&quot;&gt;&lt;a href=&quot;#云原生&quot; class=&quot;headerlink&quot; title=&quot;云原生&quot;&gt;&lt;/a&gt;云原生&lt;/h1&gt;&lt;p&gt;理解 ： 理解上要把他们拆开理解会更好&lt;/p&gt;
&lt;p&gt;云 ：云基础设施（cloud）&lt;/p&gt;
&lt;p&gt;原生：native ：在云计算平</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="云原生"><a href="#云原生" class="headerlink" title="云原生"></a>云原生</h1><p>理解 ： 理解上要把他们拆开理解会更好</p><p>云 ：云基础设施（cloud）</p><p>原生：native ：在云计算平台里可以原生的计算和运行的</p><p>云原生的概念由来：<br>2013年被prvoyal公司的Ms提出</p><p>2015年谷歌带头成立了云原生的计算基金会</p><p>云原生的定义 ：</p><p>基于微服务原理而开发的应用，用容器的方式打包，在运行时，容器由运行于云基础设施之上的平台进行调度，应用开发采用持续交付和devOps实践</p><p>2015年：容器化封装+自动化管理+面向微服务</p><p>2018年：容器化封装+面向微服务+服务网格+声明格式API</p><p>云原生有利于各种组织在共有云，私有云和混合云等新动态环境中，构建和运行可扩展性的应用</p><p>微服务 ：把原有的单体应用拆分为多个独立自治的组件，每个组件都可以独立开发，设计，测试，运维，部署，这个组件可以单独的对外进行服务，我们称其为微服务</p><p>容器化：docker容器，容器属于it基础设施层概念，是比虚拟机更轻量化的隔离工具，是微服务的最佳载体</p><p>使用k8s的资源调度与容器编排，可以实现docker容器更优管理，进一步实现其PaaS能力</p><p>服务网格</p><p>服务网格存在的目的，就是中心化的服务治理框架</p><p>以往需要对微服务或者对api接口区做治理和管理请求</p><p>不可以改变基础设施指的是镜像：日后如果想再次改变他的部署，可以用镜像进行改变</p><p>应用部署：命令行：声明式</p><p>DevOps</p><p>借助云原生的相关技术，DevOps的时代才到来</p><p>云原生的最佳实现的实现三个层面</p><p>服务编排要实现计算资源弹性化</p><p>服务构建和部署要实现高可用</p><p>实践驱动基础设施标准初始化</p><p>云原生应用的领域</p><p>云原生的生态也已经覆盖到了，大数据，人工智能，边缘计算，区域局等领域</p><p>云原生的编排以及管理</p><p>编排与调度k8s</p><p>原生调用grpc</p><p>服务代理envoy</p><p>api网关apisix</p><p>服务网格istio</p><p>服务发现coreDns</p><p>消息和流式处理kafka</p><p>Severless ：只是对服务器的关心比较少，并不是完全无服务器</p><p>自动化配置：ansible</p><p>数据库：不赘述了</p><p>容器镜像仓库：harbor</p><p>定义及镜像制作：helm</p><p>密钥管理：spiffe</p><p>存储技术：ceph</p><p>网络技术：calico</p><p>监控分析：prometheus</p><p>等</p><h1 id="4步制作超级精简的大厂docker镜像"><a href="#4步制作超级精简的大厂docker镜像" class="headerlink" title="4步制作超级精简的大厂docker镜像"></a>4步制作超级精简的大厂docker镜像</h1><h2 id="什么是镜像"><a href="#什么是镜像" class="headerlink" title="什么是镜像"></a>什么是镜像</h2><p>镜像是：分层联合文件系统</p><p>一种轻量级，可执行的独立软件包</p><p>镜像大小：有大有小</p><p>曾经网易蜂巢logo镜像只有585B</p><p><code>docker pull hub.c.163.com/public/logo</code></p><p>精简docker镜像的优势</p><p>减少构建时间</p><p>减少磁盘使用量</p><p>减少下载时间</p><p>提高安全性</p><h2 id="镜像的分层原理"><a href="#镜像的分层原理" class="headerlink" title="镜像的分层原理"></a>镜像的分层原理</h2><p>doccker镜像的分层</p><p>第一层：本机的系统</p><p>第二层：镜像上安装的虚拟环境比如：python</p><p>第三层：打的补丁文件</p><p><code>docker pull busybox:latest</code></p><p>拉下来之后我们对它进行多层的镜像,进行演示一遍</p><p>拉下来之后随便找个地方创建个文件叫dockerfile</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">FROM busybox</span><br><span class="line">RUN mkdir /tmp/foo</span><br><span class="line">RUN dd if=/dev/zero of=/tmp/foo/bar bs=1048576 count=100</span><br><span class="line">RUN rm /tmp/foo/bar</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>上面这个是设置swap的交换分区的代码，count后面跟着的是字节数，bs是每秒的吞吐量</p><p>然后同步到docker容器上并执行这个文件 <code>docker build -t busybox:text . </code>这个语句的意思是根据本地镜像，加上我们的文本语句，进行创建我们的一个新的docker镜像，后面的.代表这个文件夹里所有的文本文件，也可以单独指明是哪一个文本文件</p><p>运行之前</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-8-16-centos dockerfile]# docker images | grep busybox</span><br><span class="line">busybox                     latest    334e4a014c81   4 days ago      4.86MB</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>运行之后</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-8-16-centos dockerfile]# docker images | grep busybox</span><br><span class="line">busybox                     text      efa9b412f2f7   4 minutes ago   110MB</span><br><span class="line">busybox                     latest    334e4a014c81   4 days ago      4.86MB</span><br></pre></td></tr></table></figure><p>这个新的镜像是基于我们之前的busybox进行创建的，而且执行了上面的分区文件</p><p>简单来说，一个命令就是一层</p><h2 id="制作精简镜像"><a href="#制作精简镜像" class="headerlink" title="制作精简镜像"></a>制作精简镜像</h2><h2 id="构建企业debian-10-基础测试镜像"><a href="#构建企业debian-10-基础测试镜像" class="headerlink" title="构建企业debian 10 基础测试镜像"></a>构建企业debian 10 基础测试镜像</h2>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E4%BA%91%E5%8E%9F%E7%94%9F%EF%BC%88%E5%93%94%E5%93%A9%E5%93%94%E5%93%A9%EF%BC%89/">云原生（哔哩哔哩）</category>
      
      
      
      <comments>http://example.com/2022/12/10/12-08/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>数据可视化</title>
      <link>http://example.com/2022/12/07/12-07/</link>
      <guid>http://example.com/2022/12/07/12-07/</guid>
      <pubDate>Wed, 07 Dec 2022 00:53:05 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;数据可视化的数据库选择&quot;&gt;&lt;a href=&quot;#数据可视化的数据库选择&quot; class=&quot;headerlink&quot; title=&quot;数据可视化的数据库选择&quot;&gt;&lt;/a&gt;数据可视化的数据库选择&lt;/h1&gt;&lt;p&gt;一般选择响应数据库比较快的一般是 秒级，或者毫秒级 ： 不要选择hi</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="数据可视化的数据库选择"><a href="#数据可视化的数据库选择" class="headerlink" title="数据可视化的数据库选择"></a>数据可视化的数据库选择</h1><p>一般选择响应数据库比较快的一般是 秒级，或者毫秒级 ： 不要选择hive</p><p>因为hive太慢</p><p>我们一般都把数据最后导入到mysql里</p><p>作业：</p><p>自己做一个dashboard</p><h1 id="xxl"><a href="#xxl" class="headerlink" title="xxl"></a>xxl</h1><p>定时任务调度</p><p>就是按照每天都要做的任务</p><ul><li>crontab 进行 用的比较少 而因为不方便</li><li>定时任务的调度的框架<ul><li>ozio , azkaban,airflow,xxl,dolphinscheduler</li><li>现在ozio 和 azkaban 因为操作比较反人类，所以不太推荐</li><li>airflow ： 通过python进行任务调度的</li><li>公司首选 dolphinscheduler ，其次 xxl</li></ul></li><li>针对 xxl 或者 dolphinscheduler 可以串联的方式进行执行调度，就是a任务完成，直接执行b任务等等</li><li>但是crontab它要设置时间间隔，不可以串联的方式进行执行</li><li>多任务之间的依赖关系 ：<ul><li>DAG 有向无环图</li></ul></li><li>xxl 官网 ： 国人开发的 <code>https://github.com/xuxueli/xxl-job</code><ul><li>架构：主从架构，分布式架构</li><li>老大：调度中心</li><li>小弟：调度器</li></ul></li><li>其他的都是apache的</li></ul><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>下载源码 ；</p><p>导入idea 进行编译</p><p>初始化“调度数据库”xxl源数据库 -》 mysql中</p><p>首先在mysql中执行语句</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line">#</span><br><span class="line"># XXL-JOB v2.4.0-SNAPSHOT</span><br><span class="line"># Copyright (c) 2015-present, xuxueli.</span><br><span class="line"></span><br><span class="line">CREATE database if NOT EXISTS `xxl_job` default character set utf8mb4 collate utf8mb4_unicode_ci;</span><br><span class="line">use `xxl_job`;</span><br><span class="line"></span><br><span class="line">SET NAMES utf8mb4;</span><br><span class="line"></span><br><span class="line">CREATE TABLE `xxl_job_info` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `job_group` int(11) NOT NULL COMMENT &#x27;执行器主键ID&#x27;,</span><br><span class="line">  `job_desc` varchar(255) NOT NULL,</span><br><span class="line">  `add_time` datetime DEFAULT NULL,</span><br><span class="line">  `update_time` datetime DEFAULT NULL,</span><br><span class="line">  `author` varchar(64) DEFAULT NULL COMMENT &#x27;作者&#x27;,</span><br><span class="line">  `alarm_email` varchar(255) DEFAULT NULL COMMENT &#x27;报警邮件&#x27;,</span><br><span class="line">  `schedule_type` varchar(50) NOT NULL DEFAULT &#x27;NONE&#x27; COMMENT &#x27;调度类型&#x27;,</span><br><span class="line">  `schedule_conf` varchar(128) DEFAULT NULL COMMENT &#x27;调度配置，值含义取决于调度类型&#x27;,</span><br><span class="line">  `misfire_strategy` varchar(50) NOT NULL DEFAULT &#x27;DO_NOTHING&#x27; COMMENT &#x27;调度过期策略&#x27;,</span><br><span class="line">  `executor_route_strategy` varchar(50) DEFAULT NULL COMMENT &#x27;执行器路由策略&#x27;,</span><br><span class="line">  `executor_handler` varchar(255) DEFAULT NULL COMMENT &#x27;执行器任务handler&#x27;,</span><br><span class="line">  `executor_param` varchar(512) DEFAULT NULL COMMENT &#x27;执行器任务参数&#x27;,</span><br><span class="line">  `executor_block_strategy` varchar(50) DEFAULT NULL COMMENT &#x27;阻塞处理策略&#x27;,</span><br><span class="line">  `executor_timeout` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;任务执行超时时间，单位秒&#x27;,</span><br><span class="line">  `executor_fail_retry_count` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;失败重试次数&#x27;,</span><br><span class="line">  `glue_type` varchar(50) NOT NULL COMMENT &#x27;GLUE类型&#x27;,</span><br><span class="line">  `glue_source` mediumtext COMMENT &#x27;GLUE源代码&#x27;,</span><br><span class="line">  `glue_remark` varchar(128) DEFAULT NULL COMMENT &#x27;GLUE备注&#x27;,</span><br><span class="line">  `glue_updatetime` datetime DEFAULT NULL COMMENT &#x27;GLUE更新时间&#x27;,</span><br><span class="line">  `child_jobid` varchar(255) DEFAULT NULL COMMENT &#x27;子任务ID，多个逗号分隔&#x27;,</span><br><span class="line">  `trigger_status` tinyint(4) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;调度状态：0-停止，1-运行&#x27;,</span><br><span class="line">  `trigger_last_time` bigint(13) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;上次调度时间&#x27;,</span><br><span class="line">  `trigger_next_time` bigint(13) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;下次调度时间&#x27;,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;</span><br><span class="line"></span><br><span class="line">CREATE TABLE `xxl_job_log` (</span><br><span class="line">  `id` bigint(20) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `job_group` int(11) NOT NULL COMMENT &#x27;执行器主键ID&#x27;,</span><br><span class="line">  `job_id` int(11) NOT NULL COMMENT &#x27;任务，主键ID&#x27;,</span><br><span class="line">  `executor_address` varchar(255) DEFAULT NULL COMMENT &#x27;执行器地址，本次执行的地址&#x27;,</span><br><span class="line">  `executor_handler` varchar(255) DEFAULT NULL COMMENT &#x27;执行器任务handler&#x27;,</span><br><span class="line">  `executor_param` varchar(512) DEFAULT NULL COMMENT &#x27;执行器任务参数&#x27;,</span><br><span class="line">  `executor_sharding_param` varchar(20) DEFAULT NULL COMMENT &#x27;执行器任务分片参数，格式如 1/2&#x27;,</span><br><span class="line">  `executor_fail_retry_count` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;失败重试次数&#x27;,</span><br><span class="line">  `trigger_time` datetime DEFAULT NULL COMMENT &#x27;调度-时间&#x27;,</span><br><span class="line">  `trigger_code` int(11) NOT NULL COMMENT &#x27;调度-结果&#x27;,</span><br><span class="line">  `trigger_msg` text COMMENT &#x27;调度-日志&#x27;,</span><br><span class="line">  `handle_time` datetime DEFAULT NULL COMMENT &#x27;执行-时间&#x27;,</span><br><span class="line">  `handle_code` int(11) NOT NULL COMMENT &#x27;执行-状态&#x27;,</span><br><span class="line">  `handle_msg` text COMMENT &#x27;执行-日志&#x27;,</span><br><span class="line">  `alarm_status` tinyint(4) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;告警状态：0-默认、1-无需告警、2-告警成功、3-告警失败&#x27;,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  KEY `I_trigger_time` (`trigger_time`),</span><br><span class="line">  KEY `I_handle_code` (`handle_code`)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;</span><br><span class="line"></span><br><span class="line">CREATE TABLE `xxl_job_log_report` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `trigger_day` datetime DEFAULT NULL COMMENT &#x27;调度-时间&#x27;,</span><br><span class="line">  `running_count` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;运行中-日志数量&#x27;,</span><br><span class="line">  `suc_count` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;执行成功-日志数量&#x27;,</span><br><span class="line">  `fail_count` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;执行失败-日志数量&#x27;,</span><br><span class="line">  `update_time` datetime DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  UNIQUE KEY `i_trigger_day` (`trigger_day`) USING BTREE</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;</span><br><span class="line"></span><br><span class="line">CREATE TABLE `xxl_job_logglue` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `job_id` int(11) NOT NULL COMMENT &#x27;任务，主键ID&#x27;,</span><br><span class="line">  `glue_type` varchar(50) DEFAULT NULL COMMENT &#x27;GLUE类型&#x27;,</span><br><span class="line">  `glue_source` mediumtext COMMENT &#x27;GLUE源代码&#x27;,</span><br><span class="line">  `glue_remark` varchar(128) NOT NULL COMMENT &#x27;GLUE备注&#x27;,</span><br><span class="line">  `add_time` datetime DEFAULT NULL,</span><br><span class="line">  `update_time` datetime DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;</span><br><span class="line"></span><br><span class="line">CREATE TABLE `xxl_job_registry` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `registry_group` varchar(50) NOT NULL,</span><br><span class="line">  `registry_key` varchar(255) NOT NULL,</span><br><span class="line">  `registry_value` varchar(255) NOT NULL,</span><br><span class="line">  `update_time` datetime DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  KEY `i_g_k_v` (`registry_group`,`registry_key`,`registry_value`)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;</span><br><span class="line"></span><br><span class="line">CREATE TABLE `xxl_job_group` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `app_name` varchar(64) NOT NULL COMMENT &#x27;执行器AppName&#x27;,</span><br><span class="line">  `title` varchar(12) NOT NULL COMMENT &#x27;执行器名称&#x27;,</span><br><span class="line">  `address_type` tinyint(4) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;执行器地址类型：0=自动注册、1=手动录入&#x27;,</span><br><span class="line">  `address_list` text COMMENT &#x27;执行器地址列表，多地址逗号分隔&#x27;,</span><br><span class="line">  `update_time` datetime DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;</span><br><span class="line"></span><br><span class="line">CREATE TABLE `xxl_job_user` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `username` varchar(50) NOT NULL COMMENT &#x27;账号&#x27;,</span><br><span class="line">  `password` varchar(50) NOT NULL COMMENT &#x27;密码&#x27;,</span><br><span class="line">  `role` tinyint(4) NOT NULL COMMENT &#x27;角色：0-普通用户、1-管理员&#x27;,</span><br><span class="line">  `permission` varchar(255) DEFAULT NULL COMMENT &#x27;权限：执行器ID列表，多个逗号分割&#x27;,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  UNIQUE KEY `i_username` (`username`) USING BTREE</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;</span><br><span class="line"></span><br><span class="line">CREATE TABLE `xxl_job_lock` (</span><br><span class="line">  `lock_name` varchar(50) NOT NULL COMMENT &#x27;锁名称&#x27;,</span><br><span class="line">  PRIMARY KEY (`lock_name`)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;</span><br><span class="line"></span><br><span class="line">INSERT INTO `xxl_job_group`(`id`, `app_name`, `title`, `address_type`, `address_list`, `update_time`) VALUES (1, &#x27;xxl-job-executor-sample&#x27;, &#x27;示例执行器&#x27;, 0, NULL, &#x27;2018-11-03 22:21:31&#x27; );</span><br><span class="line">INSERT INTO `xxl_job_info`(`id`, `job_group`, `job_desc`, `add_time`, `update_time`, `author`, `alarm_email`, `schedule_type`, `schedule_conf`, `misfire_strategy`, `executor_route_strategy`, `executor_handler`, `executor_param`, `executor_block_strategy`, `executor_timeout`, `executor_fail_retry_count`, `glue_type`, `glue_source`, `glue_remark`, `glue_updatetime`, `child_jobid`) VALUES (1, 1, &#x27;测试任务1&#x27;, &#x27;2018-11-03 22:21:31&#x27;, &#x27;2018-11-03 22:21:31&#x27;, &#x27;XXL&#x27;, &#x27;&#x27;, &#x27;CRON&#x27;, &#x27;0 0 0 * * ? *&#x27;, &#x27;DO_NOTHING&#x27;, &#x27;FIRST&#x27;, &#x27;demoJobHandler&#x27;, &#x27;&#x27;, &#x27;SERIAL_EXECUTION&#x27;, 0, 0, &#x27;BEAN&#x27;, &#x27;&#x27;, &#x27;GLUE代码初始化&#x27;, &#x27;2018-11-03 22:21:31&#x27;, &#x27;&#x27;);</span><br><span class="line">INSERT INTO `xxl_job_user`(`id`, `username`, `password`, `role`, `permission`) VALUES (1, &#x27;admin&#x27;, &#x27;e10adc3949ba59abbe56e057f20f883e&#x27;, 1, NULL);</span><br><span class="line">INSERT INTO `xxl_job_lock` ( `lock_name`) VALUES ( &#x27;schedule_lock&#x27;);</span><br><span class="line"></span><br><span class="line">commit;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>然后再把从GitHub上下载的文件夹用idea打开进行编译</p><p>进行配置我们的web端口以及数据库：在application.properties文件中，然后配置一下</p><p>配置好之后maven，之间打成jar包，然后上传到linux服务器上</p><p>运行java -jar 上传的文件的路径</p><p>然后会报错，就创建个文件夹就好了 <code>mkdir -p /data/applogs/xxl-job</code></p><p>使用su 进行用户切换</p><p>然后通过chown 进行修改组以及用户 <code>chown -R hadoop:hadoop /data</code></p><p>然后再次运行就可以了</p><p>然后在调度器管理页面添加调度器，然后分配任务就可以了</p><h1 id="钉钉报警"><a href="#钉钉报警" class="headerlink" title="钉钉报警"></a>钉钉报警</h1><p>钉钉机器人可发送的类型</p><ul><li>文本</li><li>链接</li><li>markdown</li><li>actioncard</li><li>feedcard</li></ul><p>weget  ： 从互联网上下载的时候用的 ： 下载安装包 ，但是占用网络资源较大，且会一直重复下载直到结果成功，所以占用的资源较大</p><p>curl ： 发送请求的，发送网页请求访问的，也可以进行下载</p><ul><li>-o ： 把访问的一个页面存储到文件里</li></ul><p>机器人发送消息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl &#x27;机器人的token&#x27;</span><br><span class="line">-H &#x27;Content-type:application/json&#x27;</span><br><span class="line">-d &#x27;&#123;&quot;msgtype&quot; : text&#125;&#x27;</span><br></pre></td></tr></table></figure><p>需求 ： </p><ul><li>日志数据 ： hdfs 上 linux user_click.log</li><li>例子： u01,鼠标,ios</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">u01,鼠标,ios</span><br><span class="line">u01,鼠标,ios</span><br><span class="line">u01,鼠标,ios</span><br><span class="line">u01,鼠标,ios</span><br><span class="line">u01,鼠标,ios</span><br><span class="line">u02,键盘,android</span><br><span class="line">u02,键盘,android</span><br><span class="line">u02,键盘,android</span><br><span class="line">u02,键盘,android</span><br><span class="line">u03,显示器,ios</span><br><span class="line">u04,托特包,ios</span><br></pre></td></tr></table></figure><ul><li>业务数据 ：mysql user_info</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">u01,子航</span><br><span class="line">u02,祖安</span><br><span class="line">u03,海哥</span><br><span class="line">u04,轩轩</span><br></pre></td></tr></table></figure><p>统计：</p><p>uid ， name ，sku ， os 每个用户点击商品的次数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select bianhao,shop_name,caozuoxit,count(*) as cishu from user_click1 group by bianhao,shop_name,caozuoxit;</span><br></pre></td></tr></table></figure><p>取出表中重复数据，的次数做个排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select shop_name,caozuoxit,name,row_number() over (partition by name) as rm from (</span><br><span class="line">select * from user_click1 left join user_info on user_click1.bianhao=user_info.bianhao</span><br><span class="line">) as king</span><br></pre></td></tr></table></figure><p>不重复字段标识为1</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">u01,子航,鼠标,ios 1</span><br><span class="line">u01,子航,鼠标,ios 2</span><br><span class="line">u01,子航,鼠标,ios 3</span><br><span class="line">u04,托特包,ios 1</span><br></pre></td></tr></table></figure><p>统计表中不重复的数据，一起做排序，但是对于重复数据它还是对自己排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">select name,caozuoxit,shop_name,row_number() over(partition by cishu) from (</span><br><span class="line">select * from user_click1 left join (</span><br><span class="line">select bianhao,count(*) as cishu from user_click1 group by bianhao,shop_name,caozuoxit</span><br><span class="line">) as count_click on count_click.bianhao=user_click1.bianhao</span><br><span class="line">left join user_info on count_click.bianhao=user_info.bianhao  </span><br><span class="line">) as ds;</span><br><span class="line">上述是取巧的方法</span><br><span class="line">下面是正经的方法</span><br><span class="line">select count_clickbianhao,name,caozuoxit,shop_name,row_number() over(order by cishu) as rm from (</span><br><span class="line">select * from user_click1 left join (</span><br><span class="line">select bianhao as count_clickbianhao,count(*) as cishu from user_click1 group by bianhao,shop_name,caozuoxit</span><br><span class="line">) as count_click on count_click.count_clickbianhao=user_click1.bianhao </span><br><span class="line">left join user_info on count_click.count_clickbianhao=user_info.bianhao  </span><br><span class="line">) as jj where cishu = 1 </span><br><span class="line">union all</span><br><span class="line">select count_clickbianhao,name,caozuoxit,shop_name,row_number() over(partition by (name,caozuoxit,shop_name,cishu)) from (</span><br><span class="line">select * from user_click1 left join (</span><br><span class="line">select bianhao as count_clickbianhao,count(*) as cishu from user_click1 group by bianhao,shop_name,caozuoxit</span><br><span class="line">) as count_click on count_click.count_clickbianhao=user_click1.bianhao </span><br><span class="line">left join user_info on count_click.count_clickbianhao=user_info.bianhao  </span><br><span class="line">) as j where cishu != 1;</span><br></pre></td></tr></table></figure><p>整个流程使用xxl进行调度</p><p>最后结果导入到mysql</p><p>数据导入mysql保证幂等性</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/12/07/12-07/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>docker</title>
      <link>http://example.com/2022/12/07/docker/</link>
      <guid>http://example.com/2022/12/07/docker/</guid>
      <pubDate>Wed, 07 Dec 2022 00:23:07 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;这个是关于docker的简单介绍以及使用&lt;/p&gt;
&lt;p&gt;本来这个我其实不打算写的因为网上关于docker的教程很多，而且都比较全，我目前所学</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>这个是关于docker的简单介绍以及使用</p><p>本来这个我其实不打算写的因为网上关于docker的教程很多，而且都比较全，我目前所学的全部都是基于菜鸟教程的</p><h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>首先我们要明白，什么是docker</p><p>docker就是相当于一个箱子，其里面有它自己的生态圈</p><p>各种环境依赖是直接现成的那样，和之前java打包成exe文件后面绑定依赖是一样的</p><h1 id="为什么现在docker会会很火"><a href="#为什么现在docker会会很火" class="headerlink" title="为什么现在docker会会很火"></a>为什么现在docker会会很火</h1><p>因为docker不需要我们配置复杂的环境变量，只要我们通过网络下载一个包含这个功能的linux或者unbanto镜像就行</p><p>特别方便，不过方便的同时也会带来隐患，比如，不知道原生安装的话，我们如何详细的知道这个组件的功能？</p><h2 id="docker的安装"><a href="#docker的安装" class="headerlink" title="docker的安装"></a>docker的安装</h2><p>docker支持多种操作系统的安装，以下我只简单介绍关于linux和云服务器的安装方法</p><h3 id="linux"><a href="#linux" class="headerlink" title="linux"></a>linux</h3><p>使用官方命令安装</p><p><code>curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun</code></p><p>也可以用国内的daocloud安装</p><p><code>curl -sSL https://get.daocloud.io/docker | sh</code></p><p>当执行安装命令出现以下情况报错的时候</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Executing docker install script, commit: 4f282167c425347a931ccfd95cc91fab041d414f</span><br><span class="line">+ sh -c &#x27;yum install -y -q yum-utils&#x27;</span><br><span class="line">error: rpmdb: BDB0113 Thread/process 16675/139942115395648 failed: BDB1507 Thread died in Berkeley DB library</span><br><span class="line">error: db5 error(-30973) from dbenv-&gt;failchk: BDB0087 DB_RUNRECOVERY: Fatal error, run database recovery</span><br><span class="line">error: cannot open Packages index using db5 -  (-30973)</span><br><span class="line">error: cannot open Packages database in /var/lib/rpm</span><br><span class="line">CRITICAL:yum.main:</span><br><span class="line"></span><br><span class="line">Error: rpmdb open failed</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>执行 <code>mv /var/lib/rpm/__db.00* /tmp/&amp;&amp;yum clean all</code></p><p>再执行安装命令就可以了</p><p>这样在有网的机器上就安装完成了，是不是很简单 ，</p><p>接下来我们要说手动安装的情况</p><p>首先要卸载旧版本</p><p>较旧的 Docker 版本称为 docker 或 docker-engine 。如果已安装这些程序，请卸载它们以及相关的依赖项。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum remove docker \</span><br><span class="line">                  docker-client \</span><br><span class="line">                  docker-client-latest \</span><br><span class="line">                  docker-common \</span><br><span class="line">                  docker-latest \</span><br><span class="line">                  docker-latest-logrotate \</span><br><span class="line">                  docker-logrotate \</span><br><span class="line">                  docker-engine</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在新主机上首次安装 Docker Engine-Community 之前，需要设置 Docker 仓库。之后，您可以从仓库安装和更新 Docker。</p><p>安装所需的软件包。yum-utils 提供了 yum-config-manager ，并且 device mapper 存储驱动程序需要 device-mapper-persistent-data 和 lvm2。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install -y yum-utils \</span><br><span class="line">  device-mapper-persistent-data \</span><br><span class="line">  lvm2</span><br></pre></td></tr></table></figure><p>使用以下命令来设置稳定的仓库。</p><p>官方地址源</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum-config-manager \</span><br><span class="line">    --add-repo \</span><br><span class="line">    https://download.docker.com/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure><p>阿里云地址源</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum-config-manager \</span><br><span class="line">    --add-repo \</span><br><span class="line">    http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure><p>清华大学的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum-config-manager \</span><br><span class="line">    --add-repo \</span><br><span class="line">    https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure><p>在国内还是建议阿里和清华大学的</p><p>安装最新版本的 Docker Engine-Community 和 containerd，或者转到下一步安装特定版本：</p><p><code>$ sudo yum install docker-ce docker-ce-cli containerd.io docker-compose-plugin</code></p><p>如果提示您接受 GPG 密钥，请选是。</p><p>Docker 安装完默认未启动。并且已经创建好 docker 用户组，但该用户组下没有用户。</p><p><strong>要安装特定版本的 Docker Engine-Community，请在存储库中列出可用版本，然后选择并安装：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ yum list docker-ce --showduplicates | sort -r</span><br><span class="line"></span><br><span class="line">docker-ce.x86_64  3:18.09.1-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64  3:18.09.0-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64  18.06.1.ce-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64  18.06.0.ce-3.el7                    docker-ce-stable</span><br></pre></td></tr></table></figure><p>通过其完整的软件包名称安装特定版本，该软件包名称是软件包名称（docker-ce）加上版本字符串（第二列），从第一个冒号（:）一直到第一个连字符，并用连字符（-）分隔。例如：docker-ce-18.09.1。</p><p><code>$ sudo yum install docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io</code></p><p>然后启动docker</p><p><code>$ sudo systemctl start docker</code></p><p>然后运行hello world镜像查看是不是我们成功安装了这个docker</p><p><code>$ sudo docker run hello-world</code></p><h3 id="卸载docker"><a href="#卸载docker" class="headerlink" title="卸载docker"></a>卸载docker</h3><p>先删除安装包</p><p><code>yum remove docker-ce </code></p><p>然后删除镜像文件等</p><p><code>rm -rf /var/lib/docker</code></p><p>云服务器和上面一样</p><h1 id="docker-命令"><a href="#docker-命令" class="headerlink" title="docker 命令"></a>docker 命令</h1><p>docker命令的种类不多但是其中的分支较多</p><p>docker run : 原本的意义是创建一个docker容器，并运行它</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">-a stdin: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项；</span><br><span class="line"></span><br><span class="line">-d: 后台运行容器，并返回容器ID；</span><br><span class="line"></span><br><span class="line">-i: 以交互模式运行容器，通常与 -t 同时使用；</span><br><span class="line"></span><br><span class="line">-P: 随机端口映射，容器内部端口随机映射到主机的端口</span><br><span class="line"></span><br><span class="line">-p: 指定端口映射，格式为：主机(宿主)端口:容器端口</span><br><span class="line"></span><br><span class="line">-t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用；</span><br><span class="line"></span><br><span class="line">--name=&quot;nginx-lb&quot;: 为容器指定一个名称；</span><br><span class="line"></span><br><span class="line">--dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致；</span><br><span class="line"></span><br><span class="line">--dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致；</span><br><span class="line"></span><br><span class="line">-h &quot;mars&quot;: 指定容器的hostname；</span><br><span class="line"></span><br><span class="line">-e username=&quot;ritchie&quot;: 设置环境变量；</span><br><span class="line"></span><br><span class="line">--env-file=[]: 从指定文件读入环境变量；</span><br><span class="line"></span><br><span class="line">--cpuset=&quot;0-2&quot; or --cpuset=&quot;0,1,2&quot;: 绑定容器到指定CPU运行；</span><br><span class="line"></span><br><span class="line">-m :设置容器使用内存最大值；</span><br><span class="line"></span><br><span class="line">--net=&quot;bridge&quot;: 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型；</span><br><span class="line"></span><br><span class="line">--link=[]: 添加链接到另一个容器；</span><br><span class="line"></span><br><span class="line">--expose=[]: 开放一个端口或一组端口；</span><br><span class="line"></span><br><span class="line">--volume , -v: 绑定一个卷</span><br></pre></td></tr></table></figure><p><strong>docker start</strong> :启动一个或多个已经被停止的容器</p><p><strong>docker stop</strong> :停止一个运行中的容器</p><p><strong>docker restart</strong> :重启容器</p><p><strong>docker kill</strong> :杀掉一个运行中的容器。</p><ul><li><strong>-s :</strong> 向容器发送一个信号</li></ul><p><strong>docker rm ：</strong> 删除一个或多个容器。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-f :通过 SIGKILL 信号强制删除一个运行中的容器。</span><br><span class="line"></span><br><span class="line">-l :移除容器间的网络连接，而非容器本身。</span><br><span class="line"></span><br><span class="line">-v :删除与容器关联的卷。</span><br></pre></td></tr></table></figure><ul><li>命令可以嵌套使用如下 ：<ul><li><code>删除所有已经停止的容器：docker rm $(docker ps -a -q)</code></li></ul></li></ul><p><strong>docker pause</strong> :暂停容器中所有的进程。</p><p><strong>docker unpause</strong> :恢复容器中所有的进程。</p><p><strong>docker create ：</strong> 创建一个新的容器但不启动它 ：其语法和run一样</p><p><strong>docker exec ：</strong> 在运行的容器中执行命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-d :分离模式: 在后台运行</span><br><span class="line"></span><br><span class="line">-i :即使没有附加也保持STDIN 打开</span><br><span class="line"></span><br><span class="line">-t :分配一个伪终端</span><br></pre></td></tr></table></figure><p>docker ps : 列出容器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">-a :显示所有的容器，包括未运行的。</span><br><span class="line"></span><br><span class="line">-f :根据条件过滤显示的内容。</span><br><span class="line"></span><br><span class="line">--format :指定返回值的模板文件。</span><br><span class="line"></span><br><span class="line">-l :显示最近创建的容器。</span><br><span class="line"></span><br><span class="line">-n :列出最近创建的n个容器。</span><br><span class="line"></span><br><span class="line">--no-trunc :不截断输出。</span><br><span class="line"></span><br><span class="line">-q :静默模式，只显示容器编号。</span><br><span class="line"></span><br><span class="line">-s :显示总的文件大小。</span><br></pre></td></tr></table></figure><p>输出介绍</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">输出详情介绍：</span><br><span class="line"></span><br><span class="line">CONTAINER ID: 容器 ID。</span><br><span class="line"></span><br><span class="line">IMAGE: 使用的镜像。</span><br><span class="line"></span><br><span class="line">COMMAND: 启动容器时运行的命令。</span><br><span class="line"></span><br><span class="line">CREATED: 容器的创建时间。</span><br><span class="line"></span><br><span class="line">STATUS: 容器状态。</span><br><span class="line"></span><br><span class="line">状态有7种：</span><br><span class="line"></span><br><span class="line">created（已创建）</span><br><span class="line">restarting（重启中）</span><br><span class="line">running（运行中）</span><br><span class="line">removing（迁移中）</span><br><span class="line">paused（暂停）</span><br><span class="line">exited（停止）</span><br><span class="line">dead（死亡）</span><br><span class="line">PORTS: 容器的端口信息和使用的连接类型（tcp\udp）。</span><br><span class="line"></span><br><span class="line">NAMES: 自动分配的容器名称。</span><br></pre></td></tr></table></figure><p><strong>docker inspect :</strong> 获取容器&#x2F;镜像的元数据。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-f :指定返回值的模板文件。</span><br><span class="line"></span><br><span class="line">-s :显示总的文件大小。</span><br><span class="line"></span><br><span class="line">--type :为指定类型返回JSON。</span><br></pre></td></tr></table></figure><p><strong>docker top :</strong> 查看容器中运行的进程信息，支持 ps 命令参数。</p><p><code>docker top [OPTIONS] CONTAINER [ps OPTIONS]</code></p><ul><li>查看所有运行容器的进程信息。</li><li><code>for i in  </code>docker ps |grep Up|awk ‘{print $1}’<code>;do echo \ &amp;&amp;docker top $i; done</code></li></ul><p><strong>docker attach :</strong> 连接到正在运行中的容器。</p><p>要attach上去的容器必须正在运行，可以同时连接上同一个container来共享屏幕（与screen命令的attach类似）。</p><p>docker events : 从服务器获取实时事件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-f ：根据条件过滤事件；</span><br><span class="line"></span><br><span class="line">--since ：从指定的时间戳后显示所有事件;</span><br><span class="line"></span><br><span class="line">--until ：流水时间显示到指定的时间为止；</span><br><span class="line"></span><br><span class="line">如果指定的时间是到秒级的，需要将时间转成时间戳。如果时间为日期的话，可以直接使用，如--since=&quot;2016-07-01&quot;。</span><br></pre></td></tr></table></figure><p>docker logs : 获取容器的日志</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-f : 跟踪日志输出</span><br><span class="line"></span><br><span class="line">--since :显示某个开始时间的所有日志</span><br><span class="line"></span><br><span class="line">-t : 显示时间戳</span><br><span class="line"></span><br><span class="line">--tail :仅列出最新N条容器日志</span><br></pre></td></tr></table></figure><p><strong>docker wait :</strong> 阻塞运行直到容器停止，然后打印出它的退出代码</p><p><strong>docker export :</strong> 将文件系统作为一个tar归档文件导出到STDOUT。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-o :将输入内容写到文件。</span><br></pre></td></tr></table></figure><p>docker port 用于列出指定的容器的端口映射，或者查找将 PRIVATE_PORT NAT 到面向公众的端口。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker port [OPTIONS] CONTAINER [PRIVATE_PORT[/PROTO]]</span><br></pre></td></tr></table></figure><p>docker stats : 显示容器资源的使用情况，包括：CPU、内存、网络 I&#x2F;O 等。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--all , -a :显示所有的容器，包括未运行的。</span><br><span class="line"></span><br><span class="line">--format :指定返回值的模板文件。</span><br><span class="line"></span><br><span class="line">--no-stream :展示当前状态就直接退出了，不再实时更新。</span><br><span class="line"></span><br><span class="line">--no-trunc :不截断输出。</span><br></pre></td></tr></table></figure><p><strong>docker commit :</strong> 从容器创建一个新的镜像。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-a :提交的镜像作者；</span><br><span class="line"></span><br><span class="line">-c :使用Dockerfile指令来创建镜像；</span><br><span class="line"></span><br><span class="line">-m :提交时的说明文字；</span><br><span class="line"></span><br><span class="line">-p :在commit时，将容器暂停。</span><br></pre></td></tr></table></figure><p><strong>docker cp :</strong> 用于容器与主机之间的数据拷贝。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">-L :保持源目标中的链接</span><br><span class="line">docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|-</span><br><span class="line">docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH</span><br><span class="line">例子 ：</span><br><span class="line">实例</span><br><span class="line">将主机/www/runoob目录拷贝到容器96f7f14e99ab的/www目录下。</span><br><span class="line"></span><br><span class="line">docker cp /www/runoob 96f7f14e99ab:/www/</span><br><span class="line">将主机/www/runoob目录拷贝到容器96f7f14e99ab中，目录重命名为www。</span><br><span class="line"></span><br><span class="line">docker cp /www/runoob 96f7f14e99ab:/www</span><br><span class="line">将容器96f7f14e99ab的/www目录拷贝到主机的/tmp目录中。</span><br><span class="line"></span><br><span class="line">docker cp  96f7f14e99ab:/www /tmp/</span><br></pre></td></tr></table></figure><p>docker diff : 检查容器里文件结构的更改。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker diff [OPTIONS] CONTAINER</span><br></pre></td></tr></table></figure><p><strong>docker login :</strong> 登陆到一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub</p><p><strong>docker logout :</strong> 登出一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker login [OPTIONS] [SERVER]</span><br><span class="line">docker logout [OPTIONS] [SERVER]</span><br><span class="line">-u :登陆的用户名</span><br><span class="line"></span><br><span class="line">-p :登陆的密码</span><br></pre></td></tr></table></figure><p>docker pull : 从镜像仓库中拉取或者更新指定镜像</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-a :拉取所有 tagged 镜像</span><br><span class="line">docker pull [OPTIONS] NAME[:TAG|@DIGEST]</span><br><span class="line">--disable-content-trust :忽略镜像的校验,默认开启</span><br></pre></td></tr></table></figure><p>docker push : 将本地的镜像上传到镜像仓库,要先登陆到镜像仓库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--disable-content-trust :忽略镜像的校验,默认开启</span><br><span class="line">docker push [OPTIONS] NAME[:TAG]</span><br></pre></td></tr></table></figure><p><strong>docker search :</strong> 从Docker Hub查找镜像</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker search [OPTIONS] TERM</span><br><span class="line">--automated :只列出 automated build类型的镜像；</span><br><span class="line"></span><br><span class="line">--no-trunc :显示完整的镜像描述；</span><br><span class="line"></span><br><span class="line">-f &lt;过滤条件&gt;:列出收藏数不小于指定值的镜像。</span><br></pre></td></tr></table></figure><p>docker images : 列出本地镜像。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">docker images [OPTIONS] [REPOSITORY[:TAG]]</span><br><span class="line">-a :列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层）；</span><br><span class="line"></span><br><span class="line">--digests :显示镜像的摘要信息；</span><br><span class="line"></span><br><span class="line">-f :显示满足条件的镜像；</span><br><span class="line"></span><br><span class="line">--format :指定返回值的模板文件；</span><br><span class="line"></span><br><span class="line">--no-trunc :显示完整的镜像信息；</span><br><span class="line"></span><br><span class="line">-q :只显示镜像ID。</span><br></pre></td></tr></table></figure><p>docker rmi : 删除本地一个或多个镜像。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker rmi [OPTIONS] IMAGE [IMAGE...]</span><br><span class="line">-f :强制删除；</span><br><span class="line"></span><br><span class="line">--no-prune :不移除该镜像的过程镜像，默认移除；</span><br></pre></td></tr></table></figure><p>docker tag : 标记本地镜像，将其归入某一仓库。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]</span><br></pre></td></tr></table></figure><p>docker build 命令用于使用 Dockerfile 创建镜像</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">docker build [OPTIONS] PATH | URL | -</span><br><span class="line">--build-arg=[] :设置镜像创建时的变量；</span><br><span class="line"></span><br><span class="line">--cpu-shares :设置 cpu 使用权重；</span><br><span class="line"></span><br><span class="line">--cpu-period :限制 CPU CFS周期；</span><br><span class="line"></span><br><span class="line">--cpu-quota :限制 CPU CFS配额；</span><br><span class="line"></span><br><span class="line">--cpuset-cpus :指定使用的CPU id；</span><br><span class="line"></span><br><span class="line">--cpuset-mems :指定使用的内存 id；</span><br><span class="line"></span><br><span class="line">--disable-content-trust :忽略校验，默认开启；</span><br><span class="line"></span><br><span class="line">-f :指定要使用的Dockerfile路径；</span><br><span class="line"></span><br><span class="line">--force-rm :设置镜像过程中删除中间容器；</span><br><span class="line"></span><br><span class="line">--isolation :使用容器隔离技术；</span><br><span class="line"></span><br><span class="line">--label=[] :设置镜像使用的元数据；</span><br><span class="line"></span><br><span class="line">-m :设置内存最大值；</span><br><span class="line"></span><br><span class="line">--memory-swap :设置Swap的最大值为内存+swap，&quot;-1&quot;表示不限swap；</span><br><span class="line"></span><br><span class="line">--no-cache :创建镜像的过程不使用缓存；</span><br><span class="line"></span><br><span class="line">--pull :尝试去更新镜像的新版本；</span><br><span class="line"></span><br><span class="line">--quiet, -q :安静模式，成功后只输出镜像 ID；</span><br><span class="line"></span><br><span class="line">--rm :设置镜像成功后删除中间容器；</span><br><span class="line"></span><br><span class="line">--shm-size :设置/dev/shm的大小，默认值是64M；</span><br><span class="line"></span><br><span class="line">--ulimit :Ulimit配置。</span><br><span class="line"></span><br><span class="line">--squash :将 Dockerfile 中所有的操作压缩为一层。</span><br><span class="line"></span><br><span class="line">--tag, -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。</span><br><span class="line"></span><br><span class="line">--network: 默认 default。在构建期间设置RUN指令的网络模式</span><br></pre></td></tr></table></figure><p>docker history : 查看指定镜像的创建历史。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker history [OPTIONS] IMAGE</span><br><span class="line">-H :以可读的格式打印镜像大小和日期，默认为true；</span><br><span class="line"></span><br><span class="line">--no-trunc :显示完整的提交记录；</span><br><span class="line"></span><br><span class="line">-q :仅列出提交记录ID。</span><br></pre></td></tr></table></figure><p>docker save : 将指定镜像保存成 tar 归档文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker save [OPTIONS] IMAGE [IMAGE...]</span><br><span class="line">-o :输出到的文件。</span><br></pre></td></tr></table></figure><p>docker load : 导入使用 docker save 命令导出的镜像。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker load [OPTIONS]</span><br><span class="line">--input , -i : 指定导入的文件，代替 STDIN。</span><br><span class="line"></span><br><span class="line">--quiet , -q : 精简输出信息。</span><br></pre></td></tr></table></figure><p><strong>docker import :</strong> 从归档文件中创建镜像。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker import [OPTIONS] file|URL|- [REPOSITORY[:TAG]]</span><br><span class="line">-c :应用docker 指令创建镜像；</span><br><span class="line"></span><br><span class="line">-m :提交时的说明文字；</span><br></pre></td></tr></table></figure><p>docker info : 显示 Docker 系统信息，包括镜像和容器数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker info [OPTIONS]</span><br></pre></td></tr></table></figure><p>docker version :显示 Docker 版本信息。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-f :指定返回值的模板文件。</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%9D%82%E8%B4%A7%E6%8A%80%E6%9C%AF%E6%A0%88/">杂货技术栈</category>
      
      
      
      <comments>http://example.com/2022/12/07/docker/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>项目可视化框架</title>
      <link>http://example.com/2022/12/06/12-06/</link>
      <guid>http://example.com/2022/12/06/12-06/</guid>
      <pubDate>Tue, 06 Dec 2022 00:48:06 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;superset&quot;&gt;&lt;a href=&quot;#superset&quot; class=&quot;headerlink&quot; title=&quot;superset&quot;&gt;&lt;/a&gt;superset&lt;/h1&gt;&lt;p&gt;官网 ： superset ： &lt;a href=&quot;https://superset.apac</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="superset"><a href="#superset" class="headerlink" title="superset"></a>superset</h1><p>官网 ： superset ： <a href="https://superset.apache.org/">https://superset.apache.org/</a></p><p>类似于otb ： 开箱即用</p><p>把图弄到一个dashboard中 ，显示出来</p><p>底层源码  ： python编译的 ： 建议先安装python 然后再安装它 ，不要把superset和mysql在一起</p><p>因为原生的superset，需要一个和mysql冲突的包</p><p>但是docker还是可以的</p><p>先安装python环境</p><p>anconda -》 python</p><p>python原生 ： 建议</p><h2 id="docker安装"><a href="#docker安装" class="headerlink" title="docker安装"></a>docker安装</h2><p>毫无疑问docker安装是最快速的而且，不用担心依赖等</p><p>docker安装的步骤如下  ：</p><p>先安装yarn工具集</p><p><code>yum -y install yum-utils</code></p><p>然后把docker源添加到镜像里</p><p><code>yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</code></p><p>我这里添加的是阿里的镜像源</p><p>然后更新软件包索引</p><p><code>yum makecache fast</code></p><p>接下来我们就要开始安装docker了</p><p><code>yum -y install docker-ce docker-ce-cli containerd.io</code></p><p>设置一下docker开机自启</p><p><code>systemctl enable docker</code></p><p>然后我们启动docker</p><p><code>systemctl start docker</code></p><p>我们搜索superset镜像</p><p><code>docker search superset</code></p><p>直接拉去自己想要的版本 ，我这里拉去的是0.37.2的</p><p><code>docker pull amancevice/superset:0.37.2</code></p><p>接下来我们要创建存储superset配置文件及数据文件的文件夹</p><p><code>mkdir -p /opt/module/docker/superset/conf </code></p><p><code>mkdir -p /opt/module/docker/superset/data</code></p><p>接下来我们要创建superset的容器，并把端口映射出来</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name superset -u 0 -d -p 8088:8088 -v /opt/module/docker/superset/conf:/etc/superset -v /opt/module/docker/superset/data:/var/lib/superset amancevice/superset:0.37.2</span><br></pre></td></tr></table></figure><p>然后初始化我们的superset数据库</p><p><code>docker exec -it superset superset db upgrade</code></p><p>创建superset的管理员账号</p><p><code>docker exec -it superset superset fab create-admin</code></p><p>创建成功之后可以对其进行初始化了</p><p><code>docker exec -it superset superset init </code></p><p>最后执行开启服务</p><p><code>docker exec -it superset superset run --with-threads --reload --debugger</code></p><p>就可以啦 ，我们可以通过web页面 ip:8088访问 因为我们映射的端口是8088嘛</p><p>但是要注意一点，就是我们在我们的superset添加数据库的时候不能用修改了host里的别名进行IP替代</p><p>因为我们的superset是安装在我们的docker里的哪里的host并没有进行修改，识别不了别名，我们可以对其进行修改，但是嫌麻烦可以直接弄个ip</p><p>接下来我们简单弄个启动脚本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">function check_log_dir()</span><br><span class="line">&#123;</span><br><span class="line">    SUPERSET_LOG_DIR=/usr/local/src/superset/logs</span><br><span class="line">    if [ ! -d $SUPERSET_LOG_DIR ]</span><br><span class="line">    then</span><br><span class="line">        mkdir -p $SUPERSET_LOG_DIR</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">function superset_start()</span><br><span class="line">&#123;</span><br><span class="line">    cmd=&quot;docker start superset;nohup docker exec -it superset superset run --with-threads --reload --debugger &gt;$SUPERSET_LOG_DIR/superset.log 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">    eval $cmd || echo &quot;superset服务已启动&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">function superset_stop()</span><br><span class="line">&#123;</span><br><span class="line">   docker stop superset</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">function superset_status()</span><br><span class="line">&#123;</span><br><span class="line">  docker ps</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">     check_log_dir</span><br><span class="line">     superset_start</span><br><span class="line">    ;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">     superset_stop</span><br><span class="line">    ;;</span><br><span class="line">&quot;status&quot;)</span><br><span class="line">      superset_status</span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    echo Invalid Args!</span><br><span class="line">    echo &#x27;Usage: &#x27;$(basename $0)&#x27; start|stop|status&#x27;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h2 id="原生安装"><a href="#原生安装" class="headerlink" title="原生安装"></a>原生安装</h2><p>superset的原生安装是有坑的 ，以下操作只能是root用</p><p>在安装superset的时候容易出现gcc的问题，解决方法就是一直重新安装那一步</p><p>下面我们一起来进行原生安装</p><p>首先我们要安装python3的一些依赖 <code>yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel</code></p><p>然后我们要进行安装EPEL源并安装superset必备的包</p><p><code>yum install epel-release #安装epel源</code></p><p><code>yum install mysql-devel #安装MySQL开发包，属于pymysqlclient依赖</code></p><p><code>yum install gcc gcc-c++ libffi-devel python-devel python-pip python-wheel openssl-devel cyrus-sasl-devel openldap-devel</code></p><p>接下来我们要上传自己的python安装包到linuxx服务器上</p><p> <code>cd /root/公共</code></p><p><code>tar -xf ./Python-3.6.6.tgz</code></p><p>然后进入到解压出来的文件夹中进行编译</p><p><code>./configure</code></p><p><code>make &amp;&amp; make install</code></p><p>安装python3的virtualenv并建⽴superset的env</p><p><code>pip3 install --upgrade pip -i http://pypi.douban.com/simple --trusted-host pypi.douban.com</code></p><p><code>virtualenv -i http://pypi.douban.com/simple --trusted-host pypi.douban.com</code></p><p><code>pip3 install --upgrade setuptools -i http://pypi.douban.com/simple --trusted-host pypi.douban.com</code></p><p>建⽴superset的env&amp;激活</p><p><code>python3 -m venv superset-py3</code></p><p><code>source superset-py3/bin/activate #激活superset的venv</code></p><p>安装superset需要的安装包</p><p>这个包的数量很多建议大家创建一个txt文件然后安装</p><p>requirement.txt文件添加</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">alembic==1.3.2            # via flask-migrate</span><br><span class="line">amqp==2.5.2               # via kombu</span><br><span class="line">apispec[yaml]==1.3.3      # via flask-appbuilder</span><br><span class="line">attrs==19.3.0             # via jsonschema</span><br><span class="line">babel==2.8.0              # via flask-babel</span><br><span class="line">backoff==1.10.0           # via apache-superset (setup.py)</span><br><span class="line">billiard==3.6.3.0         # via celery</span><br><span class="line">bleach==3.1.0             # via apache-superset (setup.py)     ---</span><br><span class="line">celery==4.4.1             # via apache-superset (setup.py)</span><br><span class="line">cffi==1.13.2              # via cryptography</span><br><span class="line">click==7.1.1              # via apache-superset (setup.py), flask, flask-appbuilder</span><br><span class="line">colorama==0.4.3           # via apache-superset (setup.py), flask-appbuilder</span><br><span class="line">contextlib2==0.6.0.post1  # via apache-superset (setup.py)</span><br><span class="line">croniter==0.3.31          # via apache-superset (setup.py)</span><br><span class="line">cryptography==2.8         # via apache-superset (setup.py)</span><br><span class="line">decorator==4.4.1          # via retry</span><br><span class="line">defusedxml==0.6.0         # via python3-openid</span><br><span class="line">flask-appbuilder==2.2.4   # via apache-superset (setup.py)</span><br><span class="line">flask-babel==1.0.0        # via flask-appbuilder</span><br><span class="line">flask-caching==1.8.0      # via apache-superset (setup.py)</span><br><span class="line">flask-compress==1.4.0     # via apache-superset (setup.py)</span><br><span class="line">flask-jwt-extended==3.24.1  # via flask-appbuilder</span><br><span class="line">flask-login==0.4.1        # via flask-appbuilder</span><br><span class="line">flask-migrate==2.5.2      # via apache-superset (setup.py)</span><br><span class="line">flask-openid==1.2.5       # via flask-appbuilder</span><br><span class="line">flask-sqlalchemy==2.4.1   # via flask-appbuilder, flask-migrate</span><br><span class="line">flask-talisman==0.7.0     # via apache-superset (setup.py)</span><br><span class="line">flask-wtf==0.14.2         # via apache-superset (setup.py), flask-appbuilder</span><br><span class="line">flask==1.1.1              # via apache-superset (setup.py), flask-appbuilder, flask-babel, flask-caching, flask-compress, flask-jwt-extended, flask-login, flask-migrate, flask-openid, flask-sqlalchemy, flask-wtf</span><br><span class="line">geographiclib==1.50       # via geopy</span><br><span class="line">geopy==1.20.0             # via apache-superset (setup.py)</span><br><span class="line">gunicorn==20.0.4          # via apache-superset (setup.py)</span><br><span class="line">humanize==0.5.1           # via apache-superset (setup.py)</span><br><span class="line">importlib-metadata==1.4.0  # via jsonschema, kombu</span><br><span class="line">isodate==0.6.0            # via apache-superset (setup.py)</span><br><span class="line">itsdangerous==1.1.0       # via flask</span><br><span class="line">jinja2==2.10.3            # via flask, flask-babel</span><br><span class="line">jsonschema==3.2.0         # via flask-appbuilder</span><br><span class="line">kombu==4.6.8              # via celery</span><br><span class="line">mako==1.1.1               # via alembic</span><br><span class="line">markdown==3.1.1           # via apache-superset (setup.py)</span><br><span class="line">markupsafe==1.1.1         # via jinja2, mako</span><br><span class="line">marshmallow-enum==1.5.1   # via flask-appbuilder</span><br><span class="line">marshmallow-sqlalchemy==0.21.0  # via flask-appbuilder</span><br><span class="line">marshmallow==2.19.5       # via flask-appbuilder, marshmallow-enum, marshmallow-sqlalchemy</span><br><span class="line">more-itertools==8.1.0     # via zipp</span><br><span class="line">msgpack==0.6.2            # via apache-superset (setup.py)</span><br><span class="line">numpy==1.18.1             # via pandas, pyarrow</span><br><span class="line">pandas==0.25.3            # via apache-superset (setup.py)</span><br><span class="line">parsedatetime==2.5        # via apache-superset (setup.py)</span><br><span class="line">pathlib2==2.3.5           # via apache-superset (setup.py)</span><br><span class="line">polyline==1.4.0           # via apache-superset (setup.py)</span><br><span class="line">prison==0.1.2             # via flask-appbuilder</span><br><span class="line">py==1.8.1                 # via retry</span><br><span class="line">pyarrow==0.16.0           # via apache-superset (setup.py)</span><br><span class="line">pycparser==2.19           # via cffi</span><br><span class="line">pyjwt==1.7.1              # via flask-appbuilder, flask-jwt-extended</span><br><span class="line">python-dateutil==2.8.1    # via alembic, apache-superset (setup.py), croniter, flask-appbuilder, pandas</span><br><span class="line">python-dotenv==0.10.5     # via apache-superset (setup.py)</span><br><span class="line">python-editor==1.0.4      # via alembic</span><br><span class="line">python-geohash==0.8.5     # via apache-superset (setup.py)</span><br><span class="line">python3-openid==3.1.0     # via flask-openid</span><br><span class="line">pytz==2019.3              # via babel, celery, flask-babel, pandas</span><br><span class="line">pyyaml==5.3               # via apache-superset (setup.py), apispec</span><br><span class="line">retry==0.9.2              # via apache-superset (setup.py)</span><br><span class="line">selenium==3.141.0         # via apache-superset (setup.py)</span><br><span class="line">simplejson==3.17.0        # via apache-superset (setup.py)</span><br><span class="line">six==1.14.0               # via bleach, cryptography, flask-jwt-extended, flask-talisman, isodate, jsonschema, pathlib2, polyline, prison, pyarrow, pyrsistent, python-dateutil, sqlalchemy-utils, wtforms-json</span><br><span class="line">sqlalchemy-utils==0.36.1  # via apache-superset (setup.py), flask-appbuilder</span><br><span class="line">sqlalchemy==1.3.12        # via alembic, apache-superset (setup.py), flask-sqlalchemy, marshmallow-sqlalchemy, sqlalchemy-utils</span><br><span class="line">sqlparse==0.3.0           # via apache-superset (setup.py)</span><br><span class="line">urllib3==1.25.8           # via selenium</span><br><span class="line">vine==1.3.0               # via amqp, celery</span><br><span class="line">webencodings==0.5.1       # via bleach</span><br><span class="line">werkzeug==0.16.0          # via flask, flask-jwt-extended</span><br><span class="line">wtforms-json==0.3.3       # via apache-superset (setup.py)</span><br><span class="line">wtforms==2.2.1            # via flask-wtf, wtforms-json</span><br><span class="line">zipp==2.0.0               # via importlib-metadata</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>然后执行 ：</p><p><code> pip3 install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r requirement.txt</code></p><p>从阿里的镜像源安装这些依赖</p><p>接下来是安装Superset</p><p>到了这一步，就可能会报错，就是gcc的错误，那是因为安装没有成功，我安装了6次才成功，</p><p>执行 ；</p><p><code>pip3 install apache-superset==0.37.1  -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com</code></p><p>然后<strong>安装Mysql数据包</strong></p><p><code>install sqlalchemy==1.3.24 -i http://pypi.douban.com/simple --trusted-host pypi.douban.com</code></p><p><code>pip3 install mysqlclient -i http://pypi.douban.com/simple --trusted-host pypi.douban.com</code></p><p><code>pip3 install &quot;pymssql&lt;3.0&quot; -i http://pypi.douban.com/simple --trusted-host pypi.douban.com </code></p><p>接下来我们去我们的mysql里执行</p><p><code>CREATE DATABASE </code>superset <code>/*!40100 DEFAULT CHARACTER SET utf8 */;</code></p><p>创建其源数据库</p><p><strong>修改superset元数据库</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> vim superset-py3/lib/python3.6/site-packages/superset/config.py</span><br><span class="line"></span><br><span class="line">修改：</span><br><span class="line">SQLALCHEMY_DATABASE_URI = &#x27;mysql://root:123456@hadoop102/superset?charset=utf8&#x27;</span><br></pre></td></tr></table></figure><p>如果没有这个文件的同学，就是上面安装superset的那一步出问题了，要重新执行</p><p><strong>初始化Supetset数据库（Supetset是一个web应用，自带数据库，需要初始化）</strong></p><p><code> superset db upgrade</code></p><p><strong>创建管理员用户</strong></p><p><code>export FLASK_APP=superset</code></p><p><code>flask fab create-admin</code></p><p><strong>说明：</strong> flask是一个python web框架，Superset使用的就是flask</p><p><strong>Superset初始化</strong></p><p><code> superset init</code></p><p>然后我们要修改mysql里的表</p><p><code>alter table superset.table_columns modify type varchar(255);</code></p><p>然后就可以启动我们的superset了</p><p><code> superset run -h 自己的机器名或者ip  -p 启动端口</code></p><p>然后访问</p><p><code>http://机器ip或者机器名字:端口/</code></p><p>老规矩接下来我们创建个启动脚本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">function check_log_dir()</span><br><span class="line">&#123;</span><br><span class="line">    SUPERSET_LOG_DIR=/usr/local/src/superset/logs</span><br><span class="line">    if [ ! -d $SUPERSET_LOG_DIR ]</span><br><span class="line">    then</span><br><span class="line">        mkdir -p $SUPERSET_LOG_DIR</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#检查进程是否运行正常，参数1为进程名，参数2为进程端口</span><br><span class="line">function check_process()</span><br><span class="line">&#123;</span><br><span class="line">    pid=$(ps -ef 2&gt;/dev/null | grep -v grep | grep -i $1 | awk &#x27;&#123;print $2&#125;&#x27;)</span><br><span class="line">    ppid=$(netstat -nltp 2&gt;/dev/null | grep $2 | awk &#x27;&#123;print $7&#125;&#x27; | cut -d &#x27;/&#x27; -f 1)</span><br><span class="line">    echo $pid</span><br><span class="line">    [ &quot;$ppid&quot; ] &amp;&amp; return 0 || return 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function superset_start()</span><br><span class="line">&#123;</span><br><span class="line">    metapid=$(check_process superset 8889)</span><br><span class="line">    cmd=&quot;cd /root/公共/Python-3.6.6;source superset-py3/bin/activate;nohup superset run -h bigdata4 -p 8889 &gt;$SUPERSET_LOG_DIR/superset.log 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">    eval $cmd || echo &quot;superset服务已启动&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">function superset_stop()</span><br><span class="line">&#123;</span><br><span class="line">    metapid=$(check_process superset 8889)</span><br><span class="line">    [ &quot;$metapid&quot; ] &amp;&amp; kill $metapid || echo &quot;superset服务未启动&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">     check_log_dir</span><br><span class="line">     superset_start</span><br><span class="line">    ;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">     superset_stop</span><br><span class="line">    ;;</span><br><span class="line">&quot;status&quot;)</span><br><span class="line">     check_process superset 8889 &gt;/dev/null &amp;&amp; echo &quot;superset服务运行正常&quot; || echo &quot;superset服务运行异常&quot;</span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    echo Invalid Args!</span><br><span class="line">    echo &#x27;Usage: &#x27;$(basename $0)&#x27; start|stop|status&#x27;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>关于superset的原生安装就ok了</p><h1 id="dataease"><a href="#dataease" class="headerlink" title="dataease"></a>dataease</h1><p>先在官网下载官网地址 ： <a href="https://www.fit2cloud.com/dataease/features.html">https://www.fit2cloud.com/dataease/features.html</a></p><p>目前没有时间等周六周日补上</p><p>dataease安装特别简单而且图形炫酷，首推</p><p>但是有问题：安装dataease的机器上不能有mysql或者你把mysql的端口改掉，因为它要占用3306端口</p><p>不管是在线安装还是离线安装对我们的数据库都有要求，mysql 5.7起步</p><p>而且要求我们编辑&#x2F;etc&#x2F;my.cnf文件</p><p>然后添加以下内容 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">datadir=/var/lib/mysql</span><br><span class="line"></span><br><span class="line">default-storage-engine=INNODB</span><br><span class="line">character_set_server=utf8</span><br><span class="line">lower_case_table_names=1</span><br><span class="line">table_open_cache=128</span><br><span class="line">max_connections=2000</span><br><span class="line">max_connect_errors=6000</span><br><span class="line">innodb_file_per_table=1</span><br><span class="line">innodb_buffer_pool_size=1G</span><br><span class="line">max_allowed_packet=64M</span><br><span class="line">transaction_isolation=READ-COMMITTED</span><br><span class="line">innodb_flush_method=O_DIRECT</span><br><span class="line">innodb_lock_wait_timeout=1800</span><br><span class="line">innodb_flush_log_at_trx_commit=0</span><br><span class="line">sync_binlog=0</span><br><span class="line">group_concat_max_len=1024000</span><br><span class="line">sql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION</span><br><span class="line">skip-name-resolve</span><br><span class="line"></span><br><span class="line">[mysql]</span><br><span class="line">default-character-set=utf8</span><br><span class="line"></span><br><span class="line">[mysql.server]</span><br><span class="line">default-character-set=utf8</span><br></pre></td></tr></table></figure><p>在线安装 ：<code>curl -sSL https://github.com/dataease/dataease/releases/latest/download/quick_start.sh | sh</code></p><p>就可以了</p><p>离线安装：</p><p>先下载好安装包然后解压，之后到解压的目录，然后编辑install.conf文件下面是install.conf的配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"># 基础配置</span><br><span class="line">## 安装目录</span><br><span class="line">DE_BASE=/opt</span><br><span class="line">## Service 端口</span><br><span class="line">DE_PORT=80</span><br><span class="line">## 部署及运行模式，可选值有 local、simple、cluster，分别对应 本地模式、精简模式、集群模式</span><br><span class="line">DE_ENGINE_MODE=simple</span><br><span class="line">## docker 网段设置</span><br><span class="line">DE_DOCKER_SUBNET=172.19.0.0/16</span><br><span class="line">## docker 网关 IP</span><br><span class="line">DE_DOCKER_GATEWAY=172.19.0.1</span><br><span class="line">## Apache Doris FE IP (外部 Doris 此参数无效)</span><br><span class="line">DE_DORIS_FE_IP=172.19.0.198</span><br><span class="line">## Apache Doris BE IP (外部 Doris 此参数无效)</span><br><span class="line">DE_DORIS_BE_IP=172.19.0.199</span><br><span class="line"></span><br><span class="line"># 数据库配置</span><br><span class="line">## 是否使用外部数据库</span><br><span class="line">DE_EXTERNAL_MYSQL=false</span><br><span class="line">## 数据库地址</span><br><span class="line">DE_MYSQL_HOST=mysql</span><br><span class="line">## 数据库端口</span><br><span class="line">DE_MYSQL_PORT=3306</span><br><span class="line">## DataEase 数据库库名</span><br><span class="line">DE_MYSQL_DB=dataease</span><br><span class="line">## 数据库用户名</span><br><span class="line">DE_MYSQL_USER=root</span><br><span class="line">## 数据库密码</span><br><span class="line">DE_MYSQL_PASSWORD=Password123@mysql</span><br><span class="line"></span><br><span class="line"># Apache Doris 配置</span><br><span class="line">## 是否使用外部 Apache Doris</span><br><span class="line">DE_EXTERNAL_DORIS=false</span><br><span class="line">## Doris 地址</span><br><span class="line">DE_DORIS_HOST=doris-fe</span><br><span class="line">## Doris 查询连接端口</span><br><span class="line">DE_DORIS_PORT=9030</span><br><span class="line">## Doris http端口</span><br><span class="line">DE_DORIS_HTTPPORT=8030</span><br><span class="line">## Doris 数据库名称</span><br><span class="line">DE_DORIS_DB=dataease</span><br><span class="line">## Doris 用户名</span><br><span class="line">DE_DORIS_USER=root</span><br><span class="line">## Doris 密码</span><br><span class="line">DE_DORIS_PASSWORD=Password123@doris</span><br><span class="line"></span><br><span class="line"># Kettle 配置</span><br><span class="line">## 是否使用外部 Kettle - (目前还不支持外部Kettle，除非不需运行Kettle，否则请不要修改此参数)</span><br><span class="line">DE_EXTERNAL_KETTLE=false</span><br><span class="line">## Kettle 服务器地址</span><br><span class="line">DE_CARTE_HOST=kettle</span><br><span class="line">## Kettle 访问端口</span><br><span class="line">DE_CARTE_PORT=18080</span><br><span class="line">## Kettle 用户名</span><br><span class="line">DE_CARTE_USER=cluster</span><br><span class="line">## Kettle 密码</span><br><span class="line">DE_CARTE_PASSWORD=cluster</span><br></pre></td></tr></table></figure><p>安装模式有三种 ：</p><p><strong>DE_ENGINE_MODE&#x3D;local</strong><br>使用本地模式安装，DataEase 会自带 Doris 与 Kettle 组件，无需再做额外配置，但各组件均为单点，不具备高可用特性。<br>在此模式下，Excel 数据集、API 数据集以及定时同步的数据默认保存在自带的 Doris 组件中。</p><p><strong>DE_ENGINE_MODE&#x3D;simple</strong><br>使用精简模式安装，系统不会额外安装 Doris 与 Kettle 组件，提供用户轻量级的应用系统，尤其是对接数据量较小的情况。<br>在此模式下，若用户需要使用 Excel 数据集或 API 数据集可在系统管理界面配置数据引擎（目前仅支持 MySQL 类型），相关数据会存储到该数据引擎中。若只需使用数据库直连则无需做此配置。<br><strong>注意：由于精简模式未配置 Kettle 与 Doris，故相关 SQL 数据集&#x2F;数据库数据集不提供定时同步模式。</strong></p><p><strong>DE_ENGINE_MODE&#x3D;cluster</strong><br>使用集群模式安装，系统不会额外安装 Doris 与 Kettle 组件，但会在系统管理模块提供 Doris 与 Kettle 的链接配置界面（请参考【系统管理】的【系统参数】说明），用户可独立安装 Doris 集群及 Kettle 并配置在 DataEase 中。集群模式下 Excel 数据集，API 数据集以及定时同步的数据通过 Kettle 抽取到 Doris 集群中。<br>Doris 安装部署可参考：<a href="http://doris.incubator.apache.org/zh-CN/">http://doris.incubator.apache.org/zh-CN/</a><br>Kettle 安装部署可参考：<a href="http://www.kettle.org.cn/">http://www.kettle.org.cn/</a></p><p>然后对于离线安装执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 进入安装包目录</span><br><span class="line">cd dataease-v1.5.0-offline</span><br><span class="line"># 运行安装脚本</span><br><span class="line">/bin/bash install.sh</span><br></pre></td></tr></table></figure><p>就可以了，效果个很酷炫</p><h1 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h1><p>有一个城市表 ： mysql中</p><p>有一个商品表 ： mysql中</p><p>用户行为数据 ： hdfs上的</p><p>求： 最受欢迎的商品 的 top3</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/12/06/12-06/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>大数据的简单构架</title>
      <link>http://example.com/2022/12/05/12-05/</link>
      <guid>http://example.com/2022/12/05/12-05/</guid>
      <pubDate>Mon, 05 Dec 2022 01:12:53 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;大数据的三件事&quot;&gt;&lt;a href=&quot;#大数据的三件事&quot; class=&quot;headerlink&quot; title=&quot;大数据的三件事&quot;&gt;&lt;/a&gt;大数据的三件事&lt;/h1&gt;&lt;p&gt;数据采集  ：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;采集业务数据 ： sqoop ,datax,实时采集max</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="大数据的三件事"><a href="#大数据的三件事" class="headerlink" title="大数据的三件事"></a>大数据的三件事</h1><p>数据采集  ：</p><ul><li>采集业务数据 ： sqoop ,datax,实时采集maxwell ,flinkdoc</li><li>采集日志数据 ： flume ，logstush</li></ul><p>数据存储 :</p><ul><li>hdfs (hadoop中的)</li><li>hive</li><li>hbase【大数据】</li><li>数据分析之后的结果数据 ：<ul><li>mysql</li><li>clickhouse</li><li>drios</li></ul></li></ul><p>数据分析：</p><ul><li>map reduce 但是现在不怎么用了，但是思想最重要</li><li>hive ： 主要是离线数仓</li><li>hbase</li><li>spark</li><li>flink</li></ul><p>数据可视化 ：</p><ul><li>如果有前端开发人员，可以让他们来帮忙</li><li>但是如果没有 要自己做<ul><li>superset</li><li>dataease</li><li>echarts</li><li>env</li><li>anv</li></ul></li><li>收费 : 简历上最好不要写这个<ul><li>quickbi</li><li>sugar</li></ul></li></ul><p>消息中间键 ：</p><ul><li>kafka</li><li>pular</li></ul><p>(即席查询 ： 临时查询) : presto是最好用的 clickhouse 是有bug的 对内存有要求</p><ul><li>sparksql , presto , druid , clickhouse ,kylin(cube)</li></ul><p>数据种类 ：</p><ul><li>业务数据【mysql ， es】app</li><li>日志数据 【log】linux 磁盘上 ，工作中处理的一个重点<ul><li>展现日志，点击日志，跳转日志</li></ul></li><li>其他数据</li></ul><p>架构图  ：</p><ul><li>业务数据 ： mysql -》sqoop ， datax -》 hds&#x2F;hive</li><li>日志数据 ： log文件 -》 flume -》 hdfs&#x2F;hive</li><li>hive : 构建离线数仓<ul><li>数据分层</li><li>维度建模</li><li>指标输出</li></ul></li><li>数据可实话<ul><li>hive -》 sqoop -》 mysql &#x2F; clickhouse -》数据可视化</li></ul></li></ul><p>大数据的基础平台架构</p><p>提升 ：大数据的数据平台</p><p>大数据基础是相当于 从  0-1 搭建 -》 可以学到以上的所有框架</p><p>大数据的数据平台 是基于基础平台再提升了升级 -》 这个学不到什么东西</p><p>大数据升级平台</p><p>大数据数据开发</p><ul><li>离线数仓</li><li>实时数仓</li><li>临时查询</li></ul><p>大数据的etl工程师</p><ul><li>数据清洗</li><li>数据抽取</li><li>数据转换</li></ul><p>大数据运维工程师</p><ul><li>上述的框架是它负责安装以及部署的</li><li>以及后续的维护</li><li>云原生 ， docker ，k8s</li></ul><p>大数据算法组（数据分析师sql + 数学知识统计 。数据科学家）</p><ul><li>用户画像</li><li>数据挖掘<ul><li>python</li><li>spark , flink 自带的组件 机器学习相关的组件 ： 速度要比py快</li></ul></li></ul><p>新颖的</p><ul><li>数据湖 -》主要研究方向</li><li>云原生 -》 docker ，k8s<ul><li>job -》 申请资源 是再yarn上的 ，但是当yarn 做资源隔离的时候万一有三台机器 ，到时候如果所有container 都集中在一台机器上，则会造成机器得负载太大</li><li>解决方法 ：<ul><li>yarn 的底层编码重写</li><li>联合云原生</li></ul></li></ul></li></ul><h1 id="sqoop"><a href="#sqoop" class="headerlink" title="sqoop"></a>sqoop</h1><p>简介 ： 可以把数据和hadoop生态圈进行数据库同步，数据传输</p><p>sqoop</p><ul><li>我们可以通过sqoop这个组件 ，把mysql 里的表 同步到 hdfs，hive ，hbase</li><li>反之也可以</li><li>原理<ul><li>sqoop 是只用map阶段 ，无reduce 阶段 （通过mapreduce 实现的）</li></ul></li><li>指定的参数<ul><li>url</li><li>username</li><li>password</li><li>驱动</li></ul></li><li>sqoop版本<ul><li>sqoop 1 ：1.4.7</li><li>sqoop 2 ：1.99.7</li><li>注意这两个是没有任何联系的</li><li>建议用1</li></ul></li></ul><h1 id="部署sqoop"><a href="#部署sqoop" class="headerlink" title="部署sqoop"></a>部署sqoop</h1><p>这里我使用的是1.4.7的sqoop包</p><p>首先我们上传到linux上</p><p>然后解压</p><p>解压之后我们进入到conf目录下，这个里面存的是我们的 配置文件</p><p>我们把sqoop-env-template.sh 改名字成 sqoop-env.sh</p><p>然后vim 它进行编辑</p><p>把hadoop home 的路径放上 ，以及 hive的路经</p><p>然后保存退出</p><p>我们接下来在全局变量中注册一下sqoop的bin目录</p><p>然后把mysql的connect包给到lib文件夹下</p><p>然后执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.查看可用的数据库 【mysql】</span><br><span class="line">sqoop list-databases \</span><br><span class="line">--connect jdbc:mysql://bigdata2:3306  \</span><br><span class="line">--username root  \</span><br><span class="line">--password liuzihan010616</span><br></pre></td></tr></table></figure><p>1.4.7会报错 ，因为 缺少java.commons.lang包</p><p>我们把这个包上传到lib下就好了</p><h1 id="导入和导出"><a href="#导入和导出" class="headerlink" title="导入和导出"></a>导入和导出</h1><p>sqoop的导入和导出</p><p>从mysql里导出数据的时候，会默认导入到&#x2F;user&#x2F;hadoop&#x2F;*</p><p>hdfs 文件存储 默认是用 ，进行分割每个字段的</p><p>hdfs上有几个文件就是有几个map task 和reduce task</p><p>其默认的数量是4</p><p>导出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://bigdata2:3306/hive  \</span><br><span class="line">--username root  \</span><br><span class="line">--password liuzihan010616 \</span><br><span class="line">--table TBLS</span><br></pre></td></tr></table></figure><p>设置导出的列的参数</p><p><code>--columns</code></p><p>设置字段筛选的参数</p><p><code>--where</code></p><p>设置再yarn上的作业名称</p><p><code>--mapreduce-job-name</code></p><p>设置 map 和 reduce task 的个数</p><p><code>-m,--num-mappers &lt;n&gt; </code></p><p>用-m 或者后面的都行</p><p>设置hdfs上的文件夹</p><p><code>--target-dir</code></p><p>设置hdfs上存储的分隔符</p><p><code>--fields-terminated-by </code></p><p>删除目标文件夹</p><p><code>--delete-target-dir </code></p><p>总体应用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sqoop  import \</span><br><span class="line">--connect jdbc:mysql://bigdata2:3306/hive  \</span><br><span class="line">--username root  \</span><br><span class="line">--password liuzihan010616 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--fields-terminated-by &#x27;\&#x27; \</span><br><span class="line">--target-dir /ghk \</span><br><span class="line">-m 1 \</span><br><span class="line">--mapreduce-job-name &#x27;mysql 的数据try&#x27; \</span><br><span class="line">--where &#x27;TBL_ID &gt;= 10&#x27; \</span><br><span class="line">--columns &#x27;TBL_ID , OWNER&#x27; \</span><br><span class="line">--table TBLS</span><br></pre></td></tr></table></figure><p>有主键的表可以直接按照上述同步</p><p>但是没主键的，要转化</p><p>如果是没有主键的表，有两种转换方法</p><p>首先是可以通过  ： -m 设置为1 或者 –split-by 列的名字</p><p>如下 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop  import \</span><br><span class="line">--connect jdbc:mysql://bigdata2:3306/try  \</span><br><span class="line">--username root  \</span><br><span class="line">--password liuzihan010616 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--fields-terminated-by &#x27;\&#x27; \</span><br><span class="line">--target-dir /ghk \</span><br><span class="line">--split-by empno \</span><br><span class="line">--mapreduce-job-name &#x27;mysql 的数据try&#x27; \</span><br><span class="line">--table emp</span><br></pre></td></tr></table></figure><p>空值处理 ：</p><p><code>--null-non-string 0</code></p><p>上面的那个是不是string的处理</p><p><code>--null-string &#39;&#39;</code></p><h2 id="嵌套sql"><a href="#嵌套sql" class="headerlink" title="嵌套sql"></a>嵌套sql</h2><p>用 –query</p><p>但是有注意的点</p><p>有–query 的时候，不能放–table</p><p>且–query 后面只能接单引号</p><p>如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop  import \</span><br><span class="line">--connect jdbc:mysql://bigdata2:3306/try  \</span><br><span class="line">--username root  \</span><br><span class="line">--password liuzihan010616 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--fields-terminated-by &#x27;\&#x27; \</span><br><span class="line">--target-dir /ghk \</span><br><span class="line">--split-by empno \</span><br><span class="line">--mapreduce-job-name &#x27;mysql 的数据try&#x27; \</span><br><span class="line">--query &#x27;select * from emp where $CONDITIONS&#x27;</span><br></pre></td></tr></table></figure><h1 id="简化"><a href="#简化" class="headerlink" title="简化"></a>简化</h1><p>如上述</p><p>我们发现太繁琐了</p><p>我们可以进行封装到一起</p><p>然后直接调用文件就行</p><p><code>sqoop --options-file 文件路径</code></p><p>文件内容如下 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import </span><br><span class="line">--connect </span><br><span class="line">jdbc:mysql://bigdata2:3306/try  </span><br><span class="line">--username </span><br><span class="line">root  </span><br><span class="line">--password </span><br><span class="line">liuzihan010616 </span><br><span class="line">--delete-target-dir </span><br><span class="line">--fields-terminated-by </span><br><span class="line">&#x27;\&#x27; </span><br><span class="line">--target-dir</span><br><span class="line">/ghk </span><br><span class="line">--split-by </span><br><span class="line">empno </span><br><span class="line">--mapreduce-job-name </span><br><span class="line">&#x27;mysql 的数据try&#x27; </span><br><span class="line">--query </span><br><span class="line">&#x27;select * from emp where $CONDITIONS&#x27;</span><br></pre></td></tr></table></figure><h3 id="sqoop-job"><a href="#sqoop-job" class="headerlink" title="sqoop job"></a>sqoop job</h3><ul><li>create 创建job</li><li>list&#x2F;show 查看job list 是查看列表 show 是查看详情</li><li>exec 执行job</li></ul><p>代码如下 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scoop job --create mysqltry -- \</span><br><span class="line">import \</span><br><span class="line">--connect </span><br><span class="line">jdbc:mysql://bigdata2:3306/try  \</span><br><span class="line">--username root  \</span><br><span class="line">--password liuzihan010616 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--fields-terminated-by &#x27;\&#x27; \</span><br><span class="line">--target-dir /ghk \</span><br><span class="line">--split-by empno \</span><br><span class="line">--mapreduce-job-name &#x27;mysql 的数据try&#x27; \</span><br><span class="line">--query &#x27;select * from emp where $CONDITIONS&#x27;</span><br></pre></td></tr></table></figure><h2 id="shell脚本"><a href="#shell脚本" class="headerlink" title="shell脚本"></a>shell脚本</h2><p>也可以通过shell脚本调用 sqoop</p><p>如下 ：普通表的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if [ $# -lt 6 ];then</span><br><span class="line"> echo &quot;$0 use sync mysql 2 hive&quot;</span><br><span class="line"> echo &quot;USAGE:$0 mysqldb sql hivedb hivetable idautocreatetable fengefu&quot;</span><br><span class="line"> echo &quot;Example mysql的数据库 sql语句 hive的数据库  hive的表 分隔符 是不是自动创建表&quot;</span><br><span class="line"> exit 1;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#mysql parm</span><br><span class="line">mysqldb=$1</span><br><span class="line">sql=&quot;$2&quot;</span><br><span class="line">#hive parms </span><br><span class="line">hivedb=$3</span><br><span class="line">hivetable=$4</span><br><span class="line">flag =$6</span><br><span class="line"></span><br><span class="line">try &#x27;select * from emp&#x27; bigdata_hive3 emp6 , 1</span><br><span class="line"></span><br><span class="line">MySQL_URL=&quot;jdbc:mysql://bigdata2:3306/$&#123;mysqldb&#125;&quot; </span><br><span class="line">MySQL_USER=root</span><br><span class="line">MySQL_PASSWD=liuzihan010616</span><br><span class="line">FIELDS_TERMINATED=$5</span><br><span class="line"></span><br><span class="line">if [ $&#123;flag&#125; -eq 1 ];then</span><br><span class="line">sqoop import \</span><br><span class="line">--connect $&#123;MySQL_URL&#125;  \</span><br><span class="line">--username $&#123;MySQL_USER&#125;  \</span><br><span class="line">--password $&#123;MySQL_PASSWD&#125; \</span><br><span class="line">--mapreduce-job-name &#x27;mysql2hive&#x27; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--target-dir /sqoop/emp_tmp \</span><br><span class="line">--fields-terminated-by $&#123;FIELDS_TERMINATED&#125; \</span><br><span class="line">-m 1 \</span><br><span class="line">--query &quot;$&#123;sql&#125; and \$CONDITIONS &quot; \</span><br><span class="line">--hive-import \</span><br><span class="line">--create-hive-table \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-database $&#123;hivedb&#125; \</span><br><span class="line">--hive-table $&#123;hivetable&#125;</span><br><span class="line">else</span><br><span class="line">sqoop import \</span><br><span class="line">--connect $&#123;MySQL_URL&#125;  \</span><br><span class="line">--username $&#123;MySQL_USER&#125;  \</span><br><span class="line">--password $&#123;MySQL_PASSWD&#125; \</span><br><span class="line">--mapreduce-job-name &#x27;mysql2hive&#x27; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--target-dir /sqoop/emp_tmp \</span><br><span class="line">--fields-terminated-by $&#123;FIELDS_TERMINATED&#125; \</span><br><span class="line">-m 1 \</span><br><span class="line">--query &quot;$&#123;sql&#125; and \$CONDITIONS &quot; \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-database $&#123;hivedb&#125; \</span><br><span class="line">--hive-table $&#123;hivetable&#125;</span><br><span class="line">exit 200;</span><br><span class="line">fi</span><br><span class="line">---------------------------------分区表--------------------------------------------</span><br><span class="line">if [ $# -lt 8 ];then</span><br><span class="line"> echo &quot;$0 use sync mysql 2 hive&quot;</span><br><span class="line"> echo &quot;USAGE:$0 mysqldb sql hivedb hivetable idautocreatetable fengefu hivepartition hivepartitionvalue&quot;</span><br><span class="line"> echo &quot;Example mysql的数据库 sql语句 hive的数据库  hive的table 分隔符  hive的分区属性 分区属性的值 是不是自动创建表&quot;</span><br><span class="line"> exit 1;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#mysql parm</span><br><span class="line">mysqldb=$1</span><br><span class="line">sql=&quot;$2&quot;</span><br><span class="line">#hive parms </span><br><span class="line">hivedb=$3</span><br><span class="line">hivetable=$4</span><br><span class="line">hivepartition=$6</span><br><span class="line">hivepartitionvalue=$7</span><br><span class="line">flag=$8</span><br><span class="line">MySQL_URL=&quot;jdbc:mysql://bigdata2:3306/$&#123;mysqldb&#125;&quot; </span><br><span class="line">MySQL_USER=root</span><br><span class="line">MySQL_PASSWD=liuzihan010616</span><br><span class="line">FIELDS_TERMINATED=$5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if [ $&#123;flag&#125; -eq 1 ];then</span><br><span class="line">sqoop import \</span><br><span class="line">--connect $&#123;MySQL_URL&#125;  \</span><br><span class="line">--username $&#123;MySQL_USER&#125;  \</span><br><span class="line">--password $&#123;MySQL_PASSWD&#125; \</span><br><span class="line">--mapreduce-job-name &#x27;mysql2hive&#x27; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--target-dir /sqoop/emp_tmp \</span><br><span class="line">--fields-terminated-by $&#123;FIELDS_TERMINATED&#125; \</span><br><span class="line">-m 1 \</span><br><span class="line">--query &quot;$&#123;sql&#125; and \$CONDITIONS &quot; \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--create-hive-table \ </span><br><span class="line">--hive-database $&#123;hivedb&#125; \</span><br><span class="line">--hive-table $&#123;hivetable&#125; \</span><br><span class="line">--hive-partition-key $&#123;hivepartition&#125; \</span><br><span class="line">--hive-partition-value $&#123;hivepartitionvalue&#125;</span><br><span class="line">else</span><br><span class="line">sqoop import \</span><br><span class="line">--connect $&#123;MySQL_URL&#125;  \</span><br><span class="line">--username $&#123;MySQL_USER&#125;  \</span><br><span class="line">--password $&#123;MySQL_PASSWD&#125; \</span><br><span class="line">--mapreduce-job-name &#x27;mysql2hive&#x27; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--target-dir /sqoop/emp_tmp \</span><br><span class="line">--fields-terminated-by $&#123;FIELDS_TERMINATED&#125; \</span><br><span class="line">-m 1 \</span><br><span class="line">--query &quot;$&#123;sql&#125; and \$CONDITIONS &quot; \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-database $&#123;hivedb&#125; \</span><br><span class="line">--hive-table $&#123;hivetable&#125; \</span><br><span class="line">--hive-partition-key $&#123;hivepartition&#125; \</span><br><span class="line">--hive-partition-value $&#123;hivepartitionvalue&#125;</span><br><span class="line">fi</span><br><span class="line">-----------------------------------------------------hivetomysql的----------------------------------------------------</span><br><span class="line">if [ $# -lt 4 ];then</span><br><span class="line">echo &quot;error 变量小于4个&quot;</span><br><span class="line">echo &quot;example try , /user/hive/warehouse/bigdata_hive3.db/emp_partition/deptno=20 emp1&quot;</span><br><span class="line">echo &quot;mysql数据库 分隔符 hdfs上的路径 mysql里的表名&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">mysqldb=$1</span><br><span class="line">fengefu=$2</span><br><span class="line">hdfslujing=$3</span><br><span class="line">mysqltable=$4</span><br><span class="line"></span><br><span class="line">hive -e &quot;use bigdata_hive3 ; create table &quot;</span><br><span class="line"></span><br><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql://bigdata2:3306/$&#123;mysqldb&#125;  \</span><br><span class="line">--username root  \</span><br><span class="line">--password liuzihan010616 \</span><br><span class="line">--fields-terminated-by $&#123;fengefu&#125; \</span><br><span class="line">--export-dir $&#123;hdfslujing&#125; \</span><br><span class="line">--null-non-string 0</span><br><span class="line">--null-string &#x27;&#x27;</span><br><span class="line">--table $&#123;mysqltable&#125; </span><br><span class="line">----------------------------------------------hivetomysql的分区表如何同步-----------------------</span><br><span class="line">先把hive的分区表用create table zz as elect * from xxx(分区表的名字) </span><br><span class="line">然后把zz当成普通表传过去 ，但是在sqoop1.4.7 目前这个功能出现了些问题</span><br><span class="line"> </span><br></pre></td></tr></table></figure><h1 id="从mysql-到-hive等工具中"><a href="#从mysql-到-hive等工具中" class="headerlink" title="从mysql 到 hive等工具中"></a>从mysql 到 hive等工具中</h1><p>大部分都和上述一样的</p><p>只不过要换个链接以及表名字</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://bigdata2:3306/try  \</span><br><span class="line">--username root  \</span><br><span class="line">--password liuzihan010616 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--fields-terminated-by &#x27;\&#x27; \</span><br><span class="line">--target-dir /ghk \</span><br><span class="line">--split-by empno \</span><br><span class="line">--mapreduce-job-name &#x27;mysql 的数据try&#x27; \</span><br><span class="line">--query &#x27;select * from emp where $CONDITIONS&#x27; \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--create-hive-table \ </span><br><span class="line">--hive-database bigdata_hive3 \</span><br><span class="line">--hive-table emp_hive1</span><br></pre></td></tr></table></figure><p>在1.4.7的版本中其要求必须 加上 <code>--target dir 属性</code></p><p>且要从hive的lib文件夹下，把所有jar包给sqoop的lib下</p><p>上述 create-hive-table 是自动创建表</p><p>但是因为mysql里的属性只有几个 ，没hive特有的decmical等，可能会造成丢失数据</p><p>所以提议先在hive中创建表，然后再导入</p><h2 id="创建分区表"><a href="#创建分区表" class="headerlink" title="创建分区表"></a>创建分区表</h2><p>分区表如上 ；只不过要用</p><p><code>--hive-partition-key xxxx（列名）</code></p><p><code>--hive-partition-value xxx(分区字段的数值)</code></p><p>或者可以直接用–query 用代码的方式进行分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://bigdata2:3306/try  \</span><br><span class="line">--username root  \</span><br><span class="line">--password liuzihan010616 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--fields-terminated-by &#x27;\&#x27; \</span><br><span class="line">--target-dir /ghk \</span><br><span class="line">--split-by empno \</span><br><span class="line">--mapreduce-job-name &#x27;mysql 的数据try&#x27; \</span><br><span class="line">--query &#x27;select empno,ename,job,mgr,hiredate,sal,comm from emp where deptno=20 and $CONDITIONS &#x27; \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--create-hive-table \</span><br><span class="line">--hive-database bigdata_hive3 \</span><br><span class="line">--hive-table emp_partition \</span><br><span class="line">--hive-partition-key deptno \</span><br><span class="line">--hive-partition-value 20</span><br></pre></td></tr></table></figure><h2 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h2><p>不管是hdfs 或者是hive 都是基于路径导出</p><p>通过export导出</p><p>代码 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql://bigdata2:3306/try  \</span><br><span class="line">--username root  \</span><br><span class="line">--password liuzihan010616 \</span><br><span class="line">--table xxxX \</span><br><span class="line">--fields-terminated-by &#x27;分隔符&#x27; \</span><br><span class="line">--export-dir hdfs上的数据的路径 \</span><br></pre></td></tr></table></figure><p>这个是hdfs导入到mysql</p><p>sqoop导入hive到mysql的时候空值要先进行处理，</p><h1 id="开启历史日志"><a href="#开启历史日志" class="headerlink" title="开启历史日志"></a>开启历史日志</h1><p>开启开关 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>上述是日志收集的开关</p><p>下面是日志在hdfs上存储的lujing</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;http://bigdata3:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>下面是设置日志收集的时间</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;259200&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>接下来我们要配置mapred -site.xml</p><p>如下 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata3:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata3:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>然后通过命令行的方式执行命令</p><p><code>yarn timelineserver   [bigdata4]</code></p><p><code>mapred historyserver  [bigdata3] </code></p><p>提示 ： 我的namenode在bigdata3上 ，而resourcemanager 在 bigdata4上</p><p>Hive建表时，默认使用的分隔符时候一个特殊的字符，查看表决结构时候是一个’\001’</p><p>这不是真正的’\001’，其实是使用八进制编码\001表示</p><h2 id="hive-table-gt-mysql-多次导入-数据结果不同"><a href="#hive-table-gt-mysql-多次导入-数据结果不同" class="headerlink" title="hive table -&gt; mysql 多次导入 数据结果不同"></a>hive table -&gt; mysql 多次导入 数据结果不同</h2><p>幂等性 ： 多次操作 ，数据结果是不变的</p><p>mysql - &gt; hive -&gt; hive-overwrite</p><p>hive -&gt; mysql 幂等性 如何解决</p><p>方法 ：</p><p>可以通过 <code>mysql -u root -p xxx -e sql语句</code></p><p>或者 <code>mysql -uroot -pliuzihan010616 &lt; ./try.sql 这个是执行sql文件</code></p><p>上述两个语句是可以在MySQL外部直接进行执行的，不用进入到mysql里</p><p>数据库唯一主键</p><p>缺点：无法使用change buffer，InnoDB为了进行唯一性检查，必须有一次磁盘IO读页</p><p>业务状态校验</p><p>业务上根据业务ID的唯一性和业务处理的结果去做判断，但是这部分判断的逻辑需要考虑原子性。否则会因为并发问题导致幂等失效。解决途径（一）加锁，根据当前的服务环境选择单机或分布式锁。（二）采用现成方案Tomato，通过滑动窗口或者固定窗口拦截控制时间内的请求</p><p>数据库乐观锁实现幂等性</p><p>缺点：操作业务前，需要先查询出当前的version版本。会增加操作</p><p>防重 Token 令牌实现幂等性</p><p>缺点：</p><p>产生过多额外请求</p><p>先删除token，如果业务处理出现异常但token已经删除掉了，再来请求会被认定为重复请求</p><p>后删除token，如果删除redis中的token失败了，再来请求不会拦截，发生了重复请求</p><p>下游传递唯一序列号实现幂等性</p><p>缺点：无法控制下游唯一序列号的生成规则，如果序列号由时间戳生成，那么无法拦截类似重复点击这种情况下的重复请求</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/12/05/12-05/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hive第四天</title>
      <link>http://example.com/2022/12/02/12-02/</link>
      <guid>http://example.com/2022/12/02/12-02/</guid>
      <pubDate>Fri, 02 Dec 2022 00:59:25 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;维度组合分析 ：&lt;/p&gt;
&lt;p&gt;sql 关键字 ： grouping sets&lt;/p&gt;
&lt;p&gt;例子  ：&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span cla</description>
        
      
      
      
      <content:encoded><![CDATA[<p>维度组合分析 ：</p><p>sql 关键字 ： grouping sets</p><p>例子  ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table user_shop (</span><br><span class="line">user_id String,</span><br><span class="line">shop_name String,</span><br><span class="line">channe String,</span><br><span class="line">os String</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;,&#x27;</span><br></pre></td></tr></table></figure><p>一般我们进行维度计算的时候，我们可以通过group by 的方式进行</p><p>但是假如我们每次都要处理一个维度，那么我们难道要写很多个sql语句吗</p><p>这明显是不行的</p><p>那么我们如何解决呢</p><p>通过grouping sets 就可以解决了</p><p>代码如下  ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">SELECT empid,custid,</span><br><span class="line">       sum(qty) as sumqty</span><br><span class="line">FROM Orders</span><br><span class="line">GROUP BY</span><br><span class="line">    GROUPING SETS</span><br><span class="line">    (</span><br><span class="line">        (empid,custid),</span><br><span class="line">        (empid),(custid),</span><br><span class="line">        ()</span><br><span class="line">    );</span><br></pre></td></tr></table></figure><p>上面代码的意思就是 ：我要按照ｇｒｏｕｐ　ｂｙ　的方法　把empid,custid和empid和custid这几个维度都选出来，然后是上下在一起的　相当于用ｕｎｉｏｎ在一起</p><p>如果是这次选择的维度中未选择的维度，比如说　，我只选择了　维度empid　，那么custid列就会是空，但是这个比多次重复性写ｓｑｌ语句要好的多</p><h1 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h1><p>针对以下数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">+--------------+--------------------+</span><br><span class="line">| hanglie.ame  | hanglie.teresting  |</span><br><span class="line">+--------------+--------------------+</span><br><span class="line">| zuan         | 王者荣耀               |</span><br><span class="line">| zuan         | 吃饭                 |</span><br><span class="line">| zuan         | rap                |</span><br><span class="line">| zuan         | 唱歌                 |</span><br><span class="line">| chaofeng     | 王者荣耀               |</span><br><span class="line">| chaofeng     | 睡觉                 |</span><br><span class="line">| chaofeng     | 方亚                 |</span><br><span class="line">+--------------+--------------------+</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们可以把后面散开的数据转化成一个array存起来</p><p>通过 collect_list 函数转化成array 而且可以通过 concat_ws函数设置每个参数之间的分隔符</p><p>如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">ame,</span><br><span class="line">collect_list(teresting) as interesting,</span><br><span class="line">concat_ws(&#x27;:&#x27; , collect_list(teresting)) as newin</span><br><span class="line">from hanglie</span><br><span class="line">group by ame</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+-----------+---------------------------+-----------------+</span><br><span class="line">|    ame    |        interesting        |      newin      |</span><br><span class="line">+-----------+---------------------------+-----------------+</span><br><span class="line">| chaofeng  | [&quot;王者荣耀&quot;,&quot;睡觉&quot;,&quot;方亚&quot;]        | 王者荣耀:睡觉:方亚      |</span><br><span class="line">| zuan      | [&quot;王者荣耀&quot;,&quot;吃饭&quot;,&quot;rap&quot;,&quot;唱歌&quot;]  | 王者荣耀:吃饭:rap:唱歌  |</span><br><span class="line">+-----------+---------------------------+-----------------+</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="concat"><a href="#concat" class="headerlink" title="concat"></a>concat</h2><p>concat是可以更改数组分隔符的一个函数</p><p>例子 ：</p><p>拼接：</p><ul><li>concat  &#x3D;》 字符串拼接</li><li>select concat(“zuan”,”|”,”zihang”,”|”,”chaofeng”)</li><li>结果是 ：zuan|zihang|chaofeng</li><li>concat_ws(string SEP, string A, string B…) &#x3D;》 字符串拼接</li><li>select concat_ws(“|”,”zuan”,”zihang”,”chaofeng”)</li><li>可变参数 &#x3D;》 array【String】</li><li>select concat_ws(“|”,split(“a,a,a”,’,’)</li><li>select  split(“a,a,a”,’,’)  ： 这个就是切割字符串</li></ul><p>所有类型的可以转换成字符串</p><p>字符串有好处也有坏处</p><p>因为无法排序</p><p>但是经过hive优化，字符串是可以进行四则运算的</p><p>字符串排序： 按照字典序进行排序的 a-z</p><h2 id="BY"><a href="#BY" class="headerlink" title="BY"></a>BY</h2><p>四个by</p><h3 id="order-by"><a href="#order-by" class="headerlink" title="order by"></a>order by</h3><p>全局排序 ，且reduce只有一个</p><p>order by会对输入进行全局排序，因此只有一个Reducer（多个Reducer无法保证全局有序），然而只有一个Reducer会导致计算效率非常低，使用较少。事实上，在生产环境中，order by 很容易造成OOM。</p><p>如下  ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select  *  from emp order by empno;</span><br></pre></td></tr></table></figure><p>执行上述语句要开启个开关才可以</p><p>hive.mapred.mode &#x3D;&gt;some risky queries are not allowed to run 【关闭】</p><p>如果用order by 推荐和 limit搭配</p><h3 id="sort-by"><a href="#sort-by" class="headerlink" title="sort by"></a>sort by</h3><p>分区排序 ： 不能保证 全局有序</p><p>sort by不是全局排序，它会在数据进入Reducer之前完成排序。因此如果使用sort by进行排序，并且设置mapreduce.job.reduces多于一个，则sort by只会保证每个reducer的输出有序，不能保证全局有序。但是可以对最后的结果进行归并排序实现全局排序。</p><p>假如你的reduce task 个数 是 1 则它和order by 是一样的</p><p>调制reduce task 个数 ：</p><ul><li>mapred.reduce.tasks</li><li>set  mapred.reduce.tasks;</li></ul><h3 id="Distribute-By"><a href="#Distribute-By" class="headerlink" title="Distribute  By"></a>Distribute  By</h3><p>数据开发的时候会用到</p><p>distribute by的作用是控制map端如何拆分数据给reduce端。hive会根据distribute by后面的字段，对reduce的个数进行分发，默认采用的是hash算法。sort by保证每个reduce内有序，因此distribute by经常和sort by配合使用。生产环境中 distribute by + sort by用的多。</p><p>数据 如下 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">2020,1w</span><br><span class="line">2020,2w</span><br><span class="line">2020,1w</span><br><span class="line">2020,0.5w</span><br><span class="line">2021,10w</span><br><span class="line">2021,20w</span><br><span class="line">2021,19w</span><br><span class="line">2021,1.5w</span><br><span class="line">2022,1.3w</span><br><span class="line">2022,2w</span><br><span class="line">2022,1w</span><br><span class="line">2022,0.5w</span><br><span class="line"></span><br><span class="line">+-----------------------+--------------------------+</span><br><span class="line">| hive_distribute.year  | hive_distribute.earning  |</span><br><span class="line">+-----------------------+--------------------------+</span><br><span class="line">| 2020                  | 1w                       |</span><br><span class="line">| 2020                  | 2w                       |</span><br><span class="line">| 2020                  | 1w                       |</span><br><span class="line">| 2020                  | 0.5w                     |</span><br><span class="line">| 2021                  | 10w                      |</span><br><span class="line">| 2021                  | 20w                      |</span><br><span class="line">| 2021                  | 19w                      |</span><br><span class="line">| 2021                  | 1.5w                     |</span><br><span class="line">| 2022                  | 1.3w                     |</span><br><span class="line">| 2022                  | 2w                       |</span><br><span class="line">| 2022                  | 1w                       |</span><br><span class="line">| 2022                  | 0.5w                     |</span><br><span class="line">+-----------------------+--------------------------+</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>建表 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create table hive_distribute(</span><br><span class="line">year string,</span><br><span class="line">earning string</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>执行语句 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">select  *  from hive_distribute distribute by year   sort by earning;</span><br><span class="line"></span><br><span class="line">+-----------------------+--------------------------+</span><br><span class="line">| hive_distribute.year  | hive_distribute.earning  |</span><br><span class="line">+-----------------------+--------------------------+</span><br><span class="line">| 2022                  | 0.5w                     |</span><br><span class="line">| 2020                  | 0.5w                     |</span><br><span class="line">| 2022                  | 1.3w                     |</span><br><span class="line">| 2021                  | 1.5w                     |</span><br><span class="line">| 2021                  | 10w                      |</span><br><span class="line">| 2021                  | 19w                      |</span><br><span class="line">| 2022                  | 1w                       |</span><br><span class="line">| 2020                  | 1w                       |</span><br><span class="line">| 2020                  | 1w                       |</span><br><span class="line">| 2021                  | 20w                      |</span><br><span class="line">| 2022                  | 2w                       |</span><br><span class="line">| 2020                  | 2w                       |</span><br><span class="line">+-----------------------+--------------------------+</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Cluster-By"><a href="#Cluster-By" class="headerlink" title="Cluster By"></a>Cluster By</h2><p>ClusterByis a short-cut for both DistributeByand Sort By.</p><p>distributeby year   sort by year  《&#x3D;》 ClusterBy year 正确</p><p>当distribute by 和 sort by字段相同时，可以使用cluster by。<br>cluster by除了具有distribute by的功能外还兼具sort by的排序功能。但是排序只能是默认的升序，无法指定排序规则。</p><h3 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h3><p>hdfs上的文件 ，本地文件会找不到文件，一般只能识别hdfs上的</p><p>分桶表是对列值取哈希值的方式，将不同数据放到不同文件中存储。 对于 hive 中每一个表、分区都可以进一步进行分桶。 由列的哈希值除以桶的个数来决定每条数据划分在哪个桶中。</p><p>要先开启分桶支持</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.enforce.bucketing=true;</span><br></pre></td></tr></table></figure><p>分桶表的創建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[CLUSTERED BY (col_name, col_name, ...) </span><br><span class="line"> INTO num_buckets BUCKETS]</span><br></pre></td></tr></table></figure><p>数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1,name1</span><br><span class="line">2,name2</span><br><span class="line">3,name3</span><br><span class="line">4,name4</span><br><span class="line">5,name5</span><br><span class="line">6,name6</span><br><span class="line">7,name7</span><br><span class="line">8,name8</span><br></pre></td></tr></table></figure><p>创建表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table hive_bucket(</span><br><span class="line">id int,</span><br><span class="line">name string </span><br><span class="line">)</span><br><span class="line">clustered by (id) into 4 buckets</span><br><span class="line">row format delimited fields terminated by &quot;,&quot;;</span><br></pre></td></tr></table></figure><p>查询桶中数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select id,name from hive_bucket tablesample(bucket 4 out of 4 on name); //bucket后面的数字就是我们要查看的桶的编号 out of 后面的是总数 ，on 后面的是我们分桶的属性</span><br></pre></td></tr></table></figure><p>mapreduce:</p><p>hash % reducetask个数</p><p>文件存储格式</p><ul><li>行式存储  ：<ul><li>里面的列 掺杂很多数据类型</li><li>一行内容所有的列都在一个 block里面</li><li>行式存储加载所 是把所有的列都查询出来 再过滤出 用户需要的列</li><li>如果用户 仅仅查几个字段  &#x3D;》 磁盘io 开销比较大</li><li>textfile 文本文件</li><li>SequenceFile 文本文件</li></ul></li><li>列式存储 ：<ul><li>按照列进行存储</li><li>前提： 企业 table 字段 几十个 到几百个</li><li>RCFile  &#x3D;》 行 &#x3D;》 列</li><li>ORC Files + Parquet</li><li>查询几个列</li><li>加载表中所有字段</li></ul></li><li>列式存储文件 数据量 比 行式存储的数据量少 【前提 都采用压缩】</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table hive_distribute_col(</span><br><span class="line">year string,</span><br><span class="line">earning string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;,&#x27;</span><br><span class="line">stored as orc; // 这个就是存储形式</span><br></pre></td></tr></table></figure><h2 id="hive-中文件存储格式-vs-压缩"><a href="#hive-中文件存储格式-vs-压缩" class="headerlink" title="hive 中文件存储格式 vs 压缩"></a>hive 中文件存储格式 vs 压缩</h2><p>压缩格式 ：</p><p>Hive支持的压缩格式有bzip2、gzip、deflate、snappy、lzo等。Hive依赖Hadoop的压缩方法，所以Hadoop版本越高支持的压缩方法越多，可以在$HADOOP_HOME&#x2F;conf&#x2F;core-site.xml中进行配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;  </span><br><span class="line">        &lt;name&gt;io.compression.codecs&lt;/name&gt;  </span><br><span class="line">        &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec</span><br><span class="line">        &lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>常见的压缩格式 ：</p><table><thead><tr><th>压缩格式</th><th>算法实现</th><th>压缩比</th><th>效率</th><th>可切分</th><th>内置</th><th>扩展名</th><th>Native</th><th>Java</th><th>描述</th></tr></thead><tbody><tr><td>bzip2</td><td>bzip2</td><td>最高</td><td>慢</td><td>yes</td><td>Y</td><td>.bz2</td><td>yes</td><td>yes</td><td>压缩率最高，一般是源文件的30%左右 ：<br />压缩或者解压效率最慢</td></tr><tr><td>deflate</td><td>DEFLATE</td><td>高</td><td>慢</td><td>no</td><td>Y</td><td>.deflate</td><td>no</td><td>yes</td><td>标准的压缩格式</td></tr><tr><td>gzip</td><td>DEFLATE</td><td>高</td><td>慢</td><td>no</td><td>Y</td><td>.gz</td><td>no</td><td>yes</td><td>相比deflate增加文件头，尾，<br />压缩率比较高，压缩或者解压的效率比较慢</td></tr><tr><td>zlib</td><td>DEFLATE</td><td>高</td><td>慢</td><td>no</td><td>Y</td><td>.zl</td><td>yes</td><td>no</td><td>相比deflate增加文件头，尾</td></tr><tr><td>lz4</td><td>lz4</td><td>最低</td><td>最快</td><td>no</td><td>Y</td><td>.zl4</td><td>yes</td><td>no</td><td>压缩率比较低，不过压缩和解压效率最快</td></tr><tr><td>lzo</td><td>lzo</td><td>较低</td><td>快</td><td>yes</td><td>N</td><td>.lzo_deflate</td><td>yes</td><td>no</td><td>压缩率比较低，不过压缩和解压效率最快</td></tr><tr><td>lzop</td><td>snappy</td><td>较低</td><td>快</td><td>yes</td><td>N</td><td>.lzo</td><td>yes</td><td>no</td><td>压缩率比较低，不过压缩和解压效率最</td></tr><tr><td>snappy</td><td>snappy</td><td>较低</td><td>快</td><td>yes</td><td>N</td><td>.snappy</td><td>yes</td><td>no</td><td>压缩率比较低，不过压缩和解压效率最</td></tr></tbody></table><p>其中压缩比bzip2 &gt; zlib &gt; gzip &gt; deflate &gt; snappy &gt; lzo &gt; lz4，在不同的测试场景中，会有差异，这仅仅是一个大概的排名情况。bzip2、zlib、gzip、deflate可以保证最小的压缩，但在运算中过于消耗时间。</p><p>从压缩性能上来看：lz4 &gt; lzo &gt; snappy &gt; deflate &gt; gzip &gt; bzip2，其中lz4、lzo、snappy压缩和解压缩速度快，压缩比低。</p><p>所以一般在生产环境中，经常会采用lz4、lzo、snappy压缩，以保证运算效率。</p><h2 id="Native-Libraries"><a href="#Native-Libraries" class="headerlink" title="Native Libraries"></a>Native Libraries</h2><p>Hadoop由Java语言开发，所以压缩算法大多由Java实现；但有些压缩算法并不适合Java进行实现，会提供本地库Native Libraries补充支持。Native Libraries除了自带bzip2, lz4, snappy, zlib压缩方法外，还可以自定义安装需要的功能库（snappy、lzo等）进行扩展。</p><p>而且使用本地库Native Libraries提供的压缩方式，性能上会有50%左右的提升。</p><p>使用命令可以查看native libraries的加载情况：<br><code>hadoop checknative -a</code></p><p>完成对Hive表的压缩，有两种方式：配置MapReduce压缩、开启Hive表压缩功能。因为Hive会将SQL作业转换为MapReduce任务，所以直接对MapReduce进行压缩配置，可以达到压缩目的；当然为了方便起见，Hive中的特定表支持压缩属性，自动完成压缩的功能。</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/12/02/12-02/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hive第三天</title>
      <link>http://example.com/2022/12/01/12-01/</link>
      <guid>http://example.com/2022/12/01/12-01/</guid>
      <pubDate>Thu, 01 Dec 2022 01:47:23 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;内部和外部表和普通表和分区表&quot;&gt;&lt;a href=&quot;#内部和外部表和普通表和分区表&quot; class=&quot;headerlink&quot; title=&quot;内部和外部表和普通表和分区表&quot;&gt;&lt;/a&gt;内部和外部表和普通表和分区表&lt;/h1&gt;&lt;p&gt;分区表 ： 提升查询效率的表&lt;/p&gt;
&lt;p&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="内部和外部表和普通表和分区表"><a href="#内部和外部表和普通表和分区表" class="headerlink" title="内部和外部表和普通表和分区表"></a>内部和外部表和普通表和分区表</h1><p>分区表 ： 提升查询效率的表</p><p>关于hive的查询 ： 对于普通表 则是要先读取所有的数据然后进行筛选的 ， 但是对于分区表，则是把数据进行分区，如果要查询的话，则是针对符合的数据进行查询</p><p>往往用分区表进行查询，普通表数据量较少的时候可以用</p><p>创建分区表 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table order(</span><br><span class="line"></span><br><span class="line">orderid int,</span><br><span class="line"></span><br><span class="line">oredergg String</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">PARTITIONED BY (dt String)</span><br><span class="line"></span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br></pre></td></tr></table></figure><p>show partitions 表名 &#x2F;&#x2F;查看现在这个表的分区</p><p>修改分区  ：</p><p>删除分区  ：<code>alter table 表名 drop partition(分区列 = &#39;分区名&#39;)</code></p><p>创建分区 ： 在建表的时候创建</p><p>导入数据 ： load&#x2F;insert</p><ul><li>load : load data (local) inpath ‘’ (overwrite) into table 表名 partition (分区名称) ：数据列数如果对不上就会出现问题 : 加上overwrite则是把一个分区的数据给覆盖掉</li><li>insert : insert into table partition(分区) …</li><li>insert into 是追加的</li><li>如果不要追加则要进行覆盖 insert 后面的 into 变成 overwrite</li></ul><h2 id="使用一个sql让所有数据落到对应的分区里"><a href="#使用一个sql让所有数据落到对应的分区里" class="headerlink" title="使用一个sql让所有数据落到对应的分区里"></a>使用一个sql让所有数据落到对应的分区里</h2><p>动态分区：相当于我们要进行分区的字段是我们的数据的字段，就可以直接用那个字段当我们的分区但是要打开一个开关</p><p><code>set hive.exec.dynamic.partition.mode=nonstrict;</code></p><p>静态分区：就是自己制定好分区的标题的</p><p>离线任务 ： 业务周期性 T+1</p><p>就是延迟一天处理</p><p>默认底层创建的是内部表</p><p>内部表 ： 受hive管控的 ： 如果有删表的操作，那么会清理干净，所有数据都会被删除</p><p>外部表 ： 如果被删除的情况下，只是hdfs上指向metastore的索引被删除了，源数据不会被删除 ，而且我们还可通过建表的方式让他们的索引再次关联上</p><p>创建外部表 ：</p><p>相互转换 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">外部转内部</span><br><span class="line">ALTER TABLE 外部表名字 SET TBLPROPERTIES (&quot;EXTERNAL&quot; = &quot;true&quot;);</span><br><span class="line"></span><br><span class="line">内部转外部</span><br><span class="line">ALTER TABLE 外部表名字 SET TBLPROPERTIES (&quot;EXTERNAL&quot; = &quot;false&quot;);</span><br><span class="line">但是这上述的EXTERNAL 是不能小写的会造成失效的问题</span><br></pre></td></tr></table></figure><h1 id="复杂的数据类型"><a href="#复杂的数据类型" class="headerlink" title="复杂的数据类型"></a>复杂的数据类型</h1><p>中小企业用的不多，，大企业用的多</p><p>会建表 ，会查询</p><p>maps: <code>MAP&lt;primitive_type, data_type&gt; </code></p><p>数据如下 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1,zhangsan,father:xiaoming#mother:xiaohuang#brother:xiaoxu,28</span><br><span class="line">2,lisi,father:mayun#mother:huangyi#brother:guanyu,22</span><br><span class="line">3,wangwu,father:wangjianlin#mother:ruhua#sister:jingtian,29</span><br><span class="line">4,mayun,father:mayongzhen#mother:angelababy,26</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table hive_map(</span><br><span class="line">id int  comment &#x27;用户id&#x27;,</span><br><span class="line">name string comment &#x27;用户名字&#x27;,</span><br><span class="line">relation map&lt;string,string&gt; comment &#x27;家庭成员&#x27;,</span><br><span class="line">age int comment &#x27;年龄&#x27;</span><br><span class="line">)</span><br><span class="line">row format  delimited fields terminated by &#x27;,&#x27;</span><br><span class="line">collection items terminated by &#x27;#&#x27;</span><br><span class="line">map keys terminated by &#x27;:&#x27;;</span><br></pre></td></tr></table></figure><p>arrays:  <code>ARRAY&lt;data_type&gt;</code></p><p>数据</p><p><code>zihan   beijing,shanghai,chengdu,dalian</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table hive_array(</span><br><span class="line">name String,</span><br><span class="line">locations array&lt;String&gt;</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">collection items terminated by &#x27;,&#x27;;</span><br></pre></td></tr></table></figure><p>structs:<code>STRUCT&lt;col_name : data_type [COMMENT col_comment], ...&gt;</code></p><p>数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">192.168.1.1#zhangsan:40</span><br><span class="line">192.168.1.2#lisi:50</span><br><span class="line">192.168.1.3#wangwu:60</span><br><span class="line">192.168.1.4#zhaoliu:70</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table hive_struct(</span><br><span class="line">ip string,</span><br><span class="line">userinfo STRUCT&lt;name:string,age:int&gt;</span><br><span class="line">)</span><br><span class="line">row format  delimited fields terminated by &#x27;#&#x27;</span><br><span class="line">collection items terminated by &#x27;:&#x27;;</span><br></pre></td></tr></table></figure><h1 id="数据形式的不同使用方法"><a href="#数据形式的不同使用方法" class="headerlink" title="数据形式的不同使用方法"></a>数据形式的不同使用方法</h1><h2 id="array"><a href="#array" class="headerlink" title="array"></a>array</h2><p>案例分析：</p><p>1.查询每个用户第一个工作地点？</p><p>select  name ,locations[0] as first_loc_work from  hive_array;</p><p>2.查询每个人 工作地点的数量</p><p>select  name , size(locations) from  hive_array ;</p><p>3.查询在shanghai 工作的有哪些人</p><p>select  * from hive_array  where array_contains(locations,’shanghai’);</p><h3 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h3><p>思路是先把一个array的元素炸开，然后通过显示出来</p><p>显示手段 ： LATERAL VIEW（侧写视图）</p><ul><li><p>LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p></li><li><p>udtf : 一进多出</p></li><li><p>FROM baseTable (lateralView)*</p></li><li><p>最终代码 ：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> name,location</span><br><span class="line"><span class="keyword">from</span> hive_array <span class="keyword">lateral</span> <span class="keyword">view</span> explode(locations) loc_table <span class="keyword">as</span> location;</span><br></pre></td></tr></table></figure></li></ul><h2 id="map"><a href="#map" class="headerlink" title="map"></a>map</h2><p>需求： 1.查询表中每个人的father的名字</p><p>select id,name,age,relation[‘father’] as father from hive_map;</p><p>2.查询表中 每个人的家庭成员   keys</p><p>select id,name,age,map_keys(relation) as members from hive_map;</p><p>3.查询表中 每个人的家庭成员的名字 values</p><p>select id,name,age,map_values(relation) as members from hive_map;</p><p>4.查询表中 有brother的人以及brother的名字</p><p>select<br> id,name,age,relation[‘brother’] as brother<br>from hive_map<br>where<br>relation[‘brother’] is not null;</p><p>或者可以</p><p>select<br> id,name,age,relation[‘brother’] as brother<br>from hive_map<br>where<br>array_contains(map_keys(relation), ‘brother’);</p><p>&#x2F;&#x2F; map_key()函数的意思是可以把这个列的map的key当作array取出来</p><h2 id="structs"><a href="#structs" class="headerlink" title="structs"></a>structs</h2><p>select ip,userinfo.name as name ,userinfo.age as age from hive_struct;</p><h2 id="开窗函数-："><a href="#开窗函数-：" class="headerlink" title="开窗函数 ："></a>开窗函数 ：</h2><ul><li>分析函数：对开窗函数的分析的函数<ul><li>rank : 使用方法 rank()over(partition by xx order by yy) as rk  : 如果有重复的数据，会丢失排名</li><li>dense_rank :使用方法同上 ： 如果有重复数据 ，则不会丢失排名 ：</li><li>row_number:同上 ： 排名相同且不会重复 ， 就是会顺序往下 ：</li></ul></li></ul><p>上述的常用手段 ： 求topn的排名</p><p>比如要求top3 的</p><p>作业 ：</p><p>统计每个店铺的uv</p><p>统计top3的用户记录</p><p>pv ： 页面的浏览量</p><p>uv ： 访客的次数</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/12/01/12-01/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hive第二天</title>
      <link>http://example.com/2022/11/30/11-30/</link>
      <guid>http://example.com/2022/11/30/11-30/</guid>
      <pubDate>Wed, 30 Nov 2022 01:57:50 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;关于hive里的数据类型&quot;&gt;&lt;a href=&quot;#关于hive里的数据类型&quot; class=&quot;headerlink&quot; title=&quot;关于hive里的数据类型&quot;&gt;&lt;/a&gt;关于hive里的数据类型&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;整数&lt;ul&gt;
&lt;li&gt;int&lt;/li&gt;
&lt;li&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="关于hive里的数据类型"><a href="#关于hive里的数据类型" class="headerlink" title="关于hive里的数据类型"></a>关于hive里的数据类型</h1><ul><li>整数<ul><li>int</li><li>bigint &#x3D;&#x3D;long</li></ul></li><li>小数 ：<ul><li>float</li><li>double</li><li>Decimal</li></ul></li><li>字符串：<ul><li>String (建议统一用String)</li><li>varchar</li><li>char</li></ul></li><li>时间：<ul><li>时间日期 DATE 格式：YYYY-MM-DD</li><li>时间戳：TIMESTAMP YYYY-MM-DD HH:MM:SS</li></ul></li></ul><h1 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h1><p>CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name<br>  [(col_name data_type [column_constraint_specification] [COMMENT col_comment], … [constraint_specification])]<br>  [COMMENT table_comment]<br>  [PARTITIONED BY (col_name data_type [COMMENT col_comment], …)]<br>  [CLUSTERED BY (col_name, col_name, …) [SORTED BY (col_name [ASC|DESC], …)] INTO num_buckets BUCKETS]<br>  [<br>   [ROW FORMAT row_format]<br>   [STORED AS file_format]<br>     | STORED BY ‘storage.handler.class.name’ [WITH SERDEPROPERTIES (…)]  – (Note: Available in Hive 0.6.0 and later)<br>  ]</p><p>数据字段名字 字段类型</p><p>例如</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">create table mytest(</span><br><span class="line">id String comment &#x27;用户id&#x27;,</span><br><span class="line">name string,</span><br><span class="line">age bigint</span><br><span class="line">) comment &#x27;第一个表&#x27;</span><br><span class="line">ROW FORMAT delimited fields terminated by &#x27;,&#x27; //指定分隔符</span><br><span class="line">STORED as TEXTFILE; // 存储形式</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"></span><br><span class="line">create table emp2 like emp;</span><br><span class="line">复制表结构</span><br><span class="line"></span><br><span class="line">或者</span><br></pre></td></tr></table></figure><h2 id="为什么要分隔符"><a href="#为什么要分隔符" class="headerlink" title="为什么要分隔符"></a>为什么要分隔符</h2><p>因为我们的元数据都在hdfs上，对于hdfs上的数据可以通过分隔符进行自动导入到hive里，比如上述是，分割的，然后我hdfs上有如下数据</p><p>1，zihan,11</p><p>2,zhangsan,23</p><p>3,liu,33</p><p>就会自动按照每一行进行insert</p><p>导入数据 ： load data local inpath ‘本地的绝对路径’ into table 表名</p><p>清空表的操作 ： truncate table 表名</p><h1 id="删除库"><a href="#删除库" class="headerlink" title="删除库"></a>删除库</h1><p>DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];</p><p>DROP DATABASE bigdata_hive4;</p><p>DROP DATABASE bigdata_hive2 CASCADE; &#x3D;&gt;删库跑路的操作</p><p>CASCADE : 代表联合删除 ，一般删除的时候如果里面有表，会造成无法删除的问题，但是联合删除会直接删除掉</p><h1 id="DMl"><a href="#DMl" class="headerlink" title="DMl"></a>DMl</h1><h2 id="load-："><a href="#load-：" class="headerlink" title="load ："></a>load ：</h2><ul><li>加载本地数据</li><li>加载hdfs上的数据</li></ul><p>LOAD原本是追加，不是覆盖 ， 但是可以通过 加上 overwrite 关键字 进行 覆盖操作</p><h2 id="覆盖例子"><a href="#覆盖例子" class="headerlink" title="覆盖例子"></a>覆盖例子</h2><p>load data local inpath ‘&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;emp.txt’ OVERWRITE INTO TABLE emp;</p><h2 id="上传"><a href="#上传" class="headerlink" title="上传"></a>上传</h2><p>本地：load data inpath ‘本地路径’ into table 表名</p><p>hdfs ： load data inpath ‘hdfs上的路径’ into table 表名</p><p>上述的hdfs上的相当于把其路径里的文件移动到table 表名的下面 并且改名，且关联到metastore</p><p>但是我们的hdfs mv 是不会关联到metastore的</p><p>在hive 里 update 和 delete 不要做 &#x3D;》 因为效率低下</p><p>把所有的update和delete都转化成insert和overwrite</p><h2 id="插入语句-："><a href="#插入语句-：" class="headerlink" title="插入语句 ："></a>插入语句 ：</h2><p>Inserting data into Hive Tables from queries</p><p>insert into|OVERWRITE table tablename selectQury</p><p>2.Inserting values into tables from SQL 【不推荐使用】<br>INSERT INTO TABLE tablename<br>VALUES values_row [, values_row …]<br>1.每导入一条数据 就会触发一次 mapreduce job  效率太低</p><p>emp2：<br>    insert into table emp2<br>    select *  from emp;<br>insert overwrite table emp2<br>select *  from emp where deptno&#x3D;10;</p><h2 id="关于hive里的一些函数以及使用"><a href="#关于hive里的一些函数以及使用" class="headerlink" title="关于hive里的一些函数以及使用"></a>关于hive里的一些函数以及使用</h2><h3 id="1-where-过滤条件"><a href="#1-where-过滤条件" class="headerlink" title="1.where 过滤条件"></a>1.where 过滤条件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">where_condition</span><br><span class="line"> &lt;</span><br><span class="line"> &gt;</span><br><span class="line"> =</span><br><span class="line"> &lt;&gt;  !=</span><br><span class="line"> and</span><br><span class="line"> or</span><br><span class="line"> in</span><br><span class="line"> not in</span><br><span class="line"> between  and</span><br><span class="line"> is</span><br><span class="line"> is not</span><br></pre></td></tr></table></figure><h3 id="需求：查询表中-deptno-20-10"><a href="#需求：查询表中-deptno-20-10" class="headerlink" title="需求：查询表中 deptno 20 10"></a>需求：查询表中 deptno 20 10</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">*</span><br><span class="line">from emp</span><br><span class="line">where deptno=20 or deptno =10;</span><br><span class="line"></span><br><span class="line">select</span><br><span class="line">*</span><br><span class="line">from emp</span><br><span class="line">where deptno in (10,20);</span><br><span class="line"></span><br><span class="line">select</span><br><span class="line">*</span><br><span class="line">from emp</span><br><span class="line">where deptno &lt;&gt; 20;</span><br><span class="line">select</span><br><span class="line">*</span><br><span class="line">from emp</span><br><span class="line">where deptno != 20;</span><br></pre></td></tr></table></figure><h3 id="2-order-by-排序语法"><a href="#2-order-by-排序语法" class="headerlink" title="2.order by  排序语法"></a>2.order by  排序语法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.默认asc 升序</span><br><span class="line">2.降序 desc</span><br><span class="line"></span><br><span class="line">select</span><br><span class="line">sal</span><br><span class="line">from emp</span><br><span class="line">order by sal desc;</span><br></pre></td></tr></table></figure><h3 id="3-like-语法-模糊匹配"><a href="#3-like-语法-模糊匹配" class="headerlink" title="3.like 语法 模糊匹配"></a>3.like 语法 模糊匹配</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1._  占位符</span><br><span class="line">2.%  模糊</span><br><span class="line">rlike regexp</span><br></pre></td></tr></table></figure><h3 id="4-合并表"><a href="#4-合并表" class="headerlink" title="4.合并表"></a>4.合并表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">union  去重</span><br><span class="line"></span><br><span class="line">union all  不去重</span><br></pre></td></tr></table></figure><h3 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">create table a(id int ,name string) row format  delimited fields terminated by &#x27;,&#x27; ;</span><br><span class="line">create table b(id int ,name string) row format  delimited fields terminated by &#x27;,&#x27; ;</span><br><span class="line"></span><br><span class="line">load data local inpath &quot;/home/hadoop/tmp/a.txt&quot; into table a;</span><br><span class="line">load data local inpath &quot;/home/hadoop/tmp/b.txt&quot; into table b;</span><br><span class="line"></span><br><span class="line">select name from a</span><br><span class="line">union all</span><br><span class="line">select name from b;</span><br><span class="line"></span><br><span class="line">select name from a</span><br><span class="line">union all</span><br><span class="line">select name from b</span><br><span class="line">union all</span><br><span class="line">select &quot;lisi&quot; as name ;</span><br><span class="line"></span><br><span class="line">select name,&quot;1&quot; as pk from a</span><br><span class="line">union all</span><br><span class="line">select name,&quot;2&quot; as pk from b</span><br><span class="line">union all</span><br><span class="line">select &quot;lisi&quot; as name,&quot;3&quot; as id ;</span><br></pre></td></tr></table></figure><p>思考： hive建表 默认column 分割符是什么？</p><h3 id="5-null-处理"><a href="#5-null-处理" class="headerlink" title="5.null 处理"></a>5.null 处理</h3><pre><code>1. 过滤    where xxx is not nullis null 作用一样 &lt;=&gt;2. etl 转换    ifnull  =&gt; hive里没有    coalesce =》    nvl  =》</code></pre><h3 id="补充："><a href="#补充：" class="headerlink" title="补充："></a>补充：</h3><pre><code>查看hive支持的function ：            y=f(x)    SHOW FUNCTIONS [LIKE &quot;`&lt;pattern&gt;`&quot;];    show functions like nvl;  =&gt; 判断 function hive 是否存在    desc function nvl; =》  查看某个函数具体使用</code></pre><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">empno,</span><br><span class="line">ename,</span><br><span class="line">job,</span><br><span class="line">mgr,</span><br><span class="line">hiredate,</span><br><span class="line">sal,</span><br><span class="line">nvl(comm,0) as comm_alias,</span><br><span class="line">deptno</span><br><span class="line">from emp ;</span><br></pre></td></tr></table></figure><h2 id="分组-聚合函数-join"><a href="#分组-聚合函数-join" class="headerlink" title="分组 聚合函数 join"></a>分组 聚合函数 join</h2><p>聚合函数 ：</p><ul><li>sum</li><li>max</li><li>min</li><li>avg</li><li>count</li></ul><h3 id="group-by"><a href="#group-by" class="headerlink" title="group by"></a>group by</h3><ul><li>和聚合函数一起使用</li><li>一个或者多个colum进行分组</li><li>字段必须select出现 和 group by 出现要一致</li></ul><h3 id="having-："><a href="#having-：" class="headerlink" title="having ："></a>having ：</h3><ul><li>在group by 后面使用</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">select job,</span><br><span class="line"></span><br><span class="line">sum(sal) as sal_num,</span><br><span class="line"></span><br><span class="line">max(sal),</span><br><span class="line"></span><br><span class="line">min(sal),</span><br><span class="line"></span><br><span class="line">avg(sal),</span><br><span class="line"></span><br><span class="line">count(1) as cnt</span><br><span class="line"></span><br><span class="line">from emp</span><br><span class="line"></span><br><span class="line">group by job</span><br><span class="line"></span><br><span class="line">having sal_num &gt; 6000</span><br></pre></td></tr></table></figure><h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p>找准关联字段</p><ul><li>inner join [join]</li><li>left join</li><li>right join</li><li>full join</li></ul><h3 id="需求：既要显示聚合前的数据，又要显示聚合后的数据？"><a href="#需求：既要显示聚合前的数据，又要显示聚合后的数据？" class="headerlink" title="需求：既要显示聚合前的数据，又要显示聚合后的数据？"></a>需求：既要显示聚合前的数据，又要显示聚合后的数据？</h3><p>函数  over([partition by xxx,…] [order by xxx,….])</p><p>over: 以谁进行开窗 table、<br>parition by : 以谁进行分组   table columns<br>order by : 以谁进行排序  table columns</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">数据： </span><br><span class="line">haige,2022-11-10,1</span><br><span class="line">haige,2022-11-11,5</span><br><span class="line">haige,2022-11-12,7</span><br><span class="line">haige,2022-11-13,3</span><br><span class="line">haige,2022-11-14,2</span><br><span class="line">haige,2022-11-15,4</span><br><span class="line">haige,2022-11-16,4</span><br></pre></td></tr></table></figure><p>需求：<br>    统计累计问题 ，每个用户每天累计点外卖次数</p><p>[partition by xxx,…] [order by xxx,….]</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">name ,</span><br><span class="line">dt ,</span><br><span class="line">cnt ,</span><br><span class="line">sum(cnt) over(partition by name  order by dt ) as sum_cnt</span><br><span class="line">from user_mt;</span><br></pre></td></tr></table></figure><h3 id="命令行更改"><a href="#命令行更改" class="headerlink" title="命令行更改"></a>命令行更改</h3><p>command line<br>    1.hive shell<br>    2.jdbc &#x3D;&gt; hiveServer2</p><pre><code>hive clinet:    1. hive shell    2. beeline shell jdbc   开启 hiveServer2 服务 thift</code></pre><p>在beeline中 <code>!connect jdbc:hive2://localhost:10000 hadoop</code></p><p>补充：<br>beeline &#x3D;&gt; 连接 hive  &#x3D;》 hdfs<br>对hdfs 做一个设置 代理设置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">core-site.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/11/30/11-30/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hive</title>
      <link>http://example.com/2022/11/29/11-29/</link>
      <guid>http://example.com/2022/11/29/11-29/</guid>
      <pubDate>Tue, 29 Nov 2022 00:36:32 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;简单来说就是用sql处理hadoop的数据的&lt;/p&gt;
&lt;p&gt;除了hive之外 ： sparksql ，presto ， impala&lt;/p&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>简单来说就是用sql处理hadoop的数据的</p><p>除了hive之外 ： sparksql ，presto ， impala</p><h2 id="要求掌握"><a href="#要求掌握" class="headerlink" title="要求掌握"></a>要求掌握</h2><p>sql</p><p>udf</p><h2 id="hadoop包含"><a href="#hadoop包含" class="headerlink" title="hadoop包含"></a>hadoop包含</h2><p>hdfs ：命令行</p><p>mapreduce ：目前工作中几乎不用，但是关于核心类和思想要掌握</p><p>yarn：提交作业 xxx （mr&#x2F;spark&#x2F;flink）on yarn ： 必须会</p><p>mapreduce的弊端 ：</p><ul><li>开发大量代码</li><li>编程基础不错</li><li>部署麻烦</li><li>修改code蛮麻烦</li><li>对于DBA和RDBMS的小伙伴是不友好的</li></ul><p>大数据处理来说最终落地最好是sql</p><p>大数据开发角度 ：</p><ul><li>基础平台开发<ul><li>涉及很多框架底层的面很广</li></ul></li><li>应用层面开发<ul><li>基于基础平台开发 ，写sql</li></ul></li></ul><p>根据你的兴趣点 + 公司的定位</p><p>必然有新的东西诞生去解决一个场景的问题</p><p>mr主要适用于我们的p计算（离线计算）：不要求时效性 ，但是mr开发太麻烦，就诞生了hive</p><p>hive 介绍 ：</p><p>hive.apache.org</p><p>是由什么人提供出来的？</p><p>facebook 开源 去解决结构化的数据统计问题</p><p>是什么？</p><p>构建在hadoop之上的数据仓库</p><p>hdfs：hive 的数据是在hdfs之上的</p><p>yarn：可以跑在yarn之上</p><p>mmapreduce ： 可以用mr形式去运行</p><p>如何使用：</p><p>定义了一中类sql的语言，类似sql但是又有不同</p><p>适用于离线&#x2F;p处理的</p><p>开发就是写sql &#x3D;》 mr  &#x3D;》 运行在yarn上</p><p>hive的底层引擎是：</p><ul><li>mr （默认）：sql&#x3D;》mr</li><li>Tez：sql&#x3D;》Tez</li><li>Spark：sql&#x3D;》Spark</li></ul><p>hive on spark &#x3D;》生厂上用的用的不多</p><p>spark on hive &#x3D;》 用sparksql查看hive的数据</p><p>hive的存储格式，压缩格式等</p><p>官网：</p><ul><li>in distributed storage （分布式存储）：hdfs , cos,oss,aws</li><li>A command line tool and JDBC driver are provided to connect users to Hive.</li></ul><p>版本介绍 ：</p><p>x.y.z：x是大版本，y是小版本，z是小版本的修复版本</p><p>为什么要学习hive</p><ul><li>简单易用 ： 可以用sql开发</li><li>扩展性好：<ul><li>用自定义函数udf</li><li>数据存储 和 计算角度 ： 如果表中数据存不下，可以加几个节点就好了<ul><li>注意hive不是分布式的，它仅仅是个客户端</li></ul></li><li>Metastore ：hive的元数据管理<ul><li>sparksql ,presto ,impala 只要可以访问hive的元数据就可以访问里面表的数据</li><li>可以分享元数据</li></ul></li></ul></li></ul><p>hive的架构 ：</p><ul><li>元数据 ： 描述数据的数据<ul><li>表的名字，字段的名字，字段的类型，什么人创建的，数据存储在哪里等</li><li>元数据的内部内置了一个Derby但是有弊端</li><li>元数据都是用mysql进行存储的</li><li>测试的时候一个mysql就可以了</li><li>但是生产上则一个不够</li><li>生产上要遵循 HA ：高可用</li><li>就是要一台做备份</li><li>两个mysql是主从架构</li></ul></li></ul><p>hive和RDBMS的区别</p><ul><li>共同点 sql</li><li>延时性 ： hive 适用于离线计算 慢，千万不要拿hive和mysql的性能做对比，无可比性，mysql要必hive高（数据量小）数据量大的时候就反过来了</li><li>事务 ： 都支持</li><li>update ，delete<ul><li>上面两个语句在hive里基本不用，因为性能太差了</li></ul></li><li>都支持分布式</li><li>部署成本 ：<ul><li>hive ：廉价</li><li>mysql ：成本很高</li></ul></li><li>数据体量<ul><li>hive ：Tb</li><li>mysql：处理Pb都比较费劲</li></ul></li></ul><p>对hive进行部署</p><p>分布式部署 ： cwiki.apache.org(官网)</p><p>首先把用压缩包上传到linux机器上</p><p>然后对linux进行解压</p><p>解压完成之后，我们要对他的的配置进行修改在解压之后的</p><p><code>vim hive-site.xml</code></p><p>编辑完成之后我们把以下内容放进去</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;jdbc:mysql://bigdata2:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;liuzihan010616&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>然后在环境变量中编辑我们的hive_home</p><p><code>vim ~/.bashrc </code></p><p>把下列加进去</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HIVE_HOME</span></span><br><span class="line">export HIVE_HOME=/home/hadoop/app/hive</span><br><span class="line">export PATH=$&#123;PATH&#125;:$&#123;HIVE_HOME&#125;/bin</span><br></pre></td></tr></table></figure><p>然后我们要把我们的mysql链接包放在lib文件夹下</p><p>就是 <code>mysql-connector-java-5.1.28.jar</code></p><p>要放在hive的lib文件夹下</p><p>然后对我们的hive进行初始化</p><p><code>schematool -dbType mysql -initSchema</code></p><p>初始化成功之后，先启动hdfs ，然后命令行输入 hive 然后输入show databases;</p><p>成功就ok了</p><p>然后再mysql的数据库里会有hive这个数据库</p><p>mysql里\G是格式化的意思</p><h2 id="思考-：-表中的字段-存在哪里？"><a href="#思考-：-表中的字段-存在哪里？" class="headerlink" title="思考 ： 表中的字段 存在哪里？"></a>思考 ： 表中的字段 存在哪里？</h2><p>我们先进入hive ： <code>create table test(name String); </code></p><p>一个 hive 表 会被拆分成n个表存储再mysql里</p><p>比如　： TBl表存放的是我们的表名 <code>select * from tbls \G;</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">*************************** 1. row ***************************</span><br><span class="line">            TBL_ID: 1</span><br><span class="line">       CREATE_TIME: 1669707861</span><br><span class="line">             DB_ID: 1</span><br><span class="line">  LAST_ACCESS_TIME: 0</span><br><span class="line">             OWNER: hadoop</span><br><span class="line">        OWNER_TYPE: USER</span><br><span class="line">         RETENTION: 0</span><br><span class="line">             SD_ID: 1</span><br><span class="line">          TBL_NAME: test</span><br><span class="line">          TBL_TYPE: MANAGED_TABLE</span><br><span class="line">VIEW_EXPANDED_TEXT: NULL</span><br><span class="line">VIEW_ORIGINAL_TEXT: NULL</span><br><span class="line">IS_REWRITE_ENABLED:  </span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>比如 ： columns_v2表存放的是我们的字段 <code>select * from columns_v2 \G;</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">*************************** 1. row ***************************</span><br><span class="line">      CD_ID: 1</span><br><span class="line">    COMMENT: NULL</span><br><span class="line">COLUMN_NAME: name</span><br><span class="line">  TYPE_NAME: string</span><br><span class="line">INTEGER_IDX: 0</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>比如 ： DBS是存放的我们的物理存储路径的 <code>select * from DBS \G;</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">*************************** 1. row ***************************</span><br><span class="line">          DB_ID: 1</span><br><span class="line">           DESC: Default Hive database</span><br><span class="line">DB_LOCATION_URI: hdfs://bigdata3:9000/user/hive/warehouse</span><br><span class="line">           NAME: default</span><br><span class="line">     OWNER_NAME: public</span><br><span class="line">     OWNER_TYPE: ROLE</span><br><span class="line">      CTLG_NAME: hive</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><h2 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a>DDL</h2><p>hive有个默认数据库 是default 路径 ：&#x2F;user&#x2F;hive&#x2F;warehouse</p><p>非默认数据库 ： &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;dbname.db</p><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><p>CREATE [REMOTE] (DATABASE|SCHEMA) [IF NOT EXISTS] database_name<br>  [COMMENT database_comment]<br>  [LOCATION hdfs_path]<br>  [MANAGEDLOCATION hdfs_path]<br>  [WITH DBPROPERTIES (property_name&#x3D;property_value, …)];</p><p>[] 可有可无<br>(|) 选择其中一个即可</p><p>CREATE DATABASE 名称；</p><p>CREATE DATABASE 名称 LOCATION ‘创建的地方’；</p><p>例子 ：</p><p>create database if not exists bigdata_hive;<br>create database  bigdata_hive2  LOCATION ‘&#x2F;data&#x2F;bigdata_hive2’;<br>create database  bigdata_hive3 WITH DBPROPERTIES (‘creator’&#x3D;’doublehappy’, ‘create_dt’&#x3D;”2099-11-29”);<br>create database if not exists bigdata_hive4 COMMENT “这是一个数据库4”;</p><p>解决此处中文乱码的问题</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE hive.dbs MODIFY COLUMN `DESC` varchar(4000) CHARACTER SET utf8 COLLATE utf8_general_ci NULL;</span><br></pre></td></tr></table></figure><h3 id="显示数据库"><a href="#显示数据库" class="headerlink" title="显示数据库"></a>显示数据库</h3><p>show databases;<br>show databases like “bigdata_hive*”<br>desc database  bigdata_hive3;<br>desc database EXTENDED bigdata_hive3;</p><h2 id="思考-：-这个数据库在hdfs的哪一个地方"><a href="#思考-：-这个数据库在hdfs的哪一个地方" class="headerlink" title="思考 ： 这个数据库在hdfs的哪一个地方"></a>思考 ： 这个数据库在hdfs的哪一个地方</h2><p>可以通过查看DBS表</p><h1 id="hive-的注释-comment-中文乱码的解决方法"><a href="#hive-的注释-comment-中文乱码的解决方法" class="headerlink" title="hive 的注释(comment) 中文乱码的解决方法"></a>hive 的注释(comment) 中文乱码的解决方法</h1><p>（1）修改表字段注解和表注解</p><p>alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;<br>alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</p><p>（2）修改分区字段注解</p><p>alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ;<br>alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;</p><p>（3）修改索引注解</p><p>alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</p><p>修改hive-site.xml配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://IP:3306/db_name?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>上述的 <code>&amp; 是 &amp;amp; </code></p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/11/29/11-29/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>yarn</title>
      <link>http://example.com/2022/11/28/11-28/</link>
      <guid>http://example.com/2022/11/28/11-28/</guid>
      <pubDate>Mon, 28 Nov 2022 00:34:31 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;yarn&quot;&gt;&lt;a href=&quot;#yarn&quot; class=&quot;headerlink&quot; title=&quot;yarn&quot;&gt;&lt;/a&gt;yarn&lt;/h1&gt;&lt;h2 id=&quot;架构&quot;&gt;&lt;a href=&quot;#架构&quot; class=&quot;headerlink&quot; title=&quot;架构&quot;&gt;&lt;/a&gt;架构&lt;/h</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h1><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>主从架构 ： resourcemanager(资源的分配) : nodemanager(资源的供给与隔离)</p><h3 id="资源调度"><a href="#资源调度" class="headerlink" title="资源调度"></a>资源调度</h3><p>通过rm把nm的资源分配给我们的 task上</p><h3 id="资源隔离"><a href="#资源隔离" class="headerlink" title="资源隔离"></a>资源隔离</h3><p>nm按照要求给task提供资源，保证提供的资源具有独占性</p><h3 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h3><p>nm指挥分配的资源</p><p>一个task对应一个container ： cpu&#x2F;mem（cpu和内存）</p><p>每个container之间是相互隔离的</p><h2 id="yarn的架构设计"><a href="#yarn的架构设计" class="headerlink" title="yarn的架构设计"></a>yarn的架构设计</h2><p>作业提交的流程：<br>client &#x3D;&gt; 给rm 的apps 发送请求 去运行 jar （app master）</p><p>apps 分配一个container 去运行 app master</p><p>app master 会向apps manager 去注册我的作业</p><p>app master 向resource scheduler 申请资源去运行 我的代码</p><p>nodemanager 会开启资源 container 去运行map task 以及reduce task</p><p>tsak 会向 app master 汇报代码与运行情况</p><p>当代码运行完成 app master 会给apps 发送请求 ，通知我的作业完成了</p><p>apps manager 收到请求之后会通知你的客户端 ，告诉已经运行完成</p><p>输入阶段 ： map tsak 的个数 &#x3D;》container 的申请个数 redurce task 同理</p><h2 id="面试会问"><a href="#面试会问" class="headerlink" title="面试会问"></a>面试会问</h2><p>yarn的架构设计 &#x3D;&#x3D;mr作业提交流程</p><h2 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h2><p>FIFO ：</p><ul><li>先进先出</li><li>单队列</li></ul><p>Capacity ：容量调度器</p><ul><li>多队列</li><li>先进先出（针对一个队列）</li><li>每个队列之间互不影响</li><li>每个队列之间是事先定好的</li></ul><p>Fair：公平调度器</p><ul><li>多队列</li><li>每个队列之间的每个job是有影响的 不是先进先出</li><li>哪一个job的优先级高就执行哪一个</li><li>如果相同优先级，则是顺序</li></ul><p>主流的中小企业：Capacity</p><p>大公司会用：fair</p><h3 id="默认调度器"><a href="#默认调度器" class="headerlink" title="默认调度器"></a>默认调度器</h3><p>3.x：默认是容量调度器</p><p>通过调度器进行其作业调度</p><p>2.x: 版本是fair（默认）</p><h2 id="yarn的web界面的简介"><a href="#yarn的web界面的简介" class="headerlink" title="yarn的web界面的简介"></a>yarn的web界面的简介</h2><p>左侧侧边栏 ： 有几个选项卡 ： 分别是</p><h2 id="yarn资源的调优"><a href="#yarn资源的调优" class="headerlink" title="yarn资源的调优"></a>yarn资源的调优</h2><p>container?</p><p>一定比例的cpu和mem</p><p>刀片服务器的配置 ： 128G 16 core :假设一个机器的配置</p><p>刀片服务器 装完系统 消耗内存 1G</p><p>系统预留 ： 预留 20%左右 包含装完成系统 消耗的1G</p><p>原因 ：给未来部署组件预留空间，防止全部使用 ： 会导致系统夯住 就是卡住 ，oom机制【linux系统】：系统会自己杀死进程当内存不足的时候</p><p>预留空间 ： 128 * 0.2 &#x3D; 26G</p><p>其余空间用于大数据 102G</p><p>hadoop ：</p><ul><li>datanode  进程内存 ： 默认 1G &#x3D;》 生产上 2G</li><li>nodemanager 进程内存 ： ，默认 1G &#x3D;》 生产上 4G</li></ul><p>接下来还有96G全部给我们的yarn资源 ： 96G</p><p>container的资源分配 ：</p><p>内存</p><p>cpu</p><p>相比：cpu更重要一些</p><p>container的内存划分 ：默认是86G</p><p>其最小是 1G（默认）</p><p>最大是 8G（默认） 但是可以设置</p><p>注意 container的内存会自动增加 默认以1G递增</p><p>container cpu ： 是虚拟核 &#x3D;》 考虑初衷是不同节点的cpu性能不同</p><p>比如 ： 一个cpu是另外一个cpu的2倍</p><p>第一机器 ： pcore ：vcore &#x3D; 1：2 相当于1个物理核当成两个虚拟核用</p><p>给container的核数 ： 默认是8core</p><p>总数 ：</p><p>最小:1c  (默认)</p><p>最大:4c（默认）</p><p>实际开发角度 ：</p><p>mem：最大不要超过32G ，如果超过32G则会导致压缩指针失效</p><p>cpu ： cloudera的公司推荐一个container的core最好不要超过5</p><p>配置core  ：在yarn-site.xml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">cpu： </span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.pcores-vcores-multiplier&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="mapreduce"><a href="#mapreduce" class="headerlink" title="mapreduce"></a>mapreduce</h2><p>map task :mem  : 默认是1024m ，一个map task 申请的资源是1024m ， 但是如果实际使用的资源的内存量超过这个值，你的task会强制被杀死 ，reduce task 也一样</p><p>map task :vcore ： 默认是1</p><p>reduce task :mem : 默认是1024m</p><p>reduce task :vcore : 默认是1</p><p>mr作业是进程级别 &#x3D;》 jvm</p><p>map task</p><p>reduce task</p><p>jvm参数调优 ：</p><p>存储 hdfs</p><p>存储文件</p><p>压缩</p><ul><li>为什么使用压缩？</li><li>节省空间</li><li>节省时间<ul><li>网络io核磁盘io会减少</li><li>指的是mapreduce数据计算过程中</li><li>经过网络传输的数据会变少，</li><li>同样到磁盘上的时候，数据量少也会减少磁盘的io</li></ul></li><li>使用场景<ul><li>采用压缩 ， 对机器的cpu要求比较高</li><li>存储数据的空间不够了，才会用压缩</li></ul></li><li>两面性</li><li>采用压缩的确会让空间和时间减少</li><li>cpu消耗 cpu利用率高 &#x3D;》会导致处理的时间变长一点</li><li>如何使用压缩？</li></ul><p>常见的压缩格式</p><ul><li>gzip</li><li>bzip2</li><li>Lzo</li><li>Snappy</li><li>LZ4</li></ul><p>常见的压缩注意点：</p><p>压缩比 Bzip2 30%  GZIP    snappy、lzo 50%  ： 这个30%和50%代表能把源文件压缩到源文件的%多少</p><p>解压速度 ： snappy、lzo GZIP   Bzip2</p><p>压缩文件可不可以一被切分</p><p>假设一个 5G文件 不能被切分 split 意味着 只能使用一个map task去处理</p><p>map task  5G</p><p>假设一个 5G文件 能被切片  splits 10map task 去并行处理</p><p>5*1024 &#x2F;10 &#x3D; 一个map task 处理的数据</p><p>能否被切分 决定了 你的 一个map task处理的数据量有多少</p><p>压缩后的文件是否支持分割？<br>            gzip  不可分割<br>            bzip2  可分割<br>            lzo   带索引的可以分割 (默认是不支持分割的)<br>            snappy 不可分割的</p><p>mapreduce 每个阶段该如何采用这些算法？</p><p>input &#x3D;》 maps &#x3D;》 reduce &#x3D;》 output</p><p>input：<br>    1.Bzip2 ：支持分割 多个map task 进行出</p><p>map out：</p><ul><li>snappy 、lzo</li><li>shuffle 过程 要选择一个解压 速度快的压缩格式</li></ul><p>reduce out ：</p><ul><li>1.高的压缩比 + 支持分片  &#x3D;》 节省空间</li><li>2.bzip2  、lzo带索引的</li></ul><p>reduce out 数据 作为下一个<br>map 的输入咋办？<br>建议使用bzip2【如果采用压缩的话】</p><p>进行压缩配置的两种方式 ：</p><p>job &#x3D;》 code 针对少数job生产生效</p><p>集群所有的job 在配置文件里配置</p><p>对mapreduce 进行配置</p><ul><li>配置压缩的codec</li><li>map reduce 输出配置<ul><li>先打开压缩的开关</li><li>配置codec即可</li></ul></li><li>hadoop是不是支持哪些压缩 ，通过命令 ：或者配置文件</li><li>core-site.xml<ul><li>配置支持的压缩有什么</li><li>原生的hadoop默认不支持lzo的算法，因为lzo要把整个hadooop重新编译一遍才可以重新实行</li><li>&#96;&#96;&#96;<property>        <name>io.compression.codecs</name>        <value>org.apache.hadoop.io.compress.BZip2Codec,        org.apache.hadoop.io.compress.SnappyCodec,        org.apache.hadoop.io.compress.GzipCodec,        org.apache.hadoop.io.compress.DefaultCodec    </value></property><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mapred-site.xml:</span><br><span class="line"></span><br><span class="line">* 1.先打开压缩的开关</span><br><span class="line">* 2.map reduce 输出 压缩算法</span><br><span class="line"></span><br><span class="line">reduce： 开关</span><br><span class="line"></span><br><span class="line">* mapreduce.output.fileoutputformat.compress</span><br><span class="line">* ```</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">          &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;</span><br><span class="line">          &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure></li></ul></li><li>&#96;&#96;&#96;<property>        <name>mapreduce.output.fileoutputformat.compress.codec</name>        <value>org.apache.hadoop.io.compress.BZip2Codec</value></property><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">map阶段的：</span><br><span class="line"></span><br></pre></td></tr></table></figure>// 设置在map输出阶段压缩      conf.set("mapreduce.map.output.compress", "true");</li></ul><p>&#x2F;&#x2F; 设置解压缩编码器<br>        conf.set(“mapreduce.map.output.compress.codec”, “org.apache.hadoop.io.compress.DefaultCodec”);</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">各个datanode数据节点的平衡</span><br><span class="line"></span><br><span class="line">* DN1 存储空间 90%</span><br><span class="line">* DN2 存储空间 60%</span><br><span class="line">* DN3 存储空间 80%</span><br><span class="line"></span><br><span class="line">如何做呢？</span><br><span class="line"></span><br><span class="line">* sbin/start-balancer.sh</span><br><span class="line"></span><br><span class="line">parameters = Balancer.BalancerParameters</span><br><span class="line"></span><br><span class="line">[BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5</span><br><span class="line"></span><br><span class="line">数据平衡的默认阈值：threshold = 10.0</span><br><span class="line"></span><br><span class="line">每个节点的磁盘使用率 - 平均的磁盘使用率 &lt;10%</span><br><span class="line"></span><br><span class="line">DN1 存储空间 90%  -76% = 14% 说明这个节点数据多 往别的节点迁移数据 出</span><br><span class="line">DN2 存储空间 60%  -76% = -12% 说明这个节点数据少 别的节点迁移数据 进</span><br><span class="line">DN3 存储空间 80%  -76% = 4% 说明这个节点数据多 往别的节点迁移数据</span><br><span class="line"></span><br><span class="line">avg=90 + 80 +60 /3 = 76%</span><br><span class="line"></span><br><span class="line">生产上 从现在开始 start-balancer.sh -threshold 10  每天要定时做的</span><br><span class="line"></span><br><span class="line">放到业务低谷期去做 数据平衡操作</span><br><span class="line"></span><br><span class="line">注意： 不要在业务高峰期做</span><br><span class="line"></span><br><span class="line">1.数据平衡 数据传输  带宽有关</span><br><span class="line"></span><br><span class="line">调优参数 ：平衡的网络带宽  w</span><br><span class="line"></span><br><span class="line">dfs.datanode.balance.bandwidthPerSec 100m 【2.x 默认是10m】</span><br><span class="line"></span><br><span class="line">每个节点数据几十T  需要数据平衡的数据 几十T  可以申请维护窗口时间 ：</span><br><span class="line"></span><br><span class="line">dfs.datanode.balance.bandwidthPerSec 临时调大 200M</span><br><span class="line"></span><br><span class="line">单个DN节点 多块磁盘的数据平衡</span><br><span class="line"></span><br><span class="line">投产前规划：</span><br><span class="line">DN 机器   10块 2T 【不做raid】  =》 20T   副本的</span><br><span class="line"></span><br><span class="line">1.dn 配置多个磁盘</span><br><span class="line"></span><br></pre></td></tr></table></figure><property>        <name>dfs.datanode.data.dir</name>        <value>/data01,/data02,/data03</value></property>```<p>挂载磁盘一般最省钱的是 2W 转数 2T的</p><p>2.为什么要使用多块物理磁盘？</p><p>1.存储<br>2.因为多个磁盘的io也是叠加的<br>每块磁盘 磁盘io 每秒 100m<br>三块磁盘 1s 能 300m文件内容<br>一块磁盘 1s 100m</p><p>3.可以让服务一直运行，加入一个磁盘挂了，服务不会减少</p><p>做多个磁盘数据均衡<br>    dfs.disk.balancer.enabled  true 【3.x有这个功能 cdh 2.x 也有】 apache 2.x 没有这个功能</p><p>得通过命令去解决磁盘数据均衡？</p><p>hdfs diskbalancer</p><p>1.步骤</p><pre><code>hdfs diskbalancer -plan  bigdata32  =&gt; 生成一个  bigdata32.plan.json 文件    hdfs diskbalancer -execute bigdata32.plan.json =》 执行disk 数据均衡计划文件    hdfs diskbalancer -query bigdata32</code></pre><p>生产上 当你发现 磁盘写入不均衡 可以做一下 【一般 一个月 半个月 做一次即可】</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/11/28/11-28/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
