<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>枫叶冢</title>
    <link>http://example.com/</link>
    
    <atom:link href="http://example.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>只有努力不会辜负你</description>
    <pubDate>Fri, 02 Dec 2022 03:55:32 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>hive第四天</title>
      <link>http://example.com/2022/12/02/12-02/</link>
      <guid>http://example.com/2022/12/02/12-02/</guid>
      <pubDate>Fri, 02 Dec 2022 00:59:25 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;维度组合分析 ： &lt;/p&gt;
&lt;p&gt;sql 关键字 ： grouping sets&lt;/p&gt;
&lt;p&gt;例子  ：&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span cl</description>
        
      
      
      
      <content:encoded><![CDATA[<p>维度组合分析 ： </p><p>sql 关键字 ： grouping sets</p><p>例子  ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table user_shop (</span><br><span class="line">user_id String,</span><br><span class="line">shop_name String,</span><br><span class="line">channe String,</span><br><span class="line">os String</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;,&#x27;</span><br></pre></td></tr></table></figure><p>一般我们进行维度计算的时候，我们可以通过group by 的方式进行 </p><p>但是假如我们每次都要处理一个维度，那么我们难道要写很多个sql语句吗</p><p>这明显是不行的</p><p>那么我们如何解决呢</p><p>通过grouping sets 就可以解决了</p><p>代码如下  ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">SELECT empid,custid,</span><br><span class="line">       sum(qty) as sumqty</span><br><span class="line">FROM Orders</span><br><span class="line">GROUP BY</span><br><span class="line">    GROUPING SETS</span><br><span class="line">    (</span><br><span class="line">        (empid,custid),</span><br><span class="line">        (empid),(custid),</span><br><span class="line">        ()</span><br><span class="line">    );</span><br></pre></td></tr></table></figure><p>上面代码的意思就是 ：我要按照ｇｒｏｕｐ　ｂｙ　的方法　把empid,custid和empid和custid这几个维度都选出来，然后是上下在一起的　相当于用ｕｎｉｏｎ在一起</p><p>如果是这次选择的维度中未选择的维度，比如说　，我只选择了　维度empid　，那么custid列就会是空，但是这个比多次重复性写ｓｑｌ语句要好的多</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/12/02/12-02/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hive第三天</title>
      <link>http://example.com/2022/12/01/12-01/</link>
      <guid>http://example.com/2022/12/01/12-01/</guid>
      <pubDate>Thu, 01 Dec 2022 01:47:23 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;内部和外部表和普通表和分区表&quot;&gt;&lt;a href=&quot;#内部和外部表和普通表和分区表&quot; class=&quot;headerlink&quot; title=&quot;内部和外部表和普通表和分区表&quot;&gt;&lt;/a&gt;内部和外部表和普通表和分区表&lt;/h1&gt;&lt;p&gt;分区表 ： 提升查询效率的表&lt;/p&gt;
&lt;p&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="内部和外部表和普通表和分区表"><a href="#内部和外部表和普通表和分区表" class="headerlink" title="内部和外部表和普通表和分区表"></a>内部和外部表和普通表和分区表</h1><p>分区表 ： 提升查询效率的表</p><p>关于hive的查询 ： 对于普通表 则是要先读取所有的数据然后进行筛选的 ， 但是对于分区表，则是把数据进行分区，如果要查询的话，则是针对符合的数据进行查询</p><p>往往用分区表进行查询，普通表数据量较少的时候可以用</p><p>创建分区表 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table order(</span><br><span class="line"></span><br><span class="line">orderid int,</span><br><span class="line"></span><br><span class="line">oredergg String</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">PARTITIONED BY (dt String)</span><br><span class="line"></span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br></pre></td></tr></table></figure><p>show partitions 表名 &#x2F;&#x2F;查看现在这个表的分区</p><p>修改分区  ：</p><p>删除分区  ：<code>alter table 表名 drop partition(分区列 = &#39;分区名&#39;)</code></p><p>创建分区 ： 在建表的时候创建</p><p>导入数据 ： load&#x2F;insert</p><ul><li>load : load data (local) inpath ‘’ (overwrite) into table 表名 partition (分区名称) ：数据列数如果对不上就会出现问题 : 加上overwrite则是把一个分区的数据给覆盖掉</li><li>insert : insert into table partition(分区) …</li><li>insert into 是追加的</li><li>如果不要追加则要进行覆盖 insert 后面的 into 变成 overwrite</li></ul><h2 id="使用一个sql让所有数据落到对应的分区里"><a href="#使用一个sql让所有数据落到对应的分区里" class="headerlink" title="使用一个sql让所有数据落到对应的分区里"></a>使用一个sql让所有数据落到对应的分区里</h2><p>动态分区：相当于我们要进行分区的字段是我们的数据的字段，就可以直接用那个字段当我们的分区但是要打开一个开关</p><p><code>set hive.exec.dynamic.partition.mode=nonstrict;</code></p><p>静态分区：就是自己制定好分区的标题的</p><p>离线任务 ： 业务周期性 T+1</p><p>就是延迟一天处理 </p><p>默认底层创建的是内部表</p><p>内部表 ： 受hive管控的 ： 如果有删表的操作，那么会清理干净，所有数据都会被删除</p><p>外部表 ： 如果被删除的情况下，只是hdfs上指向metastore的索引被删除了，源数据不会被删除 ，而且我们还可通过建表的方式让他们的索引再次关联上</p><p>创建外部表 ：</p><p>相互转换 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">外部转内部</span><br><span class="line">ALTER TABLE 外部表名字 SET TBLPROPERTIES (&quot;EXTERNAL&quot; = &quot;true&quot;);</span><br><span class="line"></span><br><span class="line">内部转外部</span><br><span class="line">ALTER TABLE 外部表名字 SET TBLPROPERTIES (&quot;EXTERNAL&quot; = &quot;false&quot;);</span><br><span class="line">但是这上述的EXTERNAL 是不能小写的会造成失效的问题</span><br></pre></td></tr></table></figure><h1 id="复杂的数据类型"><a href="#复杂的数据类型" class="headerlink" title="复杂的数据类型"></a>复杂的数据类型</h1><p>中小企业用的不多，，大企业用的多</p><p>会建表 ，会查询</p><p>maps: <code>MAP&lt;primitive_type, data_type&gt; </code></p><p>数据如下 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1,zhangsan,father:xiaoming#mother:xiaohuang#brother:xiaoxu,28</span><br><span class="line">2,lisi,father:mayun#mother:huangyi#brother:guanyu,22</span><br><span class="line">3,wangwu,father:wangjianlin#mother:ruhua#sister:jingtian,29</span><br><span class="line">4,mayun,father:mayongzhen#mother:angelababy,26</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">create table hive_map(</span><br><span class="line">id int  comment &#x27;用户id&#x27;,</span><br><span class="line">name string comment &#x27;用户名字&#x27;,</span><br><span class="line">relation map&lt;string,string&gt; comment &#x27;家庭成员&#x27;,</span><br><span class="line">age int comment &#x27;年龄&#x27;</span><br><span class="line">)</span><br><span class="line">row format  delimited fields terminated by &#x27;,&#x27;</span><br><span class="line">collection items terminated by &#x27;#&#x27;</span><br><span class="line">map keys terminated by &#x27;:&#x27;;</span><br></pre></td></tr></table></figure><p>arrays:  <code>ARRAY&lt;data_type&gt;</code></p><p>数据</p><p><code>zihan   beijing,shanghai,chengdu,dalian</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table hive_array(</span><br><span class="line">name String,</span><br><span class="line">locations array&lt;String&gt;</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">collection items terminated by &#x27;,&#x27;;</span><br></pre></td></tr></table></figure><p>structs:<code>STRUCT&lt;col_name : data_type [COMMENT col_comment], ...&gt;</code></p><p>数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">192.168.1.1#zhangsan:40</span><br><span class="line">192.168.1.2#lisi:50</span><br><span class="line">192.168.1.3#wangwu:60</span><br><span class="line">192.168.1.4#zhaoliu:70</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table hive_struct(</span><br><span class="line">ip string,</span><br><span class="line">userinfo STRUCT&lt;name:string,age:int&gt;</span><br><span class="line">)</span><br><span class="line">row format  delimited fields terminated by &#x27;#&#x27;</span><br><span class="line">collection items terminated by &#x27;:&#x27;;</span><br></pre></td></tr></table></figure><h1 id="数据形式的不同使用方法"><a href="#数据形式的不同使用方法" class="headerlink" title="数据形式的不同使用方法"></a>数据形式的不同使用方法</h1><h2 id="array"><a href="#array" class="headerlink" title="array"></a>array</h2><p>案例分析：</p><p>1.查询每个用户第一个工作地点？</p><p>select  name ,locations[0] as first_loc_work from  hive_array;</p><p>2.查询每个人 工作地点的数量</p><p>select  name , size(locations) from  hive_array ;</p><p>3.查询在shanghai 工作的有哪些人</p><p>select  * from hive_array  where array_contains(locations,’shanghai’);</p><h3 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h3><p>思路是先把一个array的元素炸开，然后通过显示出来</p><p>显示手段 ： LATERAL VIEW（侧写视图）</p><ul><li>LATERAL VIEW udtf(expression) tableAlias AS columnAlias</li><li>udtf : 一进多出</li><li>FROM baseTable (lateralView)*</li><li>最终代码 ：</li><li><pre><code>select name,locationfrom hive_array lateral view explode(locations) loc_table as location;</code></pre></li></ul><h2 id="map"><a href="#map" class="headerlink" title="map"></a>map</h2><p>需求： 1.查询表中每个人的father的名字</p><p>select id,name,age,relation[‘father’] as father from hive_map;</p><p>2.查询表中 每个人的家庭成员   keys</p><p>select id,name,age,map_keys(relation) as members from hive_map;</p><p>3.查询表中 每个人的家庭成员的名字 values</p><p>select id,name,age,map_values(relation) as members from hive_map;</p><p>4.查询表中 有brother的人以及brother的名字</p><p>select<br> id,name,age,relation[‘brother’] as brother<br>from hive_map<br>where<br>relation[‘brother’] is not null;</p><p>或者可以</p><p>select<br> id,name,age,relation[‘brother’] as brother<br>from hive_map<br>where<br>array_contains(map_keys(relation), ‘brother’);</p><p>&#x2F;&#x2F; map_key()函数的意思是可以把这个列的map的key当作array取出来</p><h2 id="structs"><a href="#structs" class="headerlink" title="structs"></a>structs</h2><p>select ip,userinfo.name as name ,userinfo.age as age from hive_struct;</p><h2 id="开窗函数-："><a href="#开窗函数-：" class="headerlink" title="开窗函数 ："></a>开窗函数 ：</h2><ul><li>分析函数：对开窗函数的分析的函数<ul><li>rank : 使用方法 rank()over(partition by xx order by yy) as rk  : 如果有重复的数据，会丢失排名</li><li>dense_rank :使用方法同上 ： 如果有重复数据 ，则不会丢失排名 ：</li><li>row_number:同上 ： 排名相同且不会重复 ， 就是会顺序往下 ：</li></ul></li></ul><p>上述的常用手段 ： 求topn的排名</p><p>比如要求top3 的</p><p>作业 ： </p><p>统计每个店铺的uv</p><p>统计top3的用户记录</p><p>pv ： 页面的浏览量</p><p>uv ： 访客的次数</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/12/01/12-01/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hive第二天</title>
      <link>http://example.com/2022/11/30/11-30/</link>
      <guid>http://example.com/2022/11/30/11-30/</guid>
      <pubDate>Wed, 30 Nov 2022 01:57:50 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;关于hive里的数据类型&quot;&gt;&lt;a href=&quot;#关于hive里的数据类型&quot; class=&quot;headerlink&quot; title=&quot;关于hive里的数据类型&quot;&gt;&lt;/a&gt;关于hive里的数据类型&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;整数&lt;ul&gt;
&lt;li&gt;int&lt;/li&gt;
&lt;li&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="关于hive里的数据类型"><a href="#关于hive里的数据类型" class="headerlink" title="关于hive里的数据类型"></a>关于hive里的数据类型</h1><ul><li>整数<ul><li>int</li><li>bigint &#x3D;&#x3D;long</li></ul></li><li>小数 ：<ul><li>float</li><li>double</li><li>Decimal</li></ul></li><li>字符串：<ul><li>String (建议统一用String)</li><li>varchar</li><li>char</li></ul></li><li>时间：<ul><li>时间日期 DATE 格式：YYYY-MM-DD</li><li>时间戳：TIMESTAMP YYYY-MM-DD HH:MM:SS</li></ul></li></ul><h1 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h1><p>CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name<br>  [(col_name data_type [column_constraint_specification] [COMMENT col_comment], … [constraint_specification])]<br>  [COMMENT table_comment]<br>  [PARTITIONED BY (col_name data_type [COMMENT col_comment], …)]<br>  [CLUSTERED BY (col_name, col_name, …) [SORTED BY (col_name [ASC|DESC], …)] INTO num_buckets BUCKETS]<br>  [<br>   [ROW FORMAT row_format]<br>   [STORED AS file_format]<br>     | STORED BY ‘storage.handler.class.name’ [WITH SERDEPROPERTIES (…)]  – (Note: Available in Hive 0.6.0 and later)<br>  ]</p><p>数据字段名字 字段类型</p><p>例如</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">create table mytest(</span><br><span class="line">id String comment &#x27;用户id&#x27;,</span><br><span class="line">name string,</span><br><span class="line">age bigint</span><br><span class="line">) comment &#x27;第一个表&#x27;</span><br><span class="line">ROW FORMAT delimited fields terminated by &#x27;,&#x27; //指定分隔符</span><br><span class="line">STORED as TEXTFILE; // 存储形式</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"></span><br><span class="line">create table emp2 like emp;</span><br><span class="line">复制表结构</span><br><span class="line"></span><br><span class="line">或者</span><br></pre></td></tr></table></figure><h2 id="为什么要分隔符"><a href="#为什么要分隔符" class="headerlink" title="为什么要分隔符"></a>为什么要分隔符</h2><p>因为我们的元数据都在hdfs上，对于hdfs上的数据可以通过分隔符进行自动导入到hive里，比如上述是，分割的，然后我hdfs上有如下数据</p><p>1，zihan,11</p><p>2,zhangsan,23</p><p>3,liu,33</p><p>就会自动按照每一行进行insert</p><p>导入数据 ： load data local inpath ‘本地的绝对路径’ into table 表名</p><p>清空表的操作 ： truncate table 表名</p><h1 id="删除库"><a href="#删除库" class="headerlink" title="删除库"></a>删除库</h1><p>DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];</p><p>DROP DATABASE bigdata_hive4;</p><p>DROP DATABASE bigdata_hive2 CASCADE; &#x3D;&gt;删库跑路的操作</p><p>CASCADE : 代表联合删除 ，一般删除的时候如果里面有表，会造成无法删除的问题，但是联合删除会直接删除掉</p><h1 id="DMl"><a href="#DMl" class="headerlink" title="DMl"></a>DMl</h1><h2 id="load-："><a href="#load-：" class="headerlink" title="load ："></a>load ：</h2><ul><li>加载本地数据</li><li>加载hdfs上的数据</li></ul><p>LOAD原本是追加，不是覆盖 ， 但是可以通过 加上 overwrite 关键字 进行 覆盖操作</p><h2 id="覆盖例子"><a href="#覆盖例子" class="headerlink" title="覆盖例子"></a>覆盖例子</h2><p>load data local inpath ‘&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;emp.txt’ OVERWRITE INTO TABLE emp;</p><h2 id="上传"><a href="#上传" class="headerlink" title="上传"></a>上传</h2><p>本地：load data inpath ‘本地路径’ into table 表名</p><p>hdfs ： load data inpath ‘hdfs上的路径’ into table 表名</p><p>上述的hdfs上的相当于把其路径里的文件移动到table 表名的下面 并且改名，且关联到metastore</p><p>但是我们的hdfs mv 是不会关联到metastore的</p><p>在hive 里 update 和 delete 不要做 &#x3D;》 因为效率低下</p><p>把所有的update和delete都转化成insert和overwrite</p><h2 id="插入语句-："><a href="#插入语句-：" class="headerlink" title="插入语句 ："></a>插入语句 ：</h2><p>Inserting data into Hive Tables from queries</p><p>insert into|OVERWRITE table tablename selectQury</p><p>2.Inserting values into tables from SQL 【不推荐使用】<br>INSERT INTO TABLE tablename<br>VALUES values_row [, values_row …]<br>1.每导入一条数据 就会触发一次 mapreduce job  效率太低</p><p>emp2：<br>    insert into table emp2<br>    select *  from emp;<br>insert overwrite table emp2<br>select *  from emp where deptno&#x3D;10;</p><h2 id="关于hive里的一些函数以及使用"><a href="#关于hive里的一些函数以及使用" class="headerlink" title="关于hive里的一些函数以及使用"></a>关于hive里的一些函数以及使用</h2><h3 id="1-where-过滤条件"><a href="#1-where-过滤条件" class="headerlink" title="1.where 过滤条件"></a>1.where 过滤条件</h3><pre><code> where_condition &lt; &gt; = &lt;&gt;  != and or in not in between  and is is not</code></pre><h3 id="需求：查询表中-deptno-20-10"><a href="#需求：查询表中-deptno-20-10" class="headerlink" title="需求：查询表中 deptno 20 10"></a>需求：查询表中 deptno 20 10</h3><p>select<br>*<br>from emp<br>where deptno&#x3D;20 or deptno &#x3D;10;</p><p>select<br>*<br>from emp<br>where deptno in (10,20);</p><p>select<br>*<br>from emp<br>where deptno &lt;&gt; 20;<br>select<br>*<br>from emp<br>where deptno !&#x3D; 20;</p><h3 id="2-order-by-排序语法"><a href="#2-order-by-排序语法" class="headerlink" title="2.order by  排序语法"></a>2.order by  排序语法</h3><pre><code>1.默认asc 升序2.降序 desc</code></pre><p>select<br>sal<br>from emp<br>order by sal desc;</p><h3 id="3-like-语法-模糊匹配"><a href="#3-like-语法-模糊匹配" class="headerlink" title="3.like 语法 模糊匹配"></a>3.like 语法 模糊匹配</h3><pre><code>1._  占位符2.%  模糊rlike regexp</code></pre><h3 id="4-合并表"><a href="#4-合并表" class="headerlink" title="4.合并表"></a>4.合并表</h3><pre><code>1.union  去重2.union all  不去重</code></pre><h3 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h3><p>create table a(id int ,name string) row format  delimited fields terminated by ‘,’ ;<br>create table b(id int ,name string) row format  delimited fields terminated by ‘,’ ;</p><p>load data local inpath “&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;a.txt” into table a;<br>load data local inpath “&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;b.txt” into table b;</p><p>select name from a<br>union all<br>select name from b;</p><p>select name from a<br>union all<br>select name from b<br>union all<br>select “lisi” as name ;</p><p>select name,”1” as pk from a<br>union all<br>select name,”2” as pk from b<br>union all<br>select “lisi” as name,”3” as id ;</p><p>思考： hive建表 默认column 分割符是什么？</p><h3 id="5-null-处理"><a href="#5-null-处理" class="headerlink" title="5.null 处理"></a>5.null 处理</h3><pre><code>1. 过滤    where xxx is not nullis null 作用一样 &lt;=&gt;2. etl 转换    ifnull  =&gt; hive里没有    coalesce =》    nvl  =》</code></pre><h3 id="补充："><a href="#补充：" class="headerlink" title="补充："></a>补充：</h3><pre><code>查看hive支持的function ：            y=f(x)    SHOW FUNCTIONS [LIKE &quot;`&lt;pattern&gt;`&quot;];    show functions like nvl;  =&gt; 判断 function hive 是否存在    desc function nvl; =》  查看某个函数具体使用</code></pre><p>select<br>empno,<br>ename,<br>job,<br>mgr,<br>hiredate,<br>sal,<br>nvl(comm,0) as comm_alias,<br>deptno<br>from emp ;</p><h2 id="分组-聚合函数-join"><a href="#分组-聚合函数-join" class="headerlink" title="分组 聚合函数 join"></a>分组 聚合函数 join</h2><p>聚合函数 ： </p><ul><li>sum</li><li>max</li><li>min</li><li>avg</li><li>count</li></ul><h3 id="group-by"><a href="#group-by" class="headerlink" title="group by"></a>group by</h3><ul><li>和聚合函数一起使用</li><li>一个或者多个colum进行分组</li><li>字段必须select出现 和 group by 出现要一致</li></ul><h3 id="having-："><a href="#having-：" class="headerlink" title="having ："></a>having ：</h3><ul><li>在group by 后面使用</li></ul><p>select job,</p><p>sum(sal) as sal_num,</p><p>max(sal),</p><p>min(sal),</p><p>avg(sal),</p><p>count(1) as cnt</p><p>from emp</p><p>group by job</p><p>having sal_num &gt; 6000</p><h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p>找准关联字段</p><ul><li>inner join [join]</li><li>left join</li><li>right join</li><li>full join</li></ul><h3 id="需求：既要显示聚合前的数据，又要显示聚合后的数据？"><a href="#需求：既要显示聚合前的数据，又要显示聚合后的数据？" class="headerlink" title="需求：既要显示聚合前的数据，又要显示聚合后的数据？"></a>需求：既要显示聚合前的数据，又要显示聚合后的数据？</h3><p>函数  over([partition by xxx,…] [order by xxx,….])</p><p>over: 以谁进行开窗 table、<br>parition by : 以谁进行分组   table columns<br>order by : 以谁进行排序  table columns</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">数据： </span><br><span class="line">haige,2022-11-10,1</span><br><span class="line">haige,2022-11-11,5</span><br><span class="line">haige,2022-11-12,7</span><br><span class="line">haige,2022-11-13,3</span><br><span class="line">haige,2022-11-14,2</span><br><span class="line">haige,2022-11-15,4</span><br><span class="line">haige,2022-11-16,4</span><br></pre></td></tr></table></figure><p>需求：<br>    统计累计问题 ，每个用户每天累计点外卖次数</p><p>[partition by xxx,…] [order by xxx,….]</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">name ,</span><br><span class="line">dt ,</span><br><span class="line">cnt ,</span><br><span class="line">sum(cnt) over(partition by name  order by dt ) as sum_cnt</span><br><span class="line">from user_mt;</span><br></pre></td></tr></table></figure><h3 id="命令行更改"><a href="#命令行更改" class="headerlink" title="命令行更改"></a>命令行更改</h3><p>command line<br>    1.hive shell<br>    2.jdbc &#x3D;&gt; hiveServer2</p><pre><code>hive clinet:    1. hive shell    2. beeline shell jdbc   开启 hiveServer2 服务 thift</code></pre><p>在beeline中 <code>!connect jdbc:hive2://localhost:10000 hadoop</code></p><p>补充：<br>beeline &#x3D;&gt; 连接 hive  &#x3D;》 hdfs<br>对hdfs 做一个设置 代理设置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">core-site.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/11/30/11-30/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>hive</title>
      <link>http://example.com/2022/11/29/11-29/</link>
      <guid>http://example.com/2022/11/29/11-29/</guid>
      <pubDate>Tue, 29 Nov 2022 00:36:32 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;简单来说就是用sql处理hadoop的数据的&lt;/p&gt;
&lt;p&gt;除了hive之外 ： sparksql ，presto ， impala&lt;/p&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>简单来说就是用sql处理hadoop的数据的</p><p>除了hive之外 ： sparksql ，presto ， impala</p><h2 id="要求掌握"><a href="#要求掌握" class="headerlink" title="要求掌握"></a>要求掌握</h2><p>sql</p><p>udf</p><h2 id="hadoop包含"><a href="#hadoop包含" class="headerlink" title="hadoop包含"></a>hadoop包含</h2><p>hdfs ：命令行</p><p>mapreduce ：目前工作中几乎不用，但是关于核心类和思想要掌握</p><p>yarn：提交作业 xxx （mr&#x2F;spark&#x2F;flink）on yarn ： 必须会</p><p>mapreduce的弊端 ：</p><ul><li>开发大量代码</li><li>编程基础不错</li><li>部署麻烦</li><li>修改code蛮麻烦</li><li>对于DBA和RDBMS的小伙伴是不友好的</li></ul><p>大数据处理来说最终落地最好是sql</p><p>大数据开发角度 ：</p><ul><li>基础平台开发<ul><li>涉及很多框架底层的面很广</li></ul></li><li>应用层面开发<ul><li>基于基础平台开发 ，写sql</li></ul></li></ul><p>根据你的兴趣点 + 公司的定位</p><p>必然有新的东西诞生去解决一个场景的问题</p><p>mr主要适用于我们的p计算（离线计算）：不要求时效性 ，但是mr开发太麻烦，就诞生了hive</p><p>hive 介绍 ：</p><p>hive.apache.org</p><p>是由什么人提供出来的？</p><p>facebook 开源 去解决结构化的数据统计问题</p><p>是什么？</p><p>构建在hadoop之上的数据仓库</p><p>hdfs：hive 的数据是在hdfs之上的</p><p>yarn：可以跑在yarn之上</p><p>mmapreduce ： 可以用mr形式去运行</p><p>如何使用：</p><p>定义了一中类sql的语言，类似sql但是又有不同</p><p>适用于离线&#x2F;p处理的</p><p>开发就是写sql &#x3D;》 mr  &#x3D;》 运行在yarn上</p><p>hive的底层引擎是：</p><ul><li>mr （默认）：sql&#x3D;》mr</li><li>Tez：sql&#x3D;》Tez</li><li>Spark：sql&#x3D;》Spark</li></ul><p>hive on spark &#x3D;》生厂上用的用的不多</p><p>spark on hive &#x3D;》 用sparksql查看hive的数据</p><p>hive的存储格式，压缩格式等</p><p>官网：</p><ul><li>in distributed storage （分布式存储）：hdfs , cos,oss,aws</li><li>A command line tool and JDBC driver are provided to connect users to Hive.</li></ul><p>版本介绍 ：</p><p>x.y.z：x是大版本，y是小版本，z是小版本的修复版本</p><p>为什么要学习hive</p><ul><li>简单易用 ： 可以用sql开发</li><li>扩展性好：<ul><li>用自定义函数udf</li><li>数据存储 和 计算角度 ： 如果表中数据存不下，可以加几个节点就好了<ul><li>注意hive不是分布式的，它仅仅是个客户端</li></ul></li><li>Metastore ：hive的元数据管理<ul><li>sparksql ,presto ,impala 只要可以访问hive的元数据就可以访问里面表的数据</li><li>可以分享元数据</li></ul></li></ul></li></ul><p>hive的架构 ：</p><ul><li>元数据 ： 描述数据的数据<ul><li>表的名字，字段的名字，字段的类型，什么人创建的，数据存储在哪里等</li><li>元数据的内部内置了一个Derby但是有弊端</li><li>元数据都是用mysql进行存储的</li><li>测试的时候一个mysql就可以了</li><li>但是生产上则一个不够</li><li>生产上要遵循 HA ：高可用</li><li>就是要一台做备份</li><li>两个mysql是主从架构</li></ul></li></ul><p>hive和RDBMS的区别</p><ul><li>共同点 sql</li><li>延时性 ： hive 适用于离线计算 慢，千万不要拿hive和mysql的性能做对比，无可比性，mysql要必hive高（数据量小）数据量大的时候就反过来了</li><li>事务 ： 都支持</li><li>update ，delete<ul><li>上面两个语句在hive里基本不用，因为性能太差了</li></ul></li><li>都支持分布式</li><li>部署成本 ：<ul><li>hive ：廉价</li><li>mysql ：成本很高</li></ul></li><li>数据体量<ul><li>hive ：Tb</li><li>mysql：处理Pb都比较费劲</li></ul></li></ul><p>对hive进行部署</p><p>分布式部署 ： cwiki.apache.org(官网)</p><p>首先把用压缩包上传到linux机器上</p><p>然后对linux进行解压</p><p>解压完成之后，我们要对他的的配置进行修改在解压之后的</p><p><code>vim hive-site.xml</code></p><p>编辑完成之后我们把以下内容放进去</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;jdbc:mysql://bigdata2:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;liuzihan010616&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>然后在环境变量中编辑我们的hive_home</p><p><code>vim ~/.bashrc </code></p><p>把下列加进去</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HIVE_HOME</span></span><br><span class="line">export HIVE_HOME=/home/hadoop/app/hive</span><br><span class="line">export PATH=$&#123;PATH&#125;:$&#123;HIVE_HOME&#125;/bin</span><br></pre></td></tr></table></figure><p>然后我们要把我们的mysql链接包放在lib文件夹下</p><p>就是 <code>mysql-connector-java-5.1.28.jar</code></p><p>要放在hive的lib文件夹下</p><p>然后对我们的hive进行初始化</p><p><code>schematool -dbType mysql -initSchema</code></p><p>初始化成功之后，先启动hdfs ，然后命令行输入 hive 然后输入show databases;</p><p>成功就ok了</p><p>然后再mysql的数据库里会有hive这个数据库</p><p>mysql里\G是格式化的意思</p><h2 id="思考-：-表中的字段-存在哪里？"><a href="#思考-：-表中的字段-存在哪里？" class="headerlink" title="思考 ： 表中的字段 存在哪里？"></a>思考 ： 表中的字段 存在哪里？</h2><p>我们先进入hive ： <code>create table test(name String); </code></p><p>一个 hive 表 会被拆分成n个表存储再mysql里</p><p>比如　： TBl表存放的是我们的表名 <code>select * from tbls \G;</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">*************************** 1. row ***************************</span><br><span class="line">            TBL_ID: 1</span><br><span class="line">       CREATE_TIME: 1669707861</span><br><span class="line">             DB_ID: 1</span><br><span class="line">  LAST_ACCESS_TIME: 0</span><br><span class="line">             OWNER: hadoop</span><br><span class="line">        OWNER_TYPE: USER</span><br><span class="line">         RETENTION: 0</span><br><span class="line">             SD_ID: 1</span><br><span class="line">          TBL_NAME: test</span><br><span class="line">          TBL_TYPE: MANAGED_TABLE</span><br><span class="line">VIEW_EXPANDED_TEXT: NULL</span><br><span class="line">VIEW_ORIGINAL_TEXT: NULL</span><br><span class="line">IS_REWRITE_ENABLED:  </span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>比如 ： columns_v2表存放的是我们的字段 <code>select * from columns_v2 \G;</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">*************************** 1. row ***************************</span><br><span class="line">      CD_ID: 1</span><br><span class="line">    COMMENT: NULL</span><br><span class="line">COLUMN_NAME: name</span><br><span class="line">  TYPE_NAME: string</span><br><span class="line">INTEGER_IDX: 0</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>比如 ： DBS是存放的我们的物理存储路径的 <code>select * from DBS \G;</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">*************************** 1. row ***************************</span><br><span class="line">          DB_ID: 1</span><br><span class="line">           DESC: Default Hive database</span><br><span class="line">DB_LOCATION_URI: hdfs://bigdata3:9000/user/hive/warehouse</span><br><span class="line">           NAME: default</span><br><span class="line">     OWNER_NAME: public</span><br><span class="line">     OWNER_TYPE: ROLE</span><br><span class="line">      CTLG_NAME: hive</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><h2 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a>DDL</h2><p>hive有个默认数据库 是default 路径 ：&#x2F;user&#x2F;hive&#x2F;warehouse</p><p>非默认数据库 ： &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;dbname.db</p><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><p>CREATE [REMOTE] (DATABASE|SCHEMA) [IF NOT EXISTS] database_name<br>  [COMMENT database_comment]<br>  [LOCATION hdfs_path]<br>  [MANAGEDLOCATION hdfs_path]<br>  [WITH DBPROPERTIES (property_name&#x3D;property_value, …)];</p><p>[] 可有可无<br>(|) 选择其中一个即可</p><p>CREATE DATABASE 名称；</p><p>CREATE DATABASE 名称 LOCATION ‘创建的地方’；</p><p>例子 ：</p><p>create database if not exists bigdata_hive;<br>create database  bigdata_hive2  LOCATION ‘&#x2F;data&#x2F;bigdata_hive2’;<br>create database  bigdata_hive3 WITH DBPROPERTIES (‘creator’&#x3D;’doublehappy’, ‘create_dt’&#x3D;”2099-11-29”);<br>create database if not exists bigdata_hive4 COMMENT “这是一个数据库4”;</p><p>解决此处中文乱码的问题</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE hive.dbs MODIFY COLUMN `DESC` varchar(4000) CHARACTER SET utf8 COLLATE utf8_general_ci NULL;</span><br></pre></td></tr></table></figure><h3 id="显示数据库"><a href="#显示数据库" class="headerlink" title="显示数据库"></a>显示数据库</h3><p>show databases;<br>show databases like “bigdata_hive*”<br>desc database  bigdata_hive3;<br>desc database EXTENDED bigdata_hive3;</p><h2 id="思考-：-这个数据库在hdfs的哪一个地方"><a href="#思考-：-这个数据库在hdfs的哪一个地方" class="headerlink" title="思考 ： 这个数据库在hdfs的哪一个地方"></a>思考 ： 这个数据库在hdfs的哪一个地方</h2><p>可以通过查看DBS表</p><h1 id="hive-的注释-comment-中文乱码的解决方法"><a href="#hive-的注释-comment-中文乱码的解决方法" class="headerlink" title="hive 的注释(comment) 中文乱码的解决方法"></a>hive 的注释(comment) 中文乱码的解决方法</h1><p>（1）修改表字段注解和表注解</p><p>alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;<br>alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</p><p>（2）修改分区字段注解</p><p>alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ;<br>alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;</p><p>（3）修改索引注解</p><p>alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</p><p>修改hive-site.xml配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://IP:3306/db_name?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>上述的 <code>&amp; 是 &amp;amp; </code></p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/11/29/11-29/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>yarn</title>
      <link>http://example.com/2022/11/28/11-28/</link>
      <guid>http://example.com/2022/11/28/11-28/</guid>
      <pubDate>Mon, 28 Nov 2022 00:34:31 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;yarn&quot;&gt;&lt;a href=&quot;#yarn&quot; class=&quot;headerlink&quot; title=&quot;yarn&quot;&gt;&lt;/a&gt;yarn&lt;/h1&gt;&lt;h2 id=&quot;架构&quot;&gt;&lt;a href=&quot;#架构&quot; class=&quot;headerlink&quot; title=&quot;架构&quot;&gt;&lt;/a&gt;架构&lt;/h</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h1><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>主从架构 ： resourcemanager(资源的分配) : nodemanager(资源的供给与隔离)</p><h3 id="资源调度"><a href="#资源调度" class="headerlink" title="资源调度"></a>资源调度</h3><p>通过rm把nm的资源分配给我们的 task上</p><h3 id="资源隔离"><a href="#资源隔离" class="headerlink" title="资源隔离"></a>资源隔离</h3><p>nm按照要求给task提供资源，保证提供的资源具有独占性</p><h3 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h3><p>nm指挥分配的资源</p><p>一个task对应一个container ： cpu&#x2F;mem（cpu和内存）</p><p>每个container之间是相互隔离的</p><h2 id="yarn的架构设计"><a href="#yarn的架构设计" class="headerlink" title="yarn的架构设计"></a>yarn的架构设计</h2><p>作业提交的流程：<br>client &#x3D;&gt; 给rm 的apps 发送请求 去运行 jar （app master）</p><p>apps 分配一个container 去运行 app master</p><p>app master 会向apps manager 去注册我的作业</p><p>app master 向resource scheduler 申请资源去运行 我的代码</p><p>nodemanager 会开启资源 container 去运行map task 以及reduce task</p><p>tsak 会向 app master 汇报代码与运行情况 </p><p>当代码运行完成 app master 会给apps 发送请求 ，通知我的作业完成了</p><p>apps manager 收到请求之后会通知你的客户端 ，告诉已经运行完成</p><p>输入阶段 ： map tsak 的个数 &#x3D;》container 的申请个数 redurce task 同理</p><h2 id="面试会问"><a href="#面试会问" class="headerlink" title="面试会问"></a>面试会问</h2><p>yarn的架构设计 &#x3D;&#x3D;mr作业提交流程</p><h2 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h2><p>FIFO ：</p><ul><li>先进先出</li><li>单队列</li></ul><p>Capacity ：容量调度器</p><ul><li>多队列</li><li>先进先出（针对一个队列）</li><li>每个队列之间互不影响</li><li>每个队列之间是事先定好的</li></ul><p>Fair：公平调度器</p><ul><li>多队列</li><li>每个队列之间的每个job是有影响的 不是先进先出</li><li>哪一个job的优先级高就执行哪一个</li><li>如果相同优先级，则是顺序</li></ul><p>主流的中小企业：Capacity</p><p>大公司会用：fair</p><h3 id="默认调度器"><a href="#默认调度器" class="headerlink" title="默认调度器"></a>默认调度器</h3><p>3.x：默认是容量调度器</p><p>通过调度器进行其作业调度</p><p>2.x: 版本是fair（默认）</p><h2 id="yarn的web界面的简介"><a href="#yarn的web界面的简介" class="headerlink" title="yarn的web界面的简介"></a>yarn的web界面的简介</h2><p>左侧侧边栏 ： 有几个选项卡 ： 分别是 </p><h2 id="yarn资源的调优"><a href="#yarn资源的调优" class="headerlink" title="yarn资源的调优"></a>yarn资源的调优</h2><p>container? </p><p>一定比例的cpu和mem</p><p>刀片服务器的配置 ： 128G 16 core :假设一个机器的配置</p><p>刀片服务器 装完系统 消耗内存 1G </p><p>系统预留 ： 预留 20%左右 包含装完成系统 消耗的1G</p><p>原因 ：给未来部署组件预留空间，防止全部使用 ： 会导致系统夯住 就是卡住 ，oom机制【linux系统】：系统会自己杀死进程当内存不足的时候</p><p>预留空间 ： 128 * 0.2 &#x3D; 26G</p><p>其余空间用于大数据 102G</p><p>hadoop ： </p><ul><li>datanode  进程内存 ： 默认 1G &#x3D;》 生产上 2G</li><li>nodemanager 进程内存 ： ，默认 1G &#x3D;》 生产上 4G</li></ul><p>接下来还有96G全部给我们的yarn资源 ： 96G</p><p>container的资源分配 ： </p><p>内存</p><p>cpu</p><p>相比：cpu更重要一些</p><p>container的内存划分 ：默认是86G</p><p>其最小是 1G（默认）</p><p>最大是 8G（默认） 但是可以设置</p><p>注意 container的内存会自动增加 默认以1G递增</p><p>container cpu ： 是虚拟核 &#x3D;》 考虑初衷是不同节点的cpu性能不同</p><p>比如 ： 一个cpu是另外一个cpu的2倍</p><p>第一机器 ： pcore ：vcore &#x3D; 1：2 相当于1个物理核当成两个虚拟核用</p><p>给container的核数 ： 默认是8core</p><p>总数 ： </p><p>最小:1c  (默认)</p><p>最大:4c（默认）</p><p>实际开发角度 ：</p><p>mem：最大不要超过32G ，如果超过32G则会导致压缩指针失效</p><p>cpu ： cloudera的公司推荐一个container的core最好不要超过5</p><p>配置core  ：在yarn-site.xml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">cpu： </span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.pcores-vcores-multiplier&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="mapreduce"><a href="#mapreduce" class="headerlink" title="mapreduce"></a>mapreduce</h2><p>map task :mem  : 默认是1024m ，一个map task 申请的资源是1024m ， 但是如果实际使用的资源的内存量超过这个值，你的task会强制被杀死 ，reduce task 也一样</p><p>map task :vcore ： 默认是1</p><p>reduce task :mem : 默认是1024m</p><p>reduce task :vcore : 默认是1</p><p>mr作业是进程级别 &#x3D;》 jvm</p><p>map task</p><p>reduce task </p><p>jvm参数调优 ： </p><p>存储 hdfs</p><p>存储文件</p><p>压缩</p><ul><li>为什么使用压缩？</li><li>节省空间</li><li>节省时间<ul><li>网络io核磁盘io会减少</li><li>指的是mapreduce数据计算过程中</li><li>经过网络传输的数据会变少，</li><li>同样到磁盘上的时候，数据量少也会减少磁盘的io</li></ul></li><li>使用场景<ul><li>采用压缩 ， 对机器的cpu要求比较高</li><li>存储数据的空间不够了，才会用压缩</li></ul></li><li>两面性</li><li>采用压缩的确会让空间和时间减少</li><li>cpu消耗 cpu利用率高 &#x3D;》会导致处理的时间变长一点</li><li>如何使用压缩？</li></ul><p>常见的压缩格式</p><ul><li>gzip</li><li>bzip2</li><li>Lzo</li><li>Snappy</li><li>LZ4</li></ul><p>常见的压缩注意点：</p><p>压缩比 Bzip2 30%  GZIP    snappy、lzo 50%  ： 这个30%和50%代表能把源文件压缩到源文件的%多少</p><p>解压速度 ： snappy、lzo GZIP   Bzip2</p><p>压缩文件可不可以一被切分 </p><p>假设一个 5G文件 不能被切分 split 意味着 只能使用一个map task去处理</p><p>map task  5G</p><p>假设一个 5G文件 能被切片  splits 10map task 去并行处理</p><p>5*1024 &#x2F;10 &#x3D; 一个map task 处理的数据</p><p>能否被切分 决定了 你的 一个map task处理的数据量有多少</p><p>压缩后的文件是否支持分割？<br>            gzip  不可分割<br>            bzip2  可分割<br>            lzo   带索引的可以分割 (默认是不支持分割的)<br>            snappy 不可分割的</p><p>mapreduce 每个阶段该如何采用这些算法？</p><p>input &#x3D;》 maps &#x3D;》 reduce &#x3D;》 output</p><p>input：<br>    1.Bzip2 ：支持分割 多个map task 进行出</p><p>map out：</p><ul><li>snappy 、lzo</li><li>shuffle 过程 要选择一个解压 速度快的压缩格式</li></ul><p>reduce out ：</p><ul><li>1.高的压缩比 + 支持分片  &#x3D;》 节省空间</li><li>2.bzip2  、lzo带索引的</li></ul><p>reduce out 数据 作为下一个<br>map 的输入咋办？<br>建议使用bzip2【如果采用压缩的话】</p><p>进行压缩配置的两种方式 ： </p><p>job &#x3D;》 code 针对少数job生产生效</p><p>集群所有的job 在配置文件里配置</p><p>对mapreduce 进行配置 </p><ul><li>配置压缩的codec</li><li>map reduce 输出配置<ul><li>先打开压缩的开关</li><li>配置codec即可</li></ul></li><li>hadoop是不是支持哪些压缩 ，通过命令 ：或者配置文件</li><li>core-site.xm<ul><li>配置支持的压缩有什么</li><li>原生的hadoop默认不支持lzo的算法，因为lzo要把整个hadooop重新编译一遍才可以重新实行</li><li>&#96;&#96;&#96;<property>        <name>io.compression.codecs</name>        <value>org.apache.hadoop.io.compress.BZip2Codec,        org.apache.hadoop.io.compress.SnappyCodec,        org.apache.hadoop.io.compress.GzipCodec,        org.apache.hadoop.io.compress.DefaultCodec        </value></property><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mapred-site.xml:</span><br><span class="line"></span><br><span class="line">* 1.先打开压缩的开关</span><br><span class="line">* 2.map reduce 输出 压缩算法</span><br><span class="line"></span><br><span class="line">reduce： 开关</span><br><span class="line"></span><br><span class="line">* mapreduce.output.fileoutputformat.compress</span><br><span class="line">* ```</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">          &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;</span><br><span class="line">          &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure></li></ul></li><li>&#96;&#96;&#96;<property>        <name>mapreduce.output.fileoutputformat.compress.codec</name>        <value>org.apache.hadoop.io.compress.BZip2Codec</value></property><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">map阶段的：</span><br><span class="line"></span><br></pre></td></tr></table></figure>// 设置在map输出阶段压缩      conf.set("mapreduce.map.output.compress", "true");</li></ul><p>&#x2F;&#x2F; 设置解压缩编码器<br>        conf.set(“mapreduce.map.output.compress.codec”, “org.apache.hadoop.io.compress.DefaultCodec”);</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">各个datanode数据节点的平衡</span><br><span class="line"></span><br><span class="line">* DN1 存储空间 90%</span><br><span class="line">* DN2 存储空间 60%</span><br><span class="line">* DN3 存储空间 80%</span><br><span class="line"></span><br><span class="line">如何做呢？</span><br><span class="line"></span><br><span class="line">* sbin/start-balancer.sh</span><br><span class="line"></span><br><span class="line">parameters = Balancer.BalancerParameters</span><br><span class="line"></span><br><span class="line">[BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5</span><br><span class="line"></span><br><span class="line">数据平衡的默认阈值：threshold = 10.0</span><br><span class="line"></span><br><span class="line">每个节点的磁盘使用率 - 平均的磁盘使用率 &lt;10%</span><br><span class="line"></span><br><span class="line">DN1 存储空间 90%  -76% = 14% 说明这个节点数据多 往别的节点迁移数据 出</span><br><span class="line">DN2 存储空间 60%  -76% = -12% 说明这个节点数据少 别的节点迁移数据 进</span><br><span class="line">DN3 存储空间 80%  -76% = 4% 说明这个节点数据多 往别的节点迁移数据</span><br><span class="line"></span><br><span class="line">avg=90 + 80 +60 /3 = 76%</span><br><span class="line"></span><br><span class="line">生产上 从现在开始 start-balancer.sh -threshold 10  每天要定时做的</span><br><span class="line"></span><br><span class="line">放到业务低谷期去做 数据平衡操作</span><br><span class="line"></span><br><span class="line">注意： 不要在业务高峰期做</span><br><span class="line"></span><br><span class="line">1.数据平衡 数据传输  带宽有关</span><br><span class="line"></span><br><span class="line">调优参数 ：平衡的网络带宽  w</span><br><span class="line"></span><br><span class="line">dfs.datanode.balance.bandwidthPerSec 100m 【2.x 默认是10m】</span><br><span class="line"></span><br><span class="line">每个节点数据几十T  需要数据平衡的数据 几十T  可以申请维护窗口时间 ：</span><br><span class="line"></span><br><span class="line">dfs.datanode.balance.bandwidthPerSec 临时调大 200M</span><br><span class="line"></span><br><span class="line">单个DN节点 多块磁盘的数据平衡</span><br><span class="line"></span><br><span class="line">投产前规划：</span><br><span class="line">DN 机器   10块 2T 【不做raid】  =》 20T   副本的</span><br><span class="line"></span><br><span class="line">1.dn 配置多个磁盘</span><br><span class="line"></span><br></pre></td></tr></table></figure><property>        <name>dfs.datanode.data.dir</name>        <value>/data01,/data02,/data03</value></property>```<p>挂载磁盘一般最省钱的是 2W 转数 2T的</p><p>2.为什么要使用多块物理磁盘？</p><p>1.存储<br>2.因为多个磁盘的io也是叠加的<br>每块磁盘 磁盘io 每秒 100m<br>三块磁盘 1s 能 300m文件内容<br>一块磁盘 1s 100m</p><p>3.可以让服务一直运行，加入一个磁盘挂了，服务不会减少</p><p>做多个磁盘数据均衡<br>    dfs.disk.balancer.enabled  true 【3.x有这个功能 cdh 2.x 也有】 apache 2.x 没有这个功能</p><p>得通过命令去解决磁盘数据均衡？</p><p>hdfs diskbalancer</p><p>1.步骤</p><pre><code>hdfs diskbalancer -plan  bigdata32  =&gt; 生成一个  bigdata32.plan.json 文件    hdfs diskbalancer -execute bigdata32.plan.json =》 执行disk 数据均衡计划文件    hdfs diskbalancer -query bigdata32</code></pre><p>生产上 当你发现 磁盘写入不均衡 可以做一下 【一般 一个月 半个月 做一次即可】</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/11/28/11-28/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>mapreduce</title>
      <link>http://example.com/2022/11/24/11-24/</link>
      <guid>http://example.com/2022/11/24/11-24/</guid>
      <pubDate>Thu, 24 Nov 2022 00:22:22 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;typeScript&quot;&gt;&lt;a href=&quot;#typeScript&quot; class=&quot;headerlink&quot; title=&quot;typeScript&quot;&gt;&lt;/a&gt;typeScript&lt;/h1&gt;&lt;h2 id=&quot;简介：&quot;&gt;&lt;a href=&quot;#简介：&quot; class=&quot;header</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="typeScript"><a href="#typeScript" class="headerlink" title="typeScript"></a>typeScript</h1><h2 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h2><p>ts：typescript ： 简单来说就是js的超集，可以向下兼容js，所以我们使用ts不用对js进行更改，ts会自动转换成js</p><p>因为现在的浏览器不支持ts的语法 ： ts的编译环境是node.js</p><p>安装运行 ： npm install -g typescript</p><p>安装之后用tsv -v 查看</p><p>与逆行的时候下载一个json</p><p>命令行 : tsc -init</p><p>然后运行命令 ： ts-node .&#x2F;文件名</p><p>下面是关于ts的简单介绍</p><p>首先ts是静态类型，js是动态类型</p><h3 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h3><p>静态类型是编译阶段就会发现问题，而动态类型则是直到运行阶段才会问问题的</p><p>所以相比较而言，静态类型，比动态类型更加可控</p><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><p>ts的类型 ：</p><p>包括 : 布尔 ， 数值，字符串， null , symbol , BigInt,undefined</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> isDone :<span class="built_in">boolean</span> = <span class="literal">false</span></span><br><span class="line"><span class="keyword">let</span> <span class="attr">s</span>: <span class="built_in">number</span> = <span class="number">6</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">a</span>: <span class="built_in">number</span> = <span class="title class_">NaN</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">b</span>: <span class="built_in">number</span> = <span class="title class_">Infinity</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">my</span>: <span class="built_in">string</span> = <span class="string">&#x27;yon&#x27;</span></span><br><span class="line"><span class="keyword">let</span> o : <span class="literal">undefined</span></span><br><span class="line"><span class="keyword">let</span> <span class="attr">nme</span>: <span class="literal">null</span>;</span><br><span class="line"><span class="keyword">function</span> <span class="title function_">alertName</span>(<span class="params"></span>) : <span class="built_in">void</span> &#123;</span><br><span class="line">    <span class="title function_">alert</span>(<span class="string">&#x27;my first name&#x27;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">arr</span>: <span class="built_in">any</span>[] = [<span class="string">&quot;hello&quot;</span> , <span class="number">1</span> , <span class="literal">true</span>]</span><br><span class="line"><span class="keyword">let</span> <span class="attr">ids</span>:<span class="built_in">number</span>[] = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>];</span><br><span class="line"><span class="keyword">let</span> <span class="attr">names</span>: <span class="title class_">String</span>[] = [<span class="string">&quot;xxx&quot;</span> , <span class="string">&quot;yyy&quot;</span> , <span class="string">&quot;zzzz&quot;</span>];</span><br><span class="line"><span class="keyword">let</span> <span class="attr">book</span>: <span class="title class_">Object</span>[] = [&#123;<span class="attr">name1</span>:<span class="string">&#x27;tom&#x27;</span> , <span class="attr">animal</span>:<span class="string">&#x27;cont&#x27;</span>&#125;,&#123;<span class="attr">name1</span>:<span class="string">&#x27;gggg&#x27;</span>,<span class="attr">animal</span>:<span class="string">&#x27;hhhh&#x27;</span>&#125;];</span><br><span class="line">ids.<span class="title function_">push</span>(<span class="number">6</span>);</span><br><span class="line"><span class="keyword">let</span> <span class="attr">person</span>:(<span class="built_in">string</span>|<span class="built_in">number</span>|<span class="built_in">boolean</span>)[]=[<span class="string">&#x27;aslkdjla&#x27;</span>,<span class="number">1</span>,<span class="literal">true</span>]; <span class="comment">// 这个是三个值</span></span><br><span class="line"><span class="keyword">let</span> <span class="attr">person1</span>:[<span class="built_in">string</span>,<span class="built_in">number</span>,<span class="built_in">boolean</span>]=[<span class="string">&#x27;sdfjkl&#x27;</span>,<span class="number">1</span>,<span class="literal">true</span>]; <span class="comment">// 这个是一个值</span></span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">info</span>(isDone)<span class="keyword">let</span> isDone :<span class="built_in">boolean</span> = <span class="literal">false</span></span><br><span class="line"><span class="keyword">let</span> <span class="attr">s</span>: <span class="built_in">number</span> = <span class="number">6</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">a</span>: <span class="built_in">number</span> = <span class="title class_">NaN</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">b</span>: <span class="built_in">number</span> = <span class="title class_">Infinity</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="attr">my</span>: <span class="built_in">string</span> = <span class="string">&#x27;yon&#x27;</span></span><br><span class="line"><span class="keyword">let</span> o : <span class="literal">undefined</span></span><br><span class="line"><span class="keyword">let</span> <span class="attr">nme</span>: <span class="literal">null</span>;</span><br><span class="line"><span class="keyword">function</span> <span class="title function_">alertName</span>(<span class="params"></span>) : <span class="built_in">void</span> &#123;</span><br><span class="line">    <span class="title function_">alert</span>(<span class="string">&#x27;my first name&#x27;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">info</span>(isDone)</span><br></pre></td></tr></table></figure><h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2><p>我们定义接口的方式如下：</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">interface</span> a&#123;</span><br><span class="line">    <span class="attr">name</span>:<span class="built_in">string</span>;</span><br><span class="line">    <span class="attr">age</span>:<span class="built_in">number</span>;</span><br><span class="line">    <span class="attr">isP</span>:<span class="built_in">boolean</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>和上述数组的作用一样，不过这样定义可以增强复用性，随便让一个东西继承一下他，就可以了</p><p>比如：</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">interface</span> a&#123;</span><br><span class="line">    <span class="attr">name</span>:<span class="built_in">string</span>;</span><br><span class="line">    <span class="attr">age</span>:<span class="built_in">number</span>;</span><br><span class="line">    <span class="attr">isP</span>:<span class="built_in">boolean</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="attr">p</span>:a=&#123;</span><br><span class="line">    <span class="attr">name</span>:<span class="string">&#x27;kjds&#x27;</span>,</span><br><span class="line">    <span class="attr">age</span>:<span class="number">11</span>,</span><br><span class="line">    <span class="attr">isP</span>:<span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><p>函数合上述的接口很像，他是在接口创建了几个方法，如下</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">interface</span> s&#123;</span><br><span class="line">    <span class="attr">name</span>:<span class="built_in">string</span>;</span><br><span class="line">    <span class="title function_">eat</span>(<span class="attr">name</span>:<span class="built_in">string</span>):<span class="built_in">string</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="attr">j</span>:s=&#123;</span><br><span class="line">    <span class="attr">name</span>:<span class="string">&#x27;lkasdj&#x27;</span>,</span><br><span class="line">    <span class="attr">eat</span>:<span class="keyword">function</span>(<span class="params">name:<span class="built_in">string</span></span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;eat$&#123;name&#125;&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="title function_">add</span> = (<span class="params">a:<span class="built_in">number</span>,b:<span class="built_in">number</span>,c?:<span class="built_in">number</span>|<span class="built_in">string</span></span>)=&gt;&#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(c)</span><br><span class="line">    <span class="keyword">return</span> a+b</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>问号代表从后面的类型里选数据，这里是number和string</p><p>上面的是lambad多</p><p>把ts转换成js的语法是any</p><h2 id="Dom和类型转换"><a href="#Dom和类型转换" class="headerlink" title="Dom和类型转换"></a>Dom和类型转换</h2><p>ts无法像js一样进行访问dom这个就意味着ts无法确定这些dom是不是存在，所以使用非空断言，我们可用明确的告诉他，这个表达式是null&#x2F;undefined</p><p>非空断言 X! 代表把x中的null和undefined给去除了</p><p>例如 ：下面是关于ts和获取dom的</p><p><code>const link = document.querySelector(&#39;a&#39;);</code></p><p>当我们要通过class来选择一个dom元素的时候，可以进行一层转化</p><p>例如 ： <code>const from = document.getElementById(&#39;form&#39;) as HTMLAnchorElement</code></p><h2 id="类"><a href="#类" class="headerlink" title="类"></a>类</h2><p>和java中的类异常类似</p><p>比如 ： </p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Preson</span> &#123; </span><br><span class="line">    <span class="attr">name</span>:<span class="built_in">string</span>;</span><br><span class="line">    <span class="attr">iscool</span>:<span class="built_in">boolean</span>;</span><br><span class="line">    <span class="attr">age</span>:<span class="built_in">number</span>;</span><br><span class="line">    <span class="title function_">constructor</span>(<span class="params">n:<span class="built_in">string</span>,c:<span class="built_in">boolean</span>,a:<span class="built_in">number</span></span>)&#123;</span><br><span class="line">        <span class="variable language_">this</span>.<span class="property">name</span>=n</span><br><span class="line">        <span class="variable language_">this</span>.<span class="property">iscool</span>=c</span><br><span class="line">        <span class="variable language_">this</span>.<span class="property">age</span>=a</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="title function_">sayhai</span>(<span class="params"></span>)&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;hi,wosshi$&#123;this.name&#125;,jinnnan$&#123;this.age&#125;&#x27;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>类的修饰符</p><p>readonly,privatee,protected,public</p><p>简单的介绍就到这里为止，具体以后再慢慢补充</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%9D%82%E8%B4%A7%E6%8A%80%E6%9C%AF%E6%A0%88/">杂货技术栈</category>
      
      
      
      <comments>http://example.com/2022/11/24/11-24/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>mapreduce</title>
      <link>http://example.com/2022/11/21/11-21/</link>
      <guid>http://example.com/2022/11/21/11-21/</guid>
      <pubDate>Mon, 21 Nov 2022 00:47:09 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;续讲Hadoop&quot;&gt;&lt;a href=&quot;#续讲Hadoop&quot; class=&quot;headerlink&quot; title=&quot;续讲Hadoop&quot;&gt;&lt;/a&gt;续讲Hadoop&lt;/h1&gt;&lt;h2 id=&quot;模板模式&quot;&gt;&lt;a href=&quot;#模板模式&quot; class=&quot;headerlink&quot; </description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="续讲Hadoop"><a href="#续讲Hadoop" class="headerlink" title="续讲Hadoop"></a>续讲Hadoop</h1><h2 id="模板模式"><a href="#模板模式" class="headerlink" title="模板模式"></a>模板模式</h2><p>关于mapreduce的操作模式就是模板模式，加上自定义的变量</p><h3 id="模板模式-1"><a href="#模板模式-1" class="headerlink" title="模板模式"></a>模板模式</h3><p>关于模板模式，就是三步走</p><p>开始阶段 ： map阶段</p><p>处理阶段 ： reduce阶段</p><p>结束阶段 ： 关闭流并输出</p><h3 id="自定义的变量"><a href="#自定义的变量" class="headerlink" title="自定义的变量"></a>自定义的变量</h3><p>BooleanWriteable : 布尔类型</p><p>ByteWriteable : Byte类型</p><p>DoubleWriteable : double类型</p><p>FloatWriteable ：float类型</p><p>intWriteable ： int类型</p><p>…（等等）</p><h3 id="mapreduce的核心方法"><a href="#mapreduce的核心方法" class="headerlink" title="mapreduce的核心方法"></a>mapreduce的核心方法</h3><ul><li>input : InputFormat &#x3D;&gt; 如何加载数据</li><li>查看源码发现有DBInputFormat 和 FileInputFormat，代表我们可以从db（数据库中），或者文件中加载数据（上述两个是抽象类，我们实例化的时候其实是实例化其子类）</li><li>简单来说就是读取数据的方法</li><li>默认实例化的时候是实例化 TextInputFormat</li><li>关于TextInputFormat ： 其有两个参数<ul><li>key 和 value<ul><li>key 是 读取文件的位置</li><li>value 是 一行一行的内容<ul><li>在value中有个属性 判断文件可不可以被切分 ： isSpiltable<ul><li>map task 数据 是由 input 的切片数量决定的 ， 当不可被切片的时候对应的数量就是1，对应一个map task</li></ul></li></ul></li></ul></li></ul></li></ul><p>creatRecordReader</p><h2 id="一个文件加载的时候会被切分成几个切片"><a href="#一个文件加载的时候会被切分成几个切片" class="headerlink" title="一个文件加载的时候会被切分成几个切片"></a>一个文件加载的时候会被切分成几个切片</h2><p>前提 ： 文件可以被切分</p><p>当一个文件 在hdfs上的时候是按我们的大小进行切分的 以128m为基础单位</p><p>而我们的map的切片是按照blocksize进行切分的简单来说就是以blocksize的大小进行切分</p><p>总结  ：</p><ul><li>文件大小小于128m<ul><li>那么切分成1片</li></ul></li><li>如果文件大于128m<ul><li>filesize&#x2F;splitesize &#x3D; num 切片数</li><li>filesize剩余的部分 ： 和splitesize的10%比较<ul><li>如果大，则开启一个切片文件</li><li>如果小，则是和上面最后一个合并到一起</li></ul></li></ul></li></ul><p>不能被切分的文件： 某些被压缩文件</p><ul><li>关于这个切片大小的标准 ：<ul><li>在hdfs上是128m</li><li>但是在idea上是32m（在源码里能看）</li></ul></li></ul><h2 id="redurce-task-个数是由上面决定"><a href="#redurce-task-个数是由上面决定" class="headerlink" title="redurce task 个数是由上面决定?"></a>redurce task 个数是由上面决定?</h2><p>默认是 1</p><p>如果要更改要手动更改</p><p>setNumReduceTasks(数量)</p><p>如果 reduce阶段数量变多</p><p>则会把相同的文件拉到一起，就是按照一个规则进行的分区</p><p>默认是走的hashcode ： 这个按照规则来分的就是分区</p><p>一般来说 ： 分区的结果是 suffler 的输出结果是 reduce的输入</p><h3 id="简单需求"><a href="#简单需求" class="headerlink" title="简单需求"></a>简单需求</h3><p>基于phone的存储数据 ，进行分文件存储</p><p>比如</p><p>13开头的存储在一个上</p><p>15开头的在一个文件上</p><p>这就要自定义分区</p><p>通过继承Partitioner类实现方法然后导入方法</p><p>实现getPartition方法进行分区 ： 以下是一个简单的分区函数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">myPartionit</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text , IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text text, IntWritable intWritable, <span class="type">int</span> i)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (text.toString().equals(<span class="string">&quot;ANALYST&quot;</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (text.toString().equals(<span class="string">&quot;CLERK&quot;</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (text.toString().equals(<span class="string">&quot;MANAGER&quot;</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (text.toString().equals(<span class="string">&quot;PRESIDENT&quot;</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">3</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (text.toString().equals(<span class="string">&quot;SALESMAN&quot;</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">4</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">5</span>;</span><br><span class="line">&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后我们要在我们的主程序中调用这个类 ， test类：就是我们的主类，如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> sxwang</span></span><br><span class="line"><span class="comment"> * 11 18 14:00</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">test</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * driver</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (args.length &lt; <span class="number">2</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;出现问题最少两个参数&quot;</span>);</span><br><span class="line">System.exit(<span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line">        String input=args[<span class="number">0</span>];</span><br><span class="line">String output=args[<span class="number">1</span>];</span><br><span class="line"><span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"><span class="comment">//0.todo... 删除目标路径</span></span><br><span class="line">FileUtils.deletePath(conf,output);</span><br><span class="line"></span><br><span class="line"><span class="comment">//1.设置 作业名称</span></span><br><span class="line"><span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">&quot;WCAPP&quot;</span>);</span><br><span class="line"><span class="comment">//2.设置map reduce 执行代码的主类</span></span><br><span class="line">job.setJarByClass(test.class);</span><br><span class="line">job.setMapperClass(MyMapper.class);</span><br><span class="line">job.setReducerClass(MyReducer.class);</span><br><span class="line"><span class="comment">//3.指定 oupput kv类型</span></span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(IntWritable.class);</span><br><span class="line">job.setNumReduceTasks(<span class="number">6</span>);</span><br><span class="line">job.setPartitionerClass(myPartionit.class);</span><br><span class="line"><span class="comment">//4. 设置数据源路径 数据输出路径</span></span><br><span class="line">FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(input));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(output));</span><br><span class="line"><span class="comment">//5. 提交mr yarn</span></span><br><span class="line">System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * mapper</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MyMapper</span></span><br><span class="line">            <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * 1.按照分隔符 进行拆分 每个单词 ，每个单词赋值为1</span></span><br><span class="line"><span class="comment">             * (word ,1)</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line"></span><br><span class="line">String[] words = value.toString().split(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                String[] split = word.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">context.write(<span class="keyword">new</span> <span class="title class_">Text</span>(split[<span class="number">2</span>]) ,<span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * reducer</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MyReducer</span></span><br><span class="line">            <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text,IntWritable,Text,IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *  (word ,1)</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         *  (word,&lt;1,1,1,1&gt;)</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         *  1.聚合value</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         *  2.写出去</span></span><br><span class="line"><span class="comment">*  (word ,3)</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            <span class="type">int</span> sum=<span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">                sum +=Integer.parseInt(value.toString());</span><br><span class="line">&#125;</span><br><span class="line">            context.write(key,<span class="keyword">new</span> <span class="title class_">IntWritable</span>(sum));</span><br><span class="line">&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="sql-vs-mr-mapreduce"><a href="#sql-vs-mr-mapreduce" class="headerlink" title="sql vs mr(mapreduce)"></a>sql vs mr(mapreduce)</h2><ul><li>sql  :<ul><li>group by</li><li>distinct</li><li>join</li><li>order by</li><li>union<ul><li><p>group by 用mr实现</p></li><li><p>就是再map阶段进行处理reduce阶段进行合并</p></li><li><p>distinct</p></li><li></li><li><p>去重 ： sql ：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span>(ename) <span class="keyword">from</span> emp;</span><br><span class="line">#或者可以通过分组进行去重</span><br><span class="line"><span class="keyword">select</span> name <span class="keyword">from</span> <span class="keyword">group</span> <span class="keyword">by</span> name;</span><br></pre></td></tr></table></figure></li><li><p>通过上述我们可知group by 也可以进行去重 ，所以后面我们能用group by 就要用group by</p></li><li><p>因为distinct只有一个task进行处理</p></li><li><p>而group by 则是多个task 进行处理 ， 所以效率会比较高</p></li><li><p>接下来是在mr里实现</p></li><li><p>order by</p></li><li><p>mr :</p></li><li><p>全局排序 ： reduce task 是 1</p></li><li><p>分区排序 ： reduce task 是 多个</p></li><li><p>mr : 实现</p></li><li></li></ul></li></ul></li></ul><h2 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h2><p>如果你的reduce task 大于分区数 ： 会有空白文件</p><p>如果 reduce task 小于分区数 且大于1 ： 则会报错</p><p>如果 reduce task 且分区数 等于 1 ： 则会把东西整合到一个文件</p><h2 id="关于mapreduce在linux上运行的学习，官方提供了源码库"><a href="#关于mapreduce在linux上运行的学习，官方提供了源码库" class="headerlink" title="关于mapreduce在linux上运行的学习，官方提供了源码库"></a>关于mapreduce在linux上运行的学习，官方提供了源码库</h2><p>地址在github上：</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%97%A5%E5%BF%97/">日志</category>
      
      
      
      <comments>http://example.com/2022/11/21/11-21/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>把jar包打包成exe</title>
      <link>http://example.com/2022/11/19/jar%E5%88%B0exe/</link>
      <guid>http://example.com/2022/11/19/jar%E5%88%B0exe/</guid>
      <pubDate>Sat, 19 Nov 2022 03:40:21 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;使用工具&quot;&gt;&lt;a href=&quot;#使用工具&quot; class=&quot;headerlink&quot; title=&quot;使用工具&quot;&gt;&lt;/a&gt;使用工具&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;我们所使用的工具是exe4j 和 inno setup&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;exe4j&quot;&gt;&lt;a</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="使用工具"><a href="#使用工具" class="headerlink" title="使用工具"></a>使用工具</h1><ul><li>我们所使用的工具是exe4j 和 inno setup</li></ul><h2 id="exe4j"><a href="#exe4j" class="headerlink" title="exe4j"></a>exe4j</h2><ul><li>简单来说这个就是个工具，可以把java的jar包打包成exe的</li><li>可以让其他由java运行环境的机器运行这个jar</li><li>而且还可以稍微做到保密的程度</li><li>exe4j 是一个帮助你集成 Java 应用程序到 Windows 操作环境的 java 可执行文件生成工具，无论这些应用是用于服务器，还是图形用户界面（GUI）或命令行的应用程序。如果你想在任务管理器中及 Windows XP 分组的用户友好任务栏里以你的进程名取代 java.exe 的出现，那么 exe4j 可以完成这个工作。exe4j 帮助你以一种安全的方式启动你的 java 应用程序，来显示本地启动画面，检测及发布合适的 JRE 和 JDK，以及进行启动时所发生的错误处理等，以至于更多。</li><li>关于exe4j的破解我就不多赘述了，懂的都懂</li><li>接下来是实际操作</li><li>下面是开始界面</li><li><img src="https://pic.imgdb.cn/item/6378540416f2c2beb192c11f.jpg" alt="开始界面"></li><li>然后接下来</li><li>选择jar到exe的模式</li><li><img src="https://pic.imgdb.cn/item/6378546916f2c2beb1932dfa.jpg"></li><li>然后接下来要选择你的输出路径</li><li><img src="https://pic.imgdb.cn/item/637854cd16f2c2beb193c56c.jpg"></li><li>接下来选择我们的启动方式，我选择的是控制台启动</li><li><img src="https://pic.imgdb.cn/item/6378550616f2c2beb1942a62.jpg"></li><li>接下来选择可以在32位和64位机器上都可以运行</li><li><img src="https://pic.imgdb.cn/item/6378553316f2c2beb194a3be.jpg"></li><li><img src="https://pic.imgdb.cn/item/6378556516f2c2beb1952fa9.jpg"></li><li>接下来选择我们exe要执行的主类</li><li>我这个项目是boot项目，所以选择的是这个</li><li><img src="https://pic.imgdb.cn/item/6378558416f2c2beb195719f.jpg"></li><li>然后设置我们的jdk版本</li><li><img src="https://pic.imgdb.cn/item/637855bd16f2c2beb195f1d6.jpg"></li><li><img src="https://pic.imgdb.cn/item/637855dc16f2c2beb1962843.jpg"></li><li>最后一直点击next就好了</li><li><img src="https://pic.imgdb.cn/item/6378564016f2c2beb196ce41.jpg"></li><li>这样我们就打包完成了</li><li>但是这样的打包只能再有jdk的环境中运行</li><li>接下来我们要为这个添加依赖</li></ul><h2 id="inno-setup"><a href="#inno-setup" class="headerlink" title="inno setup"></a>inno setup</h2><ul><li>简单来说就是个绑定依赖的程序</li><li>打开</li><li><img src="https://pic.imgdb.cn/item/6378572316f2c2beb198e2a2.jpg"></li><li><img src="https://pic.imgdb.cn/item/6378577616f2c2beb199aecb.jpg"></li><li><img src="https://pic.imgdb.cn/item/6378579d16f2c2beb19a1b5f.jpg"></li><li>然后配置我们的名称和版本号</li><li>然后选择我们，刚刚弄个出来的exe程序</li><li><img src="https://pic.imgdb.cn/item/637858cd16f2c2beb19b780b.jpg"></li><li>然后就一直傻瓜式next</li><li><img src="https://pic.imgdb.cn/item/6378593316f2c2beb19be141.jpg"></li><li>设置输出文件夹</li><li>图标</li><li>以及运行时的密码</li><li>然后一直next就好</li><li>然后会跳出两个对话框</li><li>全部选择是</li><li>最后会出现这个东西</li><li><img src="https://pic.imgdb.cn/item/637859e916f2c2beb19d21f9.jpg"></li><li>就代表导入依赖了</li><li>接下来我们要对他进行修改</li><li><img src="https://pic.imgdb.cn/item/63785a3d16f2c2beb19d8050.jpg"></li><li>这个是自己电脑本机的jdk路径</li><li>然后还有要在这里加上MYjrename</li><li><img src="https://pic.imgdb.cn/item/63785a8116f2c2beb19dd2fa.jpg"></li><li>最后运行就成功了</li><li>他会给你成一个setup文件，通过这个文件安装的exe</li><li>就会自己带环境了</li><li></li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%9D%82%E8%B4%A7%E6%8A%80%E6%9C%AF%E6%A0%88/">杂货技术栈</category>
      
      
      
      <comments>http://example.com/2022/11/19/jar%E5%88%B0exe/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>内网穿透</title>
      <link>http://example.com/2022/11/19/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/</link>
      <guid>http://example.com/2022/11/19/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/</guid>
      <pubDate>Sat, 19 Nov 2022 03:18:15 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;内网穿透&quot;&gt;&lt;a href=&quot;#内网穿透&quot; class=&quot;headerlink&quot; title=&quot;内网穿透&quot;&gt;&lt;/a&gt;内网穿透&lt;/h1&gt;&lt;h2 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="内网穿透"><a href="#内网穿透" class="headerlink" title="内网穿透"></a>内网穿透</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><ul><li>内网穿透简单来说就是让我们处于局域网的机器或者本地的项目，可以在外部的公网访问</li></ul><h3 id="常见的内网穿透工具"><a href="#常见的内网穿透工具" class="headerlink" title="常见的内网穿透工具"></a>常见的内网穿透工具</h3><ul><li>natapp网站</li><li>frp</li><li>ngrok</li><li>蜻蜓映射</li><li>ssh命令</li></ul><h3 id="natapp"><a href="#natapp" class="headerlink" title="natapp"></a>natapp</h3><ul><li>natapp网站 ： 注册账号之后会免费给你送一条隧道，不过临时用可以，不能长久用，</li><li>因为隧道名字会被强制更换</li><li>而vip隧道就不会强制更换</li></ul><h3 id="frp"><a href="#frp" class="headerlink" title="frp"></a>frp</h3><ul><li>frp是专业的反向代理工具</li><li><code>https:github.com/fatedier/frp</code></li><li>其中部署也非常简单 ： </li><li>frps在云服务器上</li><li>而要进行内网穿透的机器运行frpc</li><li>再改改配置就ok了</li></ul><h3 id="ngrok"><a href="#ngrok" class="headerlink" title="ngrok"></a>ngrok</h3><ul><li>属于专门做内网穿透的平台，以前是免费的不过最近好像收费了</li><li>注册之后我们就可以根据平台上的帮助文档进行配置，</li></ul><h3 id="蜻蜓映射"><a href="#蜻蜓映射" class="headerlink" title="蜻蜓映射"></a>蜻蜓映射</h3><ul><li>同上</li></ul><h3 id="ssh命令"><a href="#ssh命令" class="headerlink" title="ssh命令"></a>ssh命令</h3><ul><li>我们可以通过<code>ssh -R 80:localhost:80 xxxxx@localhost.run</code></li><li>进行反向映射端口</li><li>xxxx是远程云主机</li><li>前面的80是云主机的端口</li><li>后面的是本地的端口</li><li>这个命令是ssh自带的</li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%9D%82%E8%B4%A7%E6%8A%80%E6%9C%AF%E6%A0%88/">杂货技术栈</category>
      
      
      
      <comments>http://example.com/2022/11/19/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>11-19作业</title>
      <link>http://example.com/2022/11/19/%E4%BD%9C%E4%B8%9A11-19/</link>
      <guid>http://example.com/2022/11/19/%E4%BD%9C%E4%B8%9A11-19/</guid>
      <pubDate>Sat, 19 Nov 2022 03:13:45 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;通过mapreduce的api统计emp表里的每个工作的人数&quot;&gt;&lt;a href=&quot;#通过mapreduce的api统计emp表里的每个工作的人数&quot; class=&quot;headerlink&quot; title=&quot;通过mapreduce的api统计emp表里的每个工作的人数&quot;&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="通过mapreduce的api统计emp表里的每个工作的人数"><a href="#通过mapreduce的api统计emp表里的每个工作的人数" class="headerlink" title="通过mapreduce的api统计emp表里的每个工作的人数"></a>通过mapreduce的api统计emp表里的每个工作的人数</h1><ul><li>数据源</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">7369,SMITH,CLERK,7902,1980-12-17 14:00:00,800.00,,20</span><br><span class="line">7499,ALLEN,SALESMAN,7698,1981-02-20 14:00:00,1600.00,300.00,30</span><br><span class="line">7521,WARD,SALESMAN,7698,1981-02-22 14:00:00,1250.00,500.00,30</span><br><span class="line">7566,JONES,MANAGER,7839,1981-04-02 14:00:00,2975.00,,20</span><br><span class="line">7654,MARTIN,SALESMAN,7698,1981-09-28 13:00:00,1250.00,1400.00,30</span><br><span class="line">7698,BLAKE,MANAGER,7839,1981-05-01 13:00:00,2850.00,,30</span><br><span class="line">7782,CLARK,MANAGER,7839,1981-06-09 13:00:00,2450.00,,10</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7839,KING,PRESIDENT,,1981-11-17 14:00:00,5000.00,,10</span><br><span class="line">7844,TURNER,SALESMAN,7698,1981-09-08 13:00:00,1500.00,0.00,30</span><br><span class="line">7876,ADAMS,CLERK,7788,1983-01-12 14:00:00,1100.00,,20</span><br><span class="line">7900,lebulang,CLERK,7698,1981-12-03 14:00:00,950.00,,30</span><br><span class="line">7902,FORD,ANALYST,7566,1981-12-03 14:00:00,3000.00,,20</span><br><span class="line">7934,MILLER,CLERK,7782,1982-01-23 14:00:00,1300.00,,10</span><br><span class="line">7839,KING,PRESIDENT,,1981-11-17 14:00:00,5000.00,,10</span><br><span class="line">7654,MARTIN,SALESMAN,7698,1981-09-28 13:00:00,3200.00,1400.00,30</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line">7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><p>其中第二列就是工作</p></li><li><p>代码如下 ：</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">package org.example;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @author sxwang</span><br><span class="line"> * 11 18 14:00</span><br><span class="line"> */</span><br><span class="line">public class test &#123;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * driver</span><br><span class="line">     * @param args</span><br><span class="line">     */</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        String input=&quot;D://emp.txt&quot;;</span><br><span class="line">        String output=&quot;out&quot;;</span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        //0.todo... 删除目标路径</span><br><span class="line">        FileUtils.deletePath(conf,output);</span><br><span class="line"></span><br><span class="line">        //1.设置 作业名称</span><br><span class="line">        Job job = Job.getInstance(conf, &quot;WCAPP&quot;);</span><br><span class="line">        //2.设置map reduce 执行代码的主类</span><br><span class="line">        job.setJarByClass(test.class);</span><br><span class="line">        job.setMapperClass(MyMapper.class);</span><br><span class="line">        job.setReducerClass(MyReducer.class);</span><br><span class="line">        //3.指定 oupput kv类型</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        //4. 设置数据源路径 数据输出路径</span><br><span class="line">        FileInputFormat.addInputPath(job, new Path(input));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(output));</span><br><span class="line">        //5. 提交mr yarn</span><br><span class="line">        System.exit(job.waitForCompletion(true) ? 0 : 1);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * mapper</span><br><span class="line">     */</span><br><span class="line">    public static class MyMapper</span><br><span class="line">            extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        protected void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            /**</span><br><span class="line">             * 1.按照分隔符 进行拆分 每个单词 ，每个单词赋值为1</span><br><span class="line">             * (word ,1)</span><br><span class="line">             */</span><br><span class="line"></span><br><span class="line">            String[] words = value.toString().split(&quot;\n&quot;);</span><br><span class="line"></span><br><span class="line">            for (String word : words) &#123;</span><br><span class="line">                String[] split = word.split(&quot;,&quot;);</span><br><span class="line">                context.write(new Text(split[2]) ,new IntWritable(1));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * reducer</span><br><span class="line">     */</span><br><span class="line">    public static class MyReducer</span><br><span class="line">            extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">        /**</span><br><span class="line">         *  (word ,1)</span><br><span class="line">         *</span><br><span class="line">         *  (word,&lt;1,1,1,1&gt;)</span><br><span class="line">         *</span><br><span class="line">         *  1.聚合value</span><br><span class="line">         *</span><br><span class="line">         *  2.写出去</span><br><span class="line">         *  (word ,3)</span><br><span class="line">         */</span><br><span class="line">        @Override</span><br><span class="line">        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            int sum=0;</span><br><span class="line">            for (IntWritable value : values) &#123;</span><br><span class="line">                sum +=Integer.parseInt(value.toString());</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key,new IntWritable(sum));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>基本概念 ： 把数据先通过map进行etl，然后通过redurce进行数据的整合之类的</li><li>最后输出</li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E4%BD%9C%E4%B8%9A/">作业</category>
      
      
      
      <comments>http://example.com/2022/11/19/%E4%BD%9C%E4%B8%9A11-19/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
