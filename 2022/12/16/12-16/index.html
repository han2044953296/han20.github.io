<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/%E6%A0%91%E5%8F%B6_sleaves%20(1).png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/%E6%A0%91%E5%8F%B6_sleaves.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-flash.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zihang.fun","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":15,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="MQ:消息中间件  java -》 rabbitMq kafka 大数据 -》 kafka，pular ， solar  kafka： 在cdh平台叫cdk  官网：kafka.apache.org 他是一个流式的分布式平台，构建实时的数据通道，流式数据分析，流式的app 实时处理&#x2F;流式处理 离线处理&#x2F;批处理 消息中间件：  消息：event-》事件-》数据 数据存储的地方：中">
<meta property="og:type" content="article">
<meta property="og:title" content="kafka">
<meta property="og:url" content="http://zihang.fun/2022/12/16/12-16/index.html">
<meta property="og:site_name" content="枫叶冢">
<meta property="og:description" content="MQ:消息中间件  java -》 rabbitMq kafka 大数据 -》 kafka，pular ， solar  kafka： 在cdh平台叫cdk  官网：kafka.apache.org 他是一个流式的分布式平台，构建实时的数据通道，流式数据分析，流式的app 实时处理&#x2F;流式处理 离线处理&#x2F;批处理 消息中间件：  消息：event-》事件-》数据 数据存储的地方：中">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-12-16T00:44:05.688Z">
<meta property="article:modified_time" content="2022-12-22T08:39:12.554Z">
<meta property="article:author" content="liu zihang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://zihang.fun/2022/12/16/12-16/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>kafka | 枫叶冢</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss2.xml" title="枫叶冢" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
    <a target="_blank" rel="noopener" href="https://github.com/han2044953296" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#fff; color:#151513; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">枫叶冢</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zihang.fun/2022/12/16/12-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="liu zihang">
      <meta itemprop="description" content="只有努力不会辜负你">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="枫叶冢">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          kafka
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-12-16 08:44:05" itemprop="dateCreated datePublished" datetime="2022-12-16T08:44:05+08:00">2022-12-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-12-22 16:39:12" itemprop="dateModified" datetime="2022-12-22T16:39:12+08:00">2022-12-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%97%A5%E5%BF%97/" itemprop="url" rel="index"><span itemprop="name">日志</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>22k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>20 分钟</span>
            </span>

        </div>
      </header>

    
    
    

   

    <div class="post-body" itemprop="articleBody">

      
        <p>MQ:消息中间件</p>
<ul>
<li>java -》 rabbitMq kafka</li>
<li>大数据 -》 kafka，pular ， solar</li>
</ul>
<h1 id="kafka："><a href="#kafka：" class="headerlink" title="kafka："></a>kafka：</h1><ul>
<li>在cdh平台叫cdk</li>
</ul>
<p>官网：kafka.apache.org</p>
<p>他是一个流式的分布式平台，构建实时的数据通道，流式数据分析，流式的app</p>
<p>实时处理&#x2F;流式处理</p>
<p>离线处理&#x2F;批处理</p>
<p>消息中间件：</p>
<ul>
<li>消息：event-》事件-》数据</li>
<li>数据存储的地方：中间件</li>
</ul>
<p>kafka的特性</p>
<ul>
<li>高吞吐量</li>
<li>可扩展性：支持分布式</li>
<li>永久性存储 ：有数据过期时间</li>
<li>高可用</li>
</ul>
<p>kafka的特点：</p>
<ul>
<li>读写（发布和订阅）</li>
<li>存储流式的数据</li>
<li>可以进行数据的处理<ul>
<li>正常的kafka处理，是在它后面接一个实时处理的框架 spark&#x2F;fink</li>
<li>kafkaStreaming ：是kafka自带的专门处理数据的功能（性能：以及数据的丢包什么的差距会比较大）</li>
</ul>
</li>
</ul>
<p>部署：kafka的版本要选择</p>
<ul>
<li>apache</li>
<li>cdk</li>
</ul>
<p>一般选择稳定版，最新版bug多</p>
<p>还可以根据后面的框架选择 ：根据sparkStreaming 来选择kafka的版本 ： spark对kafka的最低的版本是0.10.0 （目前3.x的spark）</p>
<p>kafka：</p>
<ul>
<li>0.10之后都可以</li>
</ul>
<p>kafka用的是scala写的</p>
<p>kafka2.8版本bug多，因为2.8版本做了个尝试，就是抛去zk，但是尝试失败了，最终出现多个bug</p>
<p>我们目前用2.2.1的版本</p>
<p>kafka的架构：</p>
<ul>
<li>生产者 ： producer 发送数据的</li>
<li>kafka ：broker 真正是属于卡夫卡的，其余是不属于kafka的组件的</li>
<li>消费者 ：customer 取出数据 ，但是数据并不会传出去，只是把副本发出去</li>
</ul>
<p>这里的数据也叫events</p>
<p>扩展架构：</p>
<ul>
<li>broker kafka的集群的一个节点 ，相当于我们的机器</li>
<li>broker：topic 主题<ul>
<li>负责存储events</li>
<li>订阅和发送都是基于topic来实现的<ul>
<li>只要订阅了这个topic，就可以知道他里面的数据</li>
<li>发送也是同理</li>
<li>一个kafka里可以有多个topic</li>
<li>相当于这个是个有编号的数据仓库</li>
<li>关于kafka的topic，我们是和他的效率成一个曲线波动的，比如在一定区间上升的比较多，一定区间反而下降</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>topic ：</p>
<ul>
<li>partition：分区<ul>
<li>一个topic可以有多个paitition</li>
<li>每个partition是一个有序的序列</li>
<li>分区并不是越多越好，最好是可以被我们的机器整除的，避免数据倾斜</li>
<li>其分区中每个分区都有一个标记信息，叫offset，在offset规定的时间里，我们可以随意的消费这个数据，他会一直在</li>
<li>而且和可以对offset的值进行修改</li>
</ul>
</li>
<li>topic的数据是放在不同的目录下面的，就是分区下面</li>
</ul>
<p>部署 ：</p>
<p>解压 - 》 软连接 -》 环境变量 -》 vim server.properties</p>
<ul>
<li>borker.id : id编号</li>
<li>log.retention.huors:数据保留的时间</li>
<li>zookeeper.connect:zk的链接</li>
<li>log.dirs：数据的存储文件夹</li>
<li>hostname：机器的名字</li>
<li>port：kafka对外的端口</li>
</ul>
<p>更改成功之后，我们启动我们的kafka</p>
<p>kafka的启动有两种方式，然后我们可以通过jps进行查看它</p>
<p>简单的命令：<code>kafka-server-start.sh  -daemon $KAFKA_HOME/config/server.properties</code></p>
<h2 id="查看kafka的topic"><a href="#查看kafka的topic" class="headerlink" title="查看kafka的topic"></a>查看kafka的topic</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics<span class="selector-class">.sh</span> \</span><br><span class="line"><span class="attr">--list</span> \</span><br><span class="line"><span class="attr">--zookeeper</span> bigdata3:<span class="number">2181</span>,bigdata4:<span class="number">2181</span>,bigdata5:<span class="number">2181</span>/kafka</span><br></pre></td></tr></table></figure>

<h2 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics<span class="selector-class">.sh</span> \</span><br><span class="line"><span class="attr">--create</span> \</span><br><span class="line"><span class="attr">--zookeeper</span> bigdata3:<span class="number">2181</span>,bigdata4:<span class="number">2181</span>,bigdata5:<span class="number">2181</span>/kafka \</span><br><span class="line"><span class="attr">--topic</span> dl2262 \</span><br><span class="line"><span class="attr">--partitions</span> <span class="number">6</span> \</span><br><span class="line"><span class="attr">--replication-factor</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>

<p>注意 ：</p>
<p>partition的随便指定，最好是机器的倍数</p>
<p>副本-》容错</p>
<p>replication-factor topic的副本数小于等于机器数</p>
<h2 id="查看详细的topic信息"><a href="#查看详细的topic信息" class="headerlink" title="查看详细的topic信息"></a>查看详细的topic信息</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">kafka-topics.sh</span> <span class="string">\</span></span><br><span class="line"><span class="string">--describe</span> <span class="string">\</span></span><br><span class="line"><span class="string">--zookeeper</span> <span class="string">bigdata3:2181,bigdata4:2181,bigdata5:2181/kafka</span> <span class="string">\</span></span><br><span class="line"><span class="string">--topic</span> <span class="string">dl2262</span> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">Topic:dl2262</span>	<span class="string">PartitionCount:6</span>	<span class="string">ReplicationFactor:3</span>	<span class="attr">Configs:</span></span><br><span class="line">	<span class="attr">Topic:</span> <span class="string">dl2262</span>	<span class="attr">Partition:</span> <span class="number">0</span>	<span class="attr">Leader:</span> <span class="number">1</span>	<span class="attr">Replicas:</span> <span class="number">1</span><span class="string">,0,2</span>	<span class="attr">Isr:</span> <span class="number">1</span><span class="string">,0,2</span></span><br><span class="line">	<span class="attr">Topic:</span> <span class="string">dl2262</span>	<span class="attr">Partition:</span> <span class="number">1</span>	<span class="attr">Leader:</span> <span class="number">2</span>	<span class="attr">Replicas:</span> <span class="number">2</span><span class="string">,1,0</span>	<span class="attr">Isr:</span> <span class="number">2</span><span class="string">,1,0</span></span><br><span class="line">	<span class="attr">Topic:</span> <span class="string">dl2262</span>	<span class="attr">Partition:</span> <span class="number">2</span>	<span class="attr">Leader:</span> <span class="number">0</span>	<span class="attr">Replicas:</span> <span class="number">0</span><span class="string">,2,1</span>	<span class="attr">Isr:</span> <span class="number">0</span><span class="string">,2,1</span></span><br><span class="line">	<span class="attr">Topic:</span> <span class="string">dl2262</span>	<span class="attr">Partition:</span> <span class="number">3</span>	<span class="attr">Leader:</span> <span class="number">1</span>	<span class="attr">Replicas:</span> <span class="number">1</span><span class="string">,2,0</span>	<span class="attr">Isr:</span> <span class="number">1</span><span class="string">,2,0</span></span><br><span class="line">	<span class="attr">Topic:</span> <span class="string">dl2262</span>	<span class="attr">Partition:</span> <span class="number">4</span>	<span class="attr">Leader:</span> <span class="number">2</span>	<span class="attr">Replicas:</span> <span class="number">2</span><span class="string">,0,1</span>	<span class="attr">Isr:</span> <span class="number">2</span><span class="string">,0,1</span></span><br><span class="line">	<span class="attr">Topic:</span> <span class="string">dl2262</span>	<span class="attr">Partition:</span> <span class="number">5</span>	<span class="attr">Leader:</span> <span class="number">0</span>	<span class="attr">Replicas:</span> <span class="number">0</span><span class="string">,1,2</span>	<span class="attr">Isr:</span> <span class="number">0</span><span class="string">,1,2</span></span><br><span class="line"></span><br><span class="line"><span class="string">第一行</span> <span class="string">：</span> <span class="string">topic的总体情况</span> <span class="string">topic的名字</span> <span class="string">分区数量</span> <span class="string">分区数量</span> <span class="string">副本数量</span></span><br><span class="line"><span class="string">第二行及以下</span> <span class="string">：</span> <span class="string">topic名字</span> <span class="string">分区的编号</span> <span class="string">leader的编号：负责对外进行读写的kafka的编号</span> <span class="string">Replicas:当前分区副本在哪些机器上是编号</span> <span class="string">isr：</span> <span class="string">负责对外进行读写请求的读写的顺序（机器编号）</span></span><br><span class="line"><span class="string">实际上是有个顺序的，leader的分配以及，读写的顺序</span></span><br></pre></td></tr></table></figure>

<h2 id="删除topic"><a href="#删除topic" class="headerlink" title="删除topic"></a>删除topic</h2><p>个人建议生产上不要删除topic</p>
<p>当topic的数量变多的时候，如果你删除了一个，则它可能会崩掉</p>
<p>修改也同上 ：</p>
<figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh <span class="string">\</span></span><br><span class="line">--<span class="keyword">delete</span> <span class="string">\</span></span><br><span class="line">--zookeeper bigdata3:<span class="number">2181</span>,bigdata4:<span class="number">2181</span>,bigdata5:<span class="number">2181</span>/kafka <span class="string">\</span></span><br><span class="line">--topic dl2262 </span><br></pre></td></tr></table></figure>

<p>topic ： 有俩个数据</p>
<ul>
<li>磁盘上</li>
<li>zk中的</li>
</ul>
<p>执行之后，那个topic会被打上删除的标记，但是实际上数据还是在磁盘上的</p>
<p>如果真的想删除，可以在conf里加上 ，delete.topic.enable&#x3D;true</p>
<h2 id="修改topic"><a href="#修改topic" class="headerlink" title="修改topic"></a>修改topic</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics<span class="selector-class">.sh</span> \</span><br><span class="line"><span class="attr">--alter</span> \</span><br><span class="line"><span class="attr">--zookeeper</span> bigdata3:<span class="number">2181</span>,bigdata4:<span class="number">2181</span>,bigdata5:<span class="number">2181</span>/kafka \</span><br><span class="line"><span class="attr">--topic</span> dl2262 \</span><br><span class="line"><span class="attr">--partitions</span> <span class="number">6</span> \</span><br><span class="line"><span class="attr">--replication-factor</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>

<p>想修改什么就加什么参数</p>
<p>但是生产上也不要用：因为可能会导致kafka崩</p>
<p>补充：</p>
<ul>
<li>kafka如何进行数据迁移</li>
<li>kafka进行压力测试</li>
</ul>
<h2 id="生产数据"><a href="#生产数据" class="headerlink" title="生产数据"></a>生产数据</h2><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-producer.<span class="keyword">sh </span>\ </span><br><span class="line">--<span class="keyword">broker-list </span><span class="keyword">bigdata3:9092,bigdata4:9092,bigdata35:9092 </span>\</span><br><span class="line">--topic test</span><br></pre></td></tr></table></figure>

<p>正常我们是通过代码的方式进行编写的</p>
<p>官网的wproduceapi里有详细的介绍</p>
<p>首先要加入我们的代码</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>而对于Streaming要添加</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-streams<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>kafka封装了一套二进制通信协议，用于对外提供各种各样的服务，对于producer而言，用户可以使用任意编程语言按照该协议的格式进行编程，从而实现向kafka发送消息。这组协议本质上为不同的协议类型分别定义了专属的紧凑二进制字节数组格式，然后通过socket发送给合适的broker，之后等待broker处理完成后返还响应给producer。</p>
<p>每个producer都是独立进行工作的，与其他producer之间没有关联，目前producer的首要功能就是向某个topic的某个分区发送一条消息，所以它首先需要确认到底向topic的哪个分区写入消息，这就是分区器（partitioner）的事情。kafka producer提供了一个默认的分区器，对于每条待发送的消息，如果该消息指定了key，那么该partitioner会根据key的哈希值来选择目标分区，若没有，会使用轮询的方式确认目标分区。当然，producer的API赋予了用户自行指定目标分区的权力。</p>
<p>在确认了目标分区后，producer要做的第二件事就是要寻找这个分区对应的leader，只有leader才能响应客户端发送过来的请求，而剩下的从节点中有一部分会同步该消息。因此在发送消息时，producer有不等待任何副本的响应便返回成功，或者只等待leader响应写入操作之后再返回成功。<br>代码如下 ；</p>
<figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> void main(<span class="keyword">String</span>[] args) &#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> <span class="type">Properties</span>();</span><br><span class="line">        <span class="comment">// 必须</span></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>,<span class="string">&quot;121.5.240.148:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// 被发送到broker的任何消息的格式都必须是字节数组</span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        <span class="comment">// 非必须参数配置</span></span><br><span class="line">        <span class="comment">// acks=0表明producer完全不管发送结果；</span></span><br><span class="line">        <span class="comment">// acks=all或-1表明producer会等待ISR所有节点均写入后的响应结果；</span></span><br><span class="line">        <span class="comment">// acks=1，表明producer会等待leader写入后的响应结果</span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>,<span class="string">&quot;-1&quot;</span>);</span><br><span class="line">        <span class="comment">// 发生可重试异常时的重试次数</span></span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>,<span class="number">3</span>);</span><br><span class="line">         <span class="comment">// producer会将发往同一分区的多条消息封装进一个batch中，</span></span><br><span class="line">        <span class="comment">// 当batch满了的时候，发送其中的所有消息,不过并不总是等待batch满了才发送消息；</span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>,<span class="number">323840</span>);</span><br><span class="line">         <span class="comment">// 控制消息发送延时，默认为0，即立即发送，无需关心batch是否已被填满。</span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>,<span class="number">10</span>);</span><br><span class="line">        <span class="comment">// 指定了producer用于缓存消息的缓冲区大小，单位字节，默认32MB</span></span><br><span class="line">        <span class="comment">// producer启动时会首先创建一块内存缓冲区用于保存待发送的消息，然后由另一个专属线程负责从缓冲区中读取消息执行真正的发送</span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>,<span class="number">33554432</span>);</span><br><span class="line">        <span class="comment">// 设置producer能发送的最大消息大小</span></span><br><span class="line">        props.put(<span class="string">&quot;max.request.size&quot;</span>,<span class="number">10485760</span>);</span><br><span class="line">        <span class="comment">// 设置是否压缩消息，默认none</span></span><br><span class="line">        props.put(<span class="string">&quot;compression.type&quot;</span>,<span class="string">&quot;lz4&quot;</span>);</span><br><span class="line">        <span class="comment">// 设置消息发送后，等待响应的最大时间</span></span><br><span class="line">        props.put(<span class="string">&quot;request.timeout.ms&quot;</span>,<span class="number">30</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt; producer = <span class="keyword">new</span> <span class="type">KafkaProducer</span>&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;(props);</span><br><span class="line">        <span class="keyword">for</span>(int i = <span class="number">0</span>;i&lt;<span class="number">5</span>;i++)&#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> <span class="type">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;my-replicated-topic&quot;</span>,<span class="string">&quot;key&quot;</span>+i,<span class="string">&quot;value&quot;</span>+i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        producer.close();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>kafka producer发送消息的主方法是send方法，在底层完全地实现了异步化发送，并且通过Java提供的Future同时实现了同步发送和异步发送+回调两种发送方式。而上述代码使用的是第三种方式，即发送之后便不再理会发送结果，这种方式在实际中是不被推荐使用的。</p>
<p><strong>如果发送时连接不上，需要修改kafka配置文件重启</strong></p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">listeners</span>=PLAINTEXT : 外网ip</span><br><span class="line"><span class="attr">advertised.listeners</span>=PLAINTEXT : 外网ip</span><br></pre></td></tr></table></figure>

<p>异步发送 ：</p>
<p>实际上所有的写入操作默认都是异步的，send方法会返回一个Java Future对象供用户稍后获取发送结果，这就是所谓的回调机制。具体代码如下：</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">producer.send(<span class="type">record</span>,<span class="built_in">new</span> Callback()&#123;</span><br><span class="line">  @Override</span><br><span class="line">  <span class="built_in">public</span> <span class="type">void</span> onCompletion(RecordMetadata metadata,<span class="keyword">Exception</span> <span class="keyword">exception</span>)&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">exception</span> == <span class="keyword">null</span>)&#123;</span><br><span class="line">      <span class="keyword">System</span>.<span class="keyword">out</span>.println(&quot;消息发送成功&quot;);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">System</span>.<span class="keyword">out</span>.println(&quot;消息发送失败&quot;);</span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;);  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>上面的代码中，Callback就是发送消息后的回调类，其onCompletion方法的两个输入参数metadata和exception不会同时非空，当消息发送成功时，exception为null，当消息发送失败时，metadata就是null。Callback实际上是一个Java接口，因此可创建自定义的Callback实现类来处理消息发送后的逻辑。</p>
<p>同步发送</p>
<p>同步发送和异步发送其实就是通过Java的Future来区分的，调用Future.get()等待返回结果，即是同步发送，具体代码如下：</p>
<figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; record = <span class="keyword">new</span> <span class="type">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;my-replicated-topic&quot;</span>,<span class="string">&quot;key&quot;</span>+i,<span class="string">&quot;value&quot;</span>+i);</span><br><span class="line"> RecordMetadata recordMetadata = producer.send(record).<span class="keyword">get</span>();</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>使用Future.get()方法会一直等待下去直到broker将结果返回给producer程序，当结果返回时，get()方法要么返回发送结果，要么抛出异常交由producer自行处理。如果没有错误，get将返回对应的RecordMetadata实例。</p>
<h3 id="消息分区机制"><a href="#消息分区机制" class="headerlink" title="消息分区机制"></a>消息分区机制</h3><p>producer发送过程中需要确定将消息发送到topic的哪一个分区，默认的分区器会尽力确保具有相同key的所有消息都被发送到相同的分区上；若没有指定key，会以轮询的方式来确保消息在topic的分区上均匀分配。</p>
<h3 id="kafka的发送异常"><a href="#kafka的发送异常" class="headerlink" title="kafka的发送异常"></a>kafka的发送异常</h3><p>当前kafka的错误类型包含了两类：可重试异常和不可重试异常，常见的可重试异常如下：</p>
<ul>
<li>LeaderNotAvailableException：分区对应的leader不可用，通常出现在leader换届选举时，因此是瞬时的异常，重试之后可自行恢复。</li>
<li>NotControllerException：表明controller当前不可用，在经历新一轮的选举，重试之后可自行恢复。</li>
<li>NetworkException：网络瞬时异常，可重试。</li>
</ul>
<p>所有可重试的异常都继承自or.apache.kafka.common.errors.RetriableException，对于这些可重试的异常，如果在producer程序中配置了重试次数，那么只要在规定的重试次数内自行恢复了，便不会出现在onCompletion的exception中。若超过了重试次数仍没成功，就会被封装到exception中，此时就需要producer程序自行处理这种异常。</p>
<p>没有继承自RetriableException的其他异常都属于不可重试异常，这类异常表明了一些非常严重或kafka无法处理的问题。</p>
<h3 id="自定义分区机制"><a href="#自定义分区机制" class="headerlink" title="自定义分区机制"></a>自定义分区机制</h3><p>自定义分区器需要实现org.apache.kafka.clients.producer.Partitioner接口，分区逻辑写在partition()方法中，例如：</p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AuditPartitioner</span> implements Partitioner &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Random <span class="built_in">random</span>;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span>(Map&lt;<span class="built_in">String</span>, ?&gt; <span class="built_in">map</span>) &#123;</span><br><span class="line">        <span class="comment">// 该方法实现必要资源的初始化工作</span></span><br><span class="line">        <span class="built_in">random</span> = <span class="keyword">new </span><span class="class title_">Random</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">partition</span>(<span class="built_in">String</span> topic, <span class="built_in">Object</span> keyObj, <span class="type">byte</span>[] keyBytes, <span class="built_in">Object</span> value, <span class="type">byte</span>[] valueBytes, Cluster cluster) &#123;</span><br><span class="line">        <span class="built_in">String</span> <span class="built_in">key</span> = (<span class="built_in">String</span>)keyObj;</span><br><span class="line">        <span class="comment">// 获取该topic可用的所有分区</span></span><br><span class="line">        List&lt;PartitionInfo&gt; partitionInfoList = cluster.<span class="property">availablePartitionsForTopic</span>(topic);</span><br><span class="line">        <span class="type">int</span> partitionCount = partitionInfoList.<span class="property">size</span>();</span><br><span class="line">        <span class="type">int</span> auditPartition = partitionCount <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">key</span> == <span class="literal">null</span> || !<span class="built_in">key</span>.<span class="property">contains</span>(<span class="string">&quot;audit&quot;</span>) ? <span class="built_in">random</span>.<span class="property">nextInt</span>(auditPartition) : auditPartition;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span>() &#123;</span><br><span class="line">         <span class="comment">// 该方法实现必要资源的清理</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>使用自定义分区器：<code>props.put(&quot;partitioner.class&quot;,&quot;xx.xx.AuditPartitioner&quot;); </code></p>
<h3 id="自定义序列化器"><a href="#自定义序列化器" class="headerlink" title="自定义序列化器"></a>自定义序列化器</h3><p>自定义序列化器需要实现org.apache.kafka.common.serialization.Serializer接口，在serializer方法中实现序列化逻辑，例如：</p>
<p>首先定义一个POJO对象</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">User</span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">String</span> firstName;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">String</span> lastName;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">int</span> age;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">String</span> address;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">(<span class="type">String</span> firstName,<span class="type">String</span> lastName,<span class="type">int</span> age,<span class="type">String</span> address)</span></span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.firstName=firstName;</span><br><span class="line">  <span class="keyword">this</span>.lastName=lastName;</span><br><span class="line">  <span class="keyword">this</span>.age=age;</span><br><span class="line">  <span class="keyword">this</span>.address=address;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>由于要用jackson-mapper-asl包中的ObjectMapper来将对象转成字节数组，因此需要将其依赖引入：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.codehaus.jackson<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jackson-mapper-asl<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.13<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span>  </span><br></pre></td></tr></table></figure>

<p>接下来创建serializer</p>
<figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">UserSerializer</span> <span class="keyword">implements</span> <span class="title class_">Serializer</span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="title class_">ObjectMapper</span> objectMapper;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="built_in">void</span> <span class="title function_">configure</span>(<span class="params"><span class="built_in">Map</span> config,<span class="built_in">boolean</span> isKey</span>)&#123;</span><br><span class="line">      objectMapper = <span class="keyword">new</span> <span class="title class_">ObjectMapper</span>();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> byte[] <span class="title function_">serialize</span>(<span class="params"><span class="built_in">String</span> topic,<span class="built_in">Object</span> data</span>)&#123;</span><br><span class="line">    byte[] res = <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">try</span>&#123;</span><br><span class="line">      res = objectMapper.<span class="title function_">writeValueAsString</span>(data).<span class="title function_">getBytes</span>(<span class="string">&quot;utf-8&quot;</span>);</span><br><span class="line">    &#125;<span class="keyword">catch</span>(<span class="title class_">Exception</span> e)&#123;</span><br><span class="line">      logger.<span class="title function_">warn</span>(<span class="string">&quot;failed to serialize the object: &#123;&#125;&quot;</span>, data, e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="keyword">public</span> <span class="built_in">void</span> <span class="title function_">close</span>(<span class="params"></span>)&#123;&#125;</span><br><span class="line">&#125;   </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>使用自定义的序列化器 <code>props.put(&quot;value.serializer&quot;,&quot;xx.xx.UserSerializer&quot;); </code></p>
<h3 id="producer拦截器"><a href="#producer拦截器" class="headerlink" title="producer拦截器"></a>producer<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=%E6%8B%A6%E6%88%AA%E5%99%A8&spm=1001.2101.3001.7020">拦截器</a></h3><p>producer拦截器使得用户在消息发送前和producer回调逻辑执行前可对消息做一些定制化处理，允许使用多个拦截器构成拦截器链。拦截器的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor。interceptor可能运行在多个线程中，因此在具体实现时用户需要确保线程安全。下面以一个简单的双interceptor组成的拦截链为例。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部，第二个interceptor会在消息发送后更新成功发送消息或失败发送消息数。</p>
<figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TimeStampPrependerInterceptor</span> <span class="keyword">implements</span> <span class="title class_">ProducerInterceptor</span>&lt;<span class="title class_">String</span>,<span class="title class_">String</span>&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * producer确保在消息被序列化前调用该方法</span></span><br><span class="line"><span class="comment">     * 可以在该方法中对消息做任何操作</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title class_">ProducerRecord</span> <span class="title function_">onSend</span>(<span class="params">ProducerRecord record</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>(record.<span class="title function_">topic</span>(),record.<span class="title function_">partition</span>(),record.<span class="title function_">timestamp</span>(),record.<span class="title function_">key</span>(),<span class="title class_">System</span>.<span class="title function_">currentTimeMillis</span>() +<span class="string">&quot;,&quot;</span>+record.<span class="title function_">value</span>().<span class="title function_">toString</span>());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 该方法会在消息被应答之前或消息发送时调用</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="built_in">void</span> <span class="title function_">onAcknowledgement</span>(<span class="params">RecordMetadata recordMetadata, Exception e</span>) &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="built_in">void</span> <span class="title function_">close</span>(<span class="params"></span>) &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="built_in">void</span> <span class="title function_">configure</span>(<span class="params"><span class="built_in">Map</span>&lt;<span class="built_in">String</span>, ?&gt; map</span>) &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CounterInterceptor</span> <span class="keyword">implements</span> <span class="title class_">ProducerInterceptor</span>&lt;<span class="title class_">String</span>,<span class="title class_">String</span>&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> int errorCounter = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> int successCounter = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title class_">ProducerRecord</span>&lt;<span class="title class_">String</span>, <span class="title class_">String</span>&gt; <span class="title function_">onSend</span>(<span class="params">ProducerRecord&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; record</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> record;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="built_in">void</span> <span class="title function_">onAcknowledgement</span>(<span class="params">RecordMetadata recordMetadata, Exception e</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (e == <span class="literal">null</span>) &#123;</span><br><span class="line">            successCounter++;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            errorCounter++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="built_in">void</span> <span class="title function_">close</span>(<span class="params"></span>) &#123;</span><br><span class="line">        <span class="title class_">System</span>.<span class="property">out</span>.<span class="title function_">println</span>(<span class="string">&quot;Successful sent: &quot;</span> + successCounter);</span><br><span class="line">        <span class="title class_">System</span>.<span class="property">out</span>.<span class="title function_">println</span>(<span class="string">&quot;Failed sent: &quot;</span> + errorCounter);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="built_in">void</span> <span class="title function_">configure</span>(<span class="params"><span class="built_in">Map</span>&lt;<span class="built_in">String</span>, ?&gt; map</span>) &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>使用自定义interceptor：</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; interceptors = new ArrayList&lt;&gt;();</span><br><span class="line">interceptors.<span class="built_in">add</span>(<span class="string">&quot;xx.xx.TimeStampPrependerInterceptor&quot;</span>);</span><br><span class="line">interceptors.<span class="built_in">add</span>(<span class="string">&quot;xx.xx.CounterInterceptor&quot;</span>);</span><br><span class="line">props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,interceptors);</span><br></pre></td></tr></table></figure>

<h3 id="消息的可靠发送"><a href="#消息的可靠发送" class="headerlink" title="消息的可靠发送"></a>消息的可靠发送</h3><p>Java版本的producer采用异步发送机制，send方法将消息放入缓冲区，由一个专属I&#x2F;O线程负责从缓冲区中提取消息并封装进消息batch中，然后发送出去。这个过程存在着数据丢失的窗口，即若I&#x2F;O线程发送之前producer崩溃，则存储缓冲区中的消息会全部丢失。producer的另一个问题就是消息的乱序，假设现发送record1和record2两条消息，由于某些原因导致record1未发送成功，同时kafka又配置了重试机制，那么producer重试record1成功后，record1在日志中的位置可能反而位于record2之后。</p>
<h3 id="无消息丢失配置"><a href="#无消息丢失配置" class="headerlink" title="无消息丢失配置"></a>无消息丢失配置</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 该配置控制 KafkaProducer.send() 和 KafkaProducer.partitionsFor() 将阻塞多长时间。此外这些方法被阻止，也可能是因为缓冲区已满或元数据不可用。在用户提供的序列化程序或分区器中的锁定不会计入此超时。默认为60000ms。</span></span><br><span class="line">max<span class="selector-class">.block</span>.ms=<span class="number">60000</span></span><br><span class="line"></span><br><span class="line">acks=<span class="attribute">all</span></span><br><span class="line">retries=Integer<span class="selector-class">.MAX_VALUE</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 该参数设置为1使producer在某个broker发送响应之前将无法再给broker发送请求，可防止topic同分区下的消息乱序问题，</span></span><br><span class="line">max<span class="selector-class">.in</span><span class="selector-class">.flight</span><span class="selector-class">.requests</span><span class="selector-class">.per</span>.connection=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置不允许非ISR中的副本被选举为leader，从而避免broker端因日志问题造成消息的丢失</span></span><br><span class="line">unclean<span class="selector-class">.leader</span>.electionenable=false</span><br><span class="line"></span><br><span class="line">replication.factor=<span class="number">3</span></span><br><span class="line"><span class="comment">// 用于控制某条消息至少被写入到ISR中的多少个副本才算成功</span></span><br><span class="line">min<span class="selector-class">.insync</span>.replicas=<span class="number">2</span></span><br><span class="line"></span><br><span class="line">enable<span class="selector-class">.auto</span>.commit=false</span><br><span class="line"></span><br><span class="line">使用带回调机制的send发送消息</span><br><span class="line">Callback逻辑中显式立即关闭producer</span><br></pre></td></tr></table></figure>

<h2 id="消费数据"><a href="#消费数据" class="headerlink" title="消费数据"></a>消费数据</h2><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.sh <span class="string">\</span></span><br><span class="line">--bootstrap-server bigdata3:<span class="number">9092</span>,bigdata4:<span class="number">9092</span>,bigdata5:<span class="number">9092</span> <span class="string">\</span></span><br><span class="line">--topic dl2262 <span class="string">\</span></span><br><span class="line">--<span class="keyword">from</span>-beginning </span><br></pre></td></tr></table></figure>

<p>–from.beginning :开始消费的位置</p>
<p>问题：消费乱序 ：数据产生的问题和消费顺序不一致</p>
<p>如何保证kafka消费全局有序</p>
<ul>
<li>多分区：不可能<ul>
<li>可以解决，spark ， flink 计算的结果是不对的</li>
<li>source ：mysql  id &#x3D; 1 进行的操作 ： insert delete update</li>
<li>然后把mysql里的binlog采集到kafka -》 flink&#x2F;spark -》hbase&#x2F;phonenix</li>
<li>因为消费乱序，可能会出现问题</li>
<li>思路：<ul>
<li>单分区 ：会影响spark或者flink的吞吐量 原因 ： 原来3个分区，三个并行度 ，现在一个，就慢了</li>
<li>多分区 ：可以利用单分区有序解决全局有序</li>
</ul>
</li>
</ul>
</li>
<li>单分区：可以，单分区数据是有序的</li>
</ul>
<p>消费者消费kafka的数据是以消费者组的方式进行消费的</p>
<p>消费者组 ：</p>
<ul>
<li>一个组内共享一个消费者组的id</li>
<li>组内的所有消费者协调在一起去消费指定的topic的分区数据</li>
<li>每个分区只能由一个消费者组的一个消费者消费</li>
<li>不能由一个消费者组的多个消费者进行重复消费</li>
<li>生产上<ul>
<li>一般是一个消费者，在一个消费者组里</li>
<li>就是可以消费全部分区数据</li>
<li>上述是小公司和一般情况的大公司</li>
<li>大公司 ：<ul>
<li>会用到消费者组 不同消费者处理不同分区数据</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>java api custome</p>
<figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> kafkacustome;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">customertestautooffset</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> void main(<span class="keyword">String</span>[] args) throws IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//1 新建一个consumer对象</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> <span class="type">Properties</span>();</span><br><span class="line">        properties.load(customertestautooffset.class.getClassLoader().getResourceAsStream(<span class="string">&quot;consumer1.properties&quot;</span>));</span><br><span class="line">        KafkaConsumer&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; consumer = <span class="keyword">new</span> <span class="type">KafkaConsumer</span>&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2 用这个对象接收消息</span></span><br><span class="line">        <span class="comment">//发布订阅模式接收消息，先订阅</span></span><br><span class="line">        consumer.subscribe(Collections.singleton(<span class="string">&quot;dl2262&quot;</span>));</span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">            <span class="comment">//从订阅的话题中拉取数据</span></span><br><span class="line">            ConsumerRecords&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; poll = consumer.poll(<span class="number">2000</span>);</span><br><span class="line">            <span class="keyword">if</span>(poll.count() == <span class="number">0</span>)&#123;</span><br><span class="line">                Thread.sleep(<span class="number">100</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//消费拉取到的数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; record : <span class="type">poll</span>) &#123;</span><br><span class="line">                System.out.println(record);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3 关闭资源</span></span><br><span class="line">        <span class="comment">//consumer.close();</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>其中“dl2262”是名为的topic的名字，其中groupid一样的就会分配到一起</p>
<p>其中 ： consumer1.properties内容是</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">key.deserializer</span>=org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line"><span class="attr">value.deserializer</span>=org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line"><span class="attr">bootstrap.servers</span>=bigdata3:<span class="number">9092</span>,bigdata4:<span class="number">9092</span>,bigdata5:<span class="number">9092</span> </span><br><span class="line"><span class="attr">enable.auto.commit</span>=<span class="literal">true</span></span><br><span class="line"><span class="attr">group.id</span>=test1</span><br><span class="line"><span class="attr">auto.offset.reset</span>=earliest</span><br></pre></td></tr></table></figure>

<p>上述是自动offset的</p>
<p>然后接下来是我们自己提交offset的</p>
<p>手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；不同点是，commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。</p>
<p>修改配置文件中的enable.auto.commit&#x3D;false选项并在消费拉取到的数据之后添加consumer.commitSync();语句即可。由于同步提交offset有失败重试机制，故更加可靠</p>
<figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">//消费拉取到的数据</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; <span class="keyword">record</span> : <span class="type">poll</span>) &#123;</span><br><span class="line">            System.<span class="keyword">out</span>.println(<span class="keyword">record</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        consumer.commitSync();</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会收到很大的影响。在更多的情况下，会选用异步提交offset的方式。</p>
<h2 id="kafka数据存储"><a href="#kafka数据存储" class="headerlink" title="kafka数据存储"></a>kafka数据存储</h2><p>partition：下面存储就是数据，一段一段相同大小的Segment文件</p>
<p>Segment ：逻辑概念</p>
<ul>
<li>它由log 和index文件组成<ul>
<li>log :实实在在的数据 ，包括元数据 默认大小 1G是由 <code>log.segment.bytes控制单位是字节在配置文件里</code></li>
<li>index：是我们的位置信息</li>
</ul>
</li>
<li>segment的命名规则<ul>
<li>offset.index和offset.log</li>
<li>offset : 偏移量 -》数据event的标号，在topic下的编号</li>
<li>比如 ： <code>00000000000000000000.index 00000000000000000000.log</code></li>
<li>然后如果达到后面最大值，则按照临界的offset来延续比如 <code>0000000000000003333333.index</code>等</li>
</ul>
</li>
</ul>
<p>借助脚本命令查看.log文件</p>
<ul>
<li>因为kafka对数据的存储是按照他自己的存储方式来的，所有我们要查看.log文件的话要用其提供的脚本文件</li>
</ul>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kafka-run-<span class="keyword">class</span>.sh \</span><br><span class="line">kafka.tools.DumpLogSegments \</span><br><span class="line">--files <span class="regexp">/home/</span>hadoop<span class="regexp">/data/</span>kafka<span class="regexp">/dl2262-0/</span><span class="number">00000000000000000000</span>.log \</span><br><span class="line">--<span class="keyword">print</span>-data-log \</span><br><span class="line">&gt; <span class="number">0</span>.log</span><br></pre></td></tr></table></figure>

<p>上述就是把其文件进行翻译，并存储在0.log中</p>
<p>在index文件中其中是offset：物理地址的形式</p>
<p>它是以稀疏表的方式进行维护的并不是每一条的都维护</p>
<h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><p>如何查找offset为11865的数据？简述过程</p>
<p>解答 ：</p>
<ul>
<li>通过二分查找算法 ， 查找offset为11865的最大Segment的文件组</li>
<li>去临近的最大的的index文件里再再用二分查找，找到对应的offset</li>
<li>如何看见其对应的partition：就是其物理地址</li>
<li>根据物理地址去迅速定位位置，按顺序查找，一直找到它</li>
</ul>
<h2 id="交付语义"><a href="#交付语义" class="headerlink" title="交付语义"></a>交付语义</h2><p>消息交付语义 （生产者消费者都有）：</p>
<ul>
<li><p>at most once : 消息可能会丢，不会重复</p>
</li>
<li><p>at least once ：不会丢失，但是可能会重复 ，没有事务，交付和记录是分开的，如果交付完成，再记录的时候挂了，下次启动就会继续从没有记录的位置进行交付，数据会重复</p>
</li>
<li><p>exactly once ： 每个数据仅仅交付一次,增加了一个事务的情况，只有交付和记录都完成才可以继续往下</p>
</li>
<li><p>producer ：</p>
<ul>
<li>发送数据的时候也有交付语义的</li>
<li>kafka 0.11.0 之前至少一次 会导致数据重复 ：at least once</li>
<li>但是 0.11.0之后，包括0.11.0 ，会变成精准交付 : exactly once</li>
</ul>
</li>
<li><p>kafka版本，大于等于0.11.0版本</p>
</li>
<li><p>customer：</p>
<ul>
<li>问题 ：如何存储上次消费到哪一个offset<ul>
<li>取决于，你的消费者组件，看他们支持什么消费者对应的交付语义<ul>
<li>sparkStreaming ： 如上的三种<ul>
<li>at most once</li>
<li>at least once</li>
<li>exactly once</li>
<li>所以我们可以选择至少一次，精准一次，但是至少一次可能由重复问题</li>
<li>但是90%sparkingStream和struedstreaming用至少一次，因为精准一次操作起来比较复杂</li>
</ul>
</li>
<li>flink同理</li>
<li>其他再议</li>
</ul>
</li>
<li>offset信息如何维护？<ul>
<li>先了解</li>
<li>CheckPoints ：很简单，但是弊端也很多 ：生产上不能用 ，因为问题多<ul>
<li>首先会有小文件问题 ，代码变更整个spark项目就不能用了，就是之前记录的chackpoint信息就失效了</li>
</ul>
</li>
<li>kafka itself ：比较推荐 ，简单不用自己写，人家给你生成好的 ，就是最少一次的</li>
<li>use your own data store ： 自己开发 ，用外部的数据源存储offset ，redis ，mysql ，hbase，建议使用hbase是最好的，但是另外两个也可以用的，但是有要求，每分钟的请求次数，mysql可能扛不住，redis可以 ，hbase可以 ， 不推荐用 redis<ul>
<li>可以达到精准一次的语义</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>总的来说工作中用至少一次和精准一次用的很多，flink用的精准一次比较多,其他的至少一次就比较多</li>
</ul>
</li>
</ul>
<h2 id="副本和数据同步"><a href="#副本和数据同步" class="headerlink" title="副本和数据同步"></a>副本和数据同步</h2><p>例子 ：8个分区 1个副本 8节点</p>
<p>kafka ：</p>
<ul>
<li>log 数据 ：topic 三份数据副本<ul>
<li>hdfs 100多个节点 3副本ok</li>
<li>但是kafka才8个节点，3副本大概会崩坏</li>
<li>所以一般是一天或者三天</li>
<li>每个kafka集群都有leader和follower</li>
</ul>
</li>
<li>leader 和 follower<ul>
<li>leader ： 负责对外读写的节点</li>
<li>follower ： 拉取分区上对应的数据 ，进行备份的</li>
</ul>
</li>
<li>机制ack：<ul>
<li>消息发送确认机制</li>
<li>针对produce的</li>
<li>ack 的值有 all 1,0,-1</li>
<li>1<ul>
<li>只要一个分区副本成功写入通知就认为生产者推送消息成功了</li>
</ul>
</li>
<li>0<ul>
<li>完全就不管发送的结果</li>
</ul>
</li>
<li>-1<ul>
<li>其是收到所有分区副本写入成功的通知才认为成功</li>
</ul>
</li>
<li>all<ul>
<li>和-1一样</li>
</ul>
</li>
</ul>
</li>
<li>工作中的选择</li>
<li>大部分时间选择all ，-1</li>
<li>其次是1</li>
<li>最后是0</li>
<li>速度关系0是最快的，1是中间的，-1是最慢的当是多个节点的时候，如果是单个leader和follower的时候1和-1是一样的</li>
</ul>
<p>kafka监控</p>
<ul>
<li>kafka manager首选  二开kafkamanager 可以自己加上 官网 <code>https://github.com/yahoo/CMAK</code></li>
<li>kafka eagle -》 也好用 不过相对上面的那个，就是lower点 ，部署简单好用 官网 <code>https://github.com/smartloli/EFAK</code></li>
</ul>
<h2 id="kafka测试"><a href="#kafka测试" class="headerlink" title="kafka测试"></a>kafka测试</h2><p>用Kafka官方自带的脚本，对Kafka进行压测。Kafka压测时，可以查看到哪个地方出现了瓶颈（CPU，内存，网络IO）。一般都是网络IO达到瓶颈。</p>
<p>kafka-consumer-<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=perf&spm=1001.2101.3001.7020">perf</a>-test.sh</p>
<p>kafka-producer-perf-test.sh</p>
<h3 id="Kafka-Producer压力测试"><a href="#Kafka-Producer压力测试" class="headerlink" title="Kafka Producer压力测试"></a>Kafka Producer压力测试</h3><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-producer-perf-test.sh  \</span><br><span class="line">--topic test \</span><br><span class="line">--record-size <span class="number">100</span> \</span><br><span class="line">--num-records <span class="number">100000</span> \</span><br><span class="line">--throughput -<span class="number">1</span> \</span><br><span class="line">--producer-props bootstrap.<span class="attr">servers=</span>bigdata3:<span class="number">9092</span>,bigdata4:<span class="number">9092</span>,bigdata5:<span class="number">9092</span></span><br><span class="line"></span><br><span class="line">record-size是一条信息有多大，单位是字节。</span><br><span class="line">num-records是总共发送多少条信息。</span><br><span class="line">throughput 是每秒多少条信息，设成-<span class="number">1</span>，表示不限流，可测出生产者最大吞吐量。</span><br><span class="line"></span><br><span class="line">他会打印下面的语句</span><br><span class="line"><span class="number">100000</span> records sent, <span class="number">149253.731343</span> records/sec (<span class="number">14.23</span> MB/sec), <span class="number">112.02</span> <span class="keyword">ms</span> <span class="title">avg</span> latency, <span class="number">207.00</span> <span class="keyword">ms</span> <span class="title">max</span> latency, <span class="number">97</span> <span class="keyword">ms</span> <span class="title">50th</span>, <span class="number">190</span> <span class="keyword">ms</span> <span class="title">95th</span>, <span class="number">206</span> <span class="keyword">ms</span> <span class="title">99th</span>, <span class="number">207</span> <span class="keyword">ms</span> <span class="title">99</span>.<span class="number">9</span>th.</span><br><span class="line">参数解析：本例中一共写入<span class="number">10</span>w条消息，吞吐量为<span class="number">14.23</span> MB/sec，每次写入的平均延迟为<span class="number">112.02</span>毫秒，最大的延迟为<span class="number">207</span>毫秒</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Kafka-Consumer压力测试"><a href="#Kafka-Consumer压力测试" class="headerlink" title="Kafka Consumer压力测试"></a>Kafka Consumer压力测试</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">kafka-consumer-perf-test.sh </span><br><span class="line"><span class="comment">--broker-list bigdata3:9092,bigdata4:9092,bigdata35:9092 </span></span><br><span class="line"><span class="comment">--topic test </span></span><br><span class="line"><span class="comment">--fetch-size 10000 </span></span><br><span class="line"><span class="comment">--messages 10000000 </span></span><br><span class="line"><span class="comment">--threads 1</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">--zookeeper 指定zookeeper的链接信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--topic 指定topic的名称</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--fetch-size 指定每次fetch的数据的大小</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--messages 总共要消费的消息个数</span></span><br><span class="line">测试结果说明：</span><br><span class="line"></span><br><span class="line"><span class="built_in">start</span>.<span class="built_in">time</span>, <span class="keyword">end</span>.<span class="built_in">time</span>, data.consumed.<span class="keyword">in</span>.MB, MB.<span class="built_in">sec</span>, data.consumed.<span class="keyword">in</span>.nMsg, nMsg.<span class="built_in">sec</span></span><br><span class="line"><span class="number">2020</span><span class="number">-03</span><span class="number">-10</span> <span class="number">03</span>:<span class="number">18</span>:<span class="number">51</span>:<span class="number">773</span>, <span class="number">2020</span><span class="number">-03</span><span class="number">-10</span> <span class="number">03</span>:<span class="number">18</span>:<span class="number">53</span>:<span class="number">815</span>, <span class="number">19.0735</span>, <span class="number">9.3406</span>, <span class="number">200000</span>, <span class="number">97943.192</span></span><br></pre></td></tr></table></figure>

<h2 id="Kafka-集群数据迁移"><a href="#Kafka-集群数据迁移" class="headerlink" title="Kafka 集群数据迁移"></a>Kafka 集群数据迁移</h2><h3 id="同集群迁移"><a href="#同集群迁移" class="headerlink" title="同集群迁移"></a>同集群迁移</h3><p>同集群之间数据迁移，比如在已有的集群中新增了一个Broker节点，此时需要将原来集群中已有的Topic的数据迁移部分到新的集群中，缓解集群压力。</p>
<p>将新的节点添加到Kafka集群很简单，只需为它们分配一个唯一的Broker ID，并在新<a target="_blank" rel="noopener" href="https://cloud.tencent.com/product/cvm?from=10680">服务器</a>上启动Kafka。但是，这些新服务器节点不会自动分配任何数据分区，因此除非将分区移动到新增的节点，否则在创建新Topic之前新节点不会执行任何操作。因此，通常在将新服务器节点添加到Kafka集群时，需要将一些现有数据迁移到这些新的节点。</p>
<p>迁移数据的过程是手动启动的，执行过程是完全自动化的。在Kafka后台服务中，Kafka将添加新服务器作为其正在迁移的分区的Follower，并允许新增节点完全复制该分区中的现有数据。当新服务器节点完全复制此分区的内容并加入同步副本（ISR）时，其中一个现有副本将删除其分区的数据。</p>
<p>Kafka系统提供了一个分区重新分配工具（kafka-reassign-partitions.sh），该工具可用于在Broker之间迁移分区。理想情况下，将确保所有Broker的数据和分区均匀分配。分区重新分配工具无法自动分析Kafka群集中的数据分布并迁移分区以实现均匀的<a target="_blank" rel="noopener" href="https://cloud.tencent.com/product/clb?from=10680">负载均衡</a>。因此，管理员在操作的时候，必须弄清楚应该迁移哪些Topic或分区。</p>
<p>分区重新分配工具可以在3种互斥模式下运行：</p>
<ul>
<li>–generate：在此模式下，给定Topic列表和Broker列表，该工具会生成候选重新分配，以将指定Topic的所有分区迁移到新Broker中。此选项仅提供了一种方便的方法，可在给定Topic和目标Broker列表的情况下生成分区重新分配计划。</li>
<li>–execute：在此模式下，该工具将根据用户提供的重新分配计划启动分区的重新分配。 （使用–reassignment-json-file选项）。由管理员手动制定自定义重新分配计划，也可以使用–generate选项提供。</li>
<li>–verify：在此模式下，该工具将验证最后一次–execute期间列出的所有分区的重新分配状态。状态可以有成功、失败或正在进行等状态。</li>
</ul>
<h3 id="迁移过程实现"><a href="#迁移过程实现" class="headerlink" title="迁移过程实现"></a>迁移过程实现</h3><p>分区重新分配工具可用于将一些Topic从当前的Broker节点中迁移到新添加的Broker中。这在扩展现有集群时通常很有用，因为将整个Topic移动到新的Broker变得更容易，而不是一次移动一个分区。当执行此操作时，用户需要提供已有的Broker节点的Topic列表，以及到新节点的Broker列表（源Broker到新Broker的映射关系）。然后，该工具在新的Broker中均匀分配给指定Topic列表的所有分区。在迁移过程中，Topic的复制因子保持不变。</p>
<p>现有如下实例，将Topic为ke01，ke02的所有分区从Broker1中移动到新增的Broker2和Broker3中。由于该工具接受Topic的输入列表作为JSON文件，因此需要明确迁移的Topic并创建json文件，如下所示：</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; cat topic-<span class="keyword">to</span>-<span class="keyword">move</span>.json</span><br><span class="line">&#123;&quot;topics&quot;: [&#123;&quot;topic&quot;: &quot;ke01&quot;&#125;,</span><br><span class="line">            &#123;&quot;topic&quot;: &quot;ke02&quot;&#125;],</span><br><span class="line">&quot;version&quot;:<span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>准备好JSON文件，然后使用分区重新分配工具生成候选分配，命令如下：</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-<span class="keyword">reassign</span>-partitions.sh <span class="comment">--zookeeper bigdata3:2181,bigdata4:2181,bigdata5:2181/kafka  --topics-to-move-json-file topics-to-move.json --broker-list &quot;1,2&quot; --generate</span></span><br></pre></td></tr></table></figure>

<p>该工具生成一个候选分配，将所有分区从Topic ke01，ke02移动到Broker1和Broker2。需求注意的是，此时分区移动尚未开始，它只是告诉你当前的分配和建议。保存当前分配，以防你想要回滚它。新的赋值应保存在JSON文件（例如expand-cluster-reassignment.json）中，以使用–execute选项执行。JSON文件如下：</p>
<figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&quot;version&quot;</span>:<span class="number">1</span>,<span class="string">&quot;partitions&quot;</span>:[&#123;<span class="string">&quot;topic&quot;</span>:<span class="string">&quot;ke02&quot;</span>,<span class="string">&quot;partition&quot;</span>:<span class="number">0</span>,<span class="string">&quot;replicas&quot;</span>:[<span class="number">2</span>]&#125;,&#123;<span class="string">&quot;topic&quot;</span>:<span class="string">&quot;ke02&quot;</span>,<span class="string">&quot;partition&quot;</span>:<span class="number">1</span>,<span class="string">&quot;replicas&quot;</span>:[<span class="number">1</span>]&#125;,&#123;<span class="string">&quot;topic&quot;</span>:<span class="string">&quot;ke02&quot;</span>,<span class="string">&quot;partition&quot;</span>:<span class="number">2</span>,<span class="string">&quot;replicas&quot;</span>:[<span class="number">2</span>]&#125;,&#123;<span class="string">&quot;topic&quot;</span>:<span class="string">&quot;ke01&quot;</span>,<span class="string">&quot;partition&quot;</span>:<span class="number">0</span>,<span class="string">&quot;replicas&quot;</span>:[<span class="number">2</span>]&#125;,&#123;<span class="string">&quot;topic&quot;</span>:<span class="string">&quot;ke01&quot;</span>,<span class="string">&quot;partition&quot;</span>:<span class="number">1</span>,<span class="string">&quot;replicas&quot;</span>:[<span class="number">1</span>]&#125;,&#123;<span class="string">&quot;topic&quot;</span>:<span class="string">&quot;ke01&quot;</span>,<span class="string">&quot;partition&quot;</span>:<span class="number">2</span>,<span class="string">&quot;replicas&quot;</span>:[<span class="number">2</span>]&#125;]&#125;</span><br></pre></td></tr></table></figure>

<p>命令</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; ./kafka-<span class="keyword">reassign</span>-partitions.sh <span class="comment">--zookeeper bigdata3:2181,bigdata4:2181,bigdata5:2181/kafka --reassignment-json-file expand-cluster-reassignment.json --execute</span></span><br></pre></td></tr></table></figure>

<p>最后，–verify选项可与该工具一起使用，以检查分区重新分配的状态。需要注意的是，相同的expand-cluster-reassignment.json（与–execute选项一起使用）应与–verify选项一起使用，执行命令如下：</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; ./kafka-<span class="keyword">reassign</span>-partitions.sh <span class="comment">--zookeeper bigdata3:2181,bigdata4:2181,bigdata5:2181/kafka --reassignment-json-file expand-cluster-reassignment.json --verify</span></span><br></pre></td></tr></table></figure>

<p>同时，我们可以通过<a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_63722685?spm=1011.2124.3001.5343">Kafka Eagle</a>工具来查看Topic的分区情况。</p>
<h2 id="跨集群迁移"><a href="#跨集群迁移" class="headerlink" title="跨集群迁移"></a>跨集群迁移</h2><p>这里跨集群迁移，我们指的是在Kafka多个集群之间复制数据“镜像”的过程，以避免与单个集群中的节点之间发生的复制混淆。 Kafka附带了一个用于在Kafka集群之间镜像数据的工具。该工具从源集群使用并生成到目标集群。这种镜像的一个常见用例是在另一个数据中心提供副本。</p>
<p>另外，你可以运行许多此类镜像进程以提高吞吐量和容错（如果一个进程终止，其他进程将占用额外负载）。将从源集群中的Topic读取数据，并将其写入目标集群中具有相同名称的主题。事实上，“镜像”数据只不过是一个Kafka将消费者和生产者联系在了一起。</p>
<p>源集群和目标集群是完全独立的实体，它们可以具有不同数量的分区，并且偏移量将不相同。出于这个原因，镜像集群并不是真正意图作为容错机制（因为消费者的位置会有所不同）;为此，建议使用正常的集群内复制。但是，镜像进程将保留并使用消息Key进行分区，因此可以按Key保留顺序。</p>
<p>下面是一个跨集群的单Topic实例，命令如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; ./kafka-mirror-maker<span class="selector-class">.sh</span> <span class="attr">--consumer</span><span class="selector-class">.config</span> consumer<span class="selector-class">.properties</span> <span class="attr">--producer</span><span class="selector-class">.config</span> producer<span class="selector-class">.properties</span> <span class="attr">--whitelist</span> ke03</span><br></pre></td></tr></table></figure>

<p>需要注意的是，consumer.properties文件配置源Kafka集群Broker地址，producer.properties文件配置目标Kafka集群地址。如果需要迁移多个Topic，可以使用 –whitelist ‘A|B’，如果需要迁移所有的Topic，可以使用 –whitelist ‘*’。</p>
<p>执行跨集群迁移命令后，目标集群中使用Kafka Eagle中查看Topic Size大小看是否与源集群的Topic Size大小相等，或者使用SQL语句，验证是否有数据迁移过来，</p>
<p>跨集群迁移数据的本质是，Kafka启动了消费者读取源集群数据，并将消费后的数据写入到目标集群，在迁移的过程中，可以启动多个实例，提供迁出的吞吐量。</p>

    </div>

    
    
    

      
   <div>
     <div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

   </div>
     
        <div class="reward-container">
  <div>你们的鼓励是对我最大的支持</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="liu zihang 微信支付">
        <p>微信支付</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>liu zihang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://zihang.fun/2022/12/16/12-16/" title="kafka">http://zihang.fun/2022/12/16/12-16/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/images/wechat_channel.jpg">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>

     
    
      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/12/15/%E6%8A%80%E6%9C%AF%E7%82%B9/" rel="prev" title="自我认为的知识点">
      <i class="fa fa-chevron-left"></i> 自我认为的知识点
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/12/16/%E5%91%A8%E5%85%AD%E6%97%A5/" rel="next" title="周六周日要补的">
      周六周日要补的 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81NzQzNy8zMzkwMQ=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#kafka%EF%BC%9A"><span class="nav-number">1.</span> <span class="nav-text">kafka：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8Bkafka%E7%9A%84topic"><span class="nav-number">1.1.</span> <span class="nav-text">查看kafka的topic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BAtopic"><span class="nav-number">1.2.</span> <span class="nav-text">创建topic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E8%AF%A6%E7%BB%86%E7%9A%84topic%E4%BF%A1%E6%81%AF"><span class="nav-number">1.3.</span> <span class="nav-text">查看详细的topic信息</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%A0%E9%99%A4topic"><span class="nav-number">1.4.</span> <span class="nav-text">删除topic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9topic"><span class="nav-number">1.5.</span> <span class="nav-text">修改topic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E4%BA%A7%E6%95%B0%E6%8D%AE"><span class="nav-number">1.6.</span> <span class="nav-text">生产数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B6%88%E6%81%AF%E5%88%86%E5%8C%BA%E6%9C%BA%E5%88%B6"><span class="nav-number">1.6.1.</span> <span class="nav-text">消息分区机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka%E7%9A%84%E5%8F%91%E9%80%81%E5%BC%82%E5%B8%B8"><span class="nav-number">1.6.2.</span> <span class="nav-text">kafka的发送异常</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA%E6%9C%BA%E5%88%B6"><span class="nav-number">1.6.3.</span> <span class="nav-text">自定义分区机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BA%8F%E5%88%97%E5%8C%96%E5%99%A8"><span class="nav-number">1.6.4.</span> <span class="nav-text">自定义序列化器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#producer%E6%8B%A6%E6%88%AA%E5%99%A8"><span class="nav-number">1.6.5.</span> <span class="nav-text">producer拦截器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B6%88%E6%81%AF%E7%9A%84%E5%8F%AF%E9%9D%A0%E5%8F%91%E9%80%81"><span class="nav-number">1.6.6.</span> <span class="nav-text">消息的可靠发送</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A0%E6%B6%88%E6%81%AF%E4%B8%A2%E5%A4%B1%E9%85%8D%E7%BD%AE"><span class="nav-number">1.6.7.</span> <span class="nav-text">无消息丢失配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E6%95%B0%E6%8D%AE"><span class="nav-number">1.7.</span> <span class="nav-text">消费数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kafka%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="nav-number">1.8.</span> <span class="nav-text">kafka数据存储</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9D%A2%E8%AF%95%E9%A2%98"><span class="nav-number">1.9.</span> <span class="nav-text">面试题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%A4%E4%BB%98%E8%AF%AD%E4%B9%89"><span class="nav-number">1.10.</span> <span class="nav-text">交付语义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%AF%E6%9C%AC%E5%92%8C%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5"><span class="nav-number">1.11.</span> <span class="nav-text">副本和数据同步</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kafka%E6%B5%8B%E8%AF%95"><span class="nav-number">1.12.</span> <span class="nav-text">kafka测试</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-Producer%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95"><span class="nav-number">1.12.1.</span> <span class="nav-text">Kafka Producer压力测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-Consumer%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95"><span class="nav-number">1.12.2.</span> <span class="nav-text">Kafka Consumer压力测试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka-%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB"><span class="nav-number">1.13.</span> <span class="nav-text">Kafka 集群数据迁移</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8C%E9%9B%86%E7%BE%A4%E8%BF%81%E7%A7%BB"><span class="nav-number">1.13.1.</span> <span class="nav-text">同集群迁移</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%81%E7%A7%BB%E8%BF%87%E7%A8%8B%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.13.2.</span> <span class="nav-text">迁移过程实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B7%A8%E9%9B%86%E7%BE%A4%E8%BF%81%E7%A7%BB"><span class="nav-number">1.14.</span> <span class="nav-text">跨集群迁移</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="liu zihang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">liu zihang</p>
  <div class="site-description" itemprop="description">只有努力不会辜负你</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      链接网站
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://baidu.com/" title="https:&#x2F;&#x2F;baidu.com" rel="noopener" target="_blank">百度</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://fishc.com.cn/" title="https:&#x2F;&#x2F;fishc.com.cn" rel="noopener" target="_blank">鱼C论坛</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">liu zihang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共133.9k字</span>
</div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'http://zihang.fun/2022/12/16/12-16/',]
      });
      });
  </script>

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
