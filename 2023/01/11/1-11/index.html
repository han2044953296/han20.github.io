<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/%E6%A0%91%E5%8F%B6_sleaves%20(1).png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/%E6%A0%91%E5%8F%B6_sleaves.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-flash.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zihang.fun","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":15,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="构建df rdd hive 外部数据源 json,csv,jdbc&#x2F;odbc    加载外部数据源api简介TEXT   Property Name Default Meaning Scope    wholetext false If true, read each file from input path(s) as a single row. read   lineSep \r,">
<meta property="og:type" content="article">
<meta property="og:title" content="sparksql-2">
<meta property="og:url" content="http://zihang.fun/2023/01/11/1-11/index.html">
<meta property="og:site_name" content="枫叶冢">
<meta property="og:description" content="构建df rdd hive 外部数据源 json,csv,jdbc&#x2F;odbc    加载外部数据源api简介TEXT   Property Name Default Meaning Scope    wholetext false If true, read each file from input path(s) as a single row. read   lineSep \r,">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic.imgdb.cn/item/63be1a9cbe43e0d30e184271.jpg">
<meta property="og:image" content="https://pic.imgdb.cn/item/63be1ad7be43e0d30e18aa99.jpg">
<meta property="og:image" content="https://pic.imgdb.cn/item/63be621fbe43e0d30e93423c.jpg">
<meta property="article:published_time" content="2023-01-11T00:53:32.465Z">
<meta property="article:modified_time" content="2023-01-11T09:07:28.381Z">
<meta property="article:author" content="liu zihang">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.imgdb.cn/item/63be1a9cbe43e0d30e184271.jpg">

<link rel="canonical" href="http://zihang.fun/2023/01/11/1-11/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>sparksql-2 | 枫叶冢</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss2.xml" title="枫叶冢" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
    <a target="_blank" rel="noopener" href="https://github.com/han2044953296" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#fff; color:#151513; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">枫叶冢</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zihang.fun/2023/01/11/1-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="liu zihang">
      <meta itemprop="description" content="只有努力不会辜负你">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="枫叶冢">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          sparksql-2
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-11 08:53:32 / 修改时间：17:07:28" itemprop="dateCreated datePublished" datetime="2023-01-11T08:53:32+08:00">2023-01-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%97%A5%E5%BF%97/" itemprop="url" rel="index"><span itemprop="name">日志</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>38k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>35 分钟</span>
            </span>

        </div>
      </header>

    
    
    

   

    <div class="post-body" itemprop="articleBody">

      
        <h1 id="构建df"><a href="#构建df" class="headerlink" title="构建df"></a>构建df</h1><ul>
<li>rdd</li>
<li>hive</li>
<li>外部数据源<ul>
<li>json,csv,jdbc&#x2F;odbc</li>
</ul>
</li>
</ul>
<h1 id="加载外部数据源"><a href="#加载外部数据源" class="headerlink" title="加载外部数据源"></a>加载外部数据源</h1><h2 id="api简介"><a href="#api简介" class="headerlink" title="api简介"></a>api简介</h2><h3 id="TEXT"><a href="#TEXT" class="headerlink" title="TEXT"></a>TEXT</h3><table>
<thead>
<tr>
<th><strong>Property Name</strong></th>
<th><strong>Default</strong></th>
<th><strong>Meaning</strong></th>
<th><strong>Scope</strong></th>
</tr>
</thead>
<tbody><tr>
<td>wholetext</td>
<td>false</td>
<td>If true, read each file from input path(s) as a single row.</td>
<td>read</td>
</tr>
<tr>
<td>lineSep</td>
<td><code>\r</code>, <code>\r\n</code>, <code>\n</code> (for reading), <code>\n</code> (for writing)</td>
<td>Defines the line separator that should be used for reading or writing.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td>compression</td>
<td>(none)</td>
<td>Compression codec to use when saving to file. <br />This can be one of the known case-insensitive shorten names<br /> (none, bzip2, gzip, lz4, snappy and deflate).</td>
<td>write</td>
</tr>
</tbody></table>
<h3 id="json"><a href="#json" class="headerlink" title="json"></a>json</h3><table>
<thead>
<tr>
<th><strong>Property Name</strong></th>
<th><strong>Default</strong></th>
<th><strong>Meaning</strong></th>
<th><strong>Scope</strong></th>
</tr>
</thead>
<tbody><tr>
<td><code>timeZone</code></td>
<td>(value of <code>spark.sql.session.timeZone</code> configuration)</td>
<td>Sets the string that indicates a time zone ID to be used to format timestamps in the JSON datasources or partition values. The following formats of <code>timeZone</code> are supported:&#96;&#96;* Region-based zone ID: <br />It should have the form ‘area&#x2F;city’,<br /> such as ‘America&#x2F;Los_Angeles’.* Zone offset: It should be in the format ‘(+</td>
<td>-)HH:mm’, for example ‘-08:00’ or ‘+01:00’. Also ‘UTC’ and ‘Z’ are supported as aliases of ‘+00:00’.Other short names like ‘CST’ are not recommended to use because they can be ambiguous.</td>
</tr>
<tr>
<td><code>primitivesAsString</code></td>
<td><code>false</code></td>
<td>Infers all primitive values as a string type.</td>
<td>read</td>
</tr>
<tr>
<td><code>prefersDecimal</code></td>
<td><code>false</code></td>
<td>Infers all floating-point values as a decimal type. <br />If the values do not fit in decimal, then it infers them as doubles.</td>
<td>read</td>
</tr>
<tr>
<td><code>allowComments</code></td>
<td><code>false</code></td>
<td>Ignores Java&#x2F;C++ style comment in JSON records.</td>
<td>read</td>
</tr>
<tr>
<td><code>allowUnquotedFieldNames</code></td>
<td><code>false</code></td>
<td>Allows unquoted JSON field names.</td>
<td>read</td>
</tr>
<tr>
<td><code>allowSingleQuotes</code></td>
<td><code>true</code></td>
<td>Allows single quotes in addition to double quotes.</td>
<td>read</td>
</tr>
<tr>
<td><code>allowNumericLeadingZero</code></td>
<td><code>false</code></td>
<td>Allows leading zeros in numbers (e.g. 00012).</td>
<td>read</td>
</tr>
<tr>
<td><code>allowBackslashEscapingAnyCharacter</code></td>
<td><code>false</code></td>
<td>Allows accepting quoting of all character using backslash quoting mechanism<br />.</td>
<td>read</td>
</tr>
<tr>
<td><code>mode</code></td>
<td><code>PERMISSIVE</code></td>
<td>Allows a mode for dealing with corrupt records during parsing.&#96;&#96;* <code>PERMISSIVE</code>: when it meets a corrupted record, puts the malformed string into a field configured by <code>columnNameOfCorruptRecord</code>, and sets malformed fields to <code>null</code>. To keep corrupt records, an user can set a string type field named <code>columnNameOfCorruptRecord</code> in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. When inferring a schema, it implicitly adds a <code>columnNameOfCorruptRecord</code> field in an output schema.* <code>DROPMALFORMED</code>: ignores the whole corrupted records. This mode is unsupported in the JSON built-in functions.* <code>FAILFAST</code>: throws an exception when it meets corrupted records.</td>
<td>read</td>
</tr>
<tr>
<td><code>columnNameOfCorruptRecord</code></td>
<td>(value of <code>spark.sql.columnNameOfCorruptRecord</code> configuration)</td>
<td>Allows renaming the new field having malformed string created by <code>PERMISSIVE</code> mode. This overrides spark.sql.columnNameOfCorruptRecord.</td>
<td>read</td>
</tr>
<tr>
<td><code>dateFormat</code></td>
<td><code>yyyy-MM-dd</code></td>
<td>Sets the string that indicates a date format. Custom date formats follow the formats at<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html">datetime pattern</a>. This applies to date type.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>timestampFormat</code></td>
<td><code>yyyy-MM-dd&#39;T&#39;HH:mm:ss[.SSS][XXX]</code></td>
<td>Sets the string that indicates a timestamp format. Custom date formats follow the formats at<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html">datetime pattern</a>. This applies to timestamp type.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>timestampNTZFormat</code></td>
<td>yyyy-MM-dd’T’HH:mm:ss[.SSS]</td>
<td>Sets the string that indicates a timestamp without timezone format. Custom date formats follow the formats at<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html">Datetime Patterns</a>. This applies to timestamp without timezone type, note that zone-offset and time-zone components are not supported when writing or reading this data type.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>multiLine</code></td>
<td><code>false</code></td>
<td>Parse one record, which may span multiple lines, per file. JSON built-in functions ignore this option.</td>
<td>read</td>
</tr>
<tr>
<td><code>allowUnquotedControlChars</code></td>
<td><code>false</code></td>
<td>Allows JSON Strings to contain unquoted control characters (ASCII characters with value less than 32, including tab and line feed characters) or not.</td>
<td>read</td>
</tr>
<tr>
<td><code>encoding</code></td>
<td>Detected automatically when <code>multiLine</code> is set to <code>true</code> (for reading), <code>UTF-8</code> (for writing)</td>
<td>For reading, allows to forcibly set one of standard basic or extended encoding for the JSON files. For example UTF-16BE, UTF-32LE. For writing, Specifies encoding (charset) of saved json files. JSON built-in functions ignore this option.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>lineSep</code></td>
<td><code>\r</code>, <code>\r\n</code>, <code>\n</code> (for reading), <code>\n</code> (for writing)</td>
<td>Defines the line separator that should be used for parsing. JSON built-in functions ignore this option.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>samplingRatio</code></td>
<td><code>1.0</code></td>
<td>Defines fraction of input JSON objects used for schema inferring.</td>
<td>read</td>
</tr>
<tr>
<td><code>dropFieldIfAllNull</code></td>
<td><code>false</code></td>
<td>Whether to ignore column of all null values or empty array&#x2F;struct during schema inference.</td>
<td>read</td>
</tr>
<tr>
<td><code>locale</code></td>
<td><code>en-US</code></td>
<td>Sets a locale as language tag in IETF BCP 47 format. For instance,<code>locale</code> is used while parsing dates and timestamps.</td>
<td>read</td>
</tr>
<tr>
<td><code>allowNonNumericNumbers</code></td>
<td><code>true</code></td>
<td>Allows JSON parser to recognize set of “Not-a-Number” (NaN) tokens as legal floating number values.&#96;&#96;* <code>+INF</code>:<br /> for positive infinity, as well as alias of <code>+Infinity</code> and <code>Infinity</code>.* <code>-INF</code>: for negative infinity, alias <code>-Infinity</code>.* <code>NaN</code>: for other not-a-numbers, like result of division by zero.</td>
<td>read</td>
</tr>
<tr>
<td><code>compression</code></td>
<td>(none)</td>
<td>Compression codec to use when saving to file. This can be one of the known case-insensitive shorten names<br /> (none, bzip2, gzip, lz4, snappy and deflate). JSON built-in functions ignore this option.</td>
<td>write</td>
</tr>
<tr>
<td><code>ignoreNullFields</code></td>
<td>(value of <code>spark.sql.jsonGenerator.ignoreNullFields</code> configuration)</td>
<td>Whether to ignore null fields when generating JSON objects.</td>
<td>write</td>
</tr>
</tbody></table>
<h3 id="csv"><a href="#csv" class="headerlink" title="csv"></a>csv</h3><table>
<thead>
<tr>
<th><strong>Property Name</strong></th>
<th><strong>Default</strong></th>
<th><strong>Meaning</strong></th>
<th><strong>Scope</strong></th>
</tr>
</thead>
<tbody><tr>
<td><code>sep</code></td>
<td>,</td>
<td>Sets a separator for each field and value. This separator can be one or more characters.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>encoding</code></td>
<td>UTF-8</td>
<td>For reading, decodes the CSV files by the given encoding type. For writing, specifies encoding (charset) of saved CSV files. CSV built-in functions ignore this option.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>quote</code></td>
<td>“</td>
<td>Sets a single character used for escaping quoted values where the separator can be part of the value. For reading, if you would like to turn off quotations, you need to set not <code>null</code> but an empty string. For writing, if an empty string is set, it uses <code>u0000</code> (null character).</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>quoteAll</code></td>
<td>false</td>
<td>A flag indicating whether all values should always be enclosed in quotes. Default is to only escape values containing a quote character.</td>
<td>write</td>
</tr>
<tr>
<td><code>escape</code></td>
<td>\</td>
<td>Sets a single character used for escaping quotes inside an already quoted value.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>escapeQuotes</code></td>
<td>true</td>
<td>A flag indicating whether values containing quotes should always be enclosed in quotes. Default is to escape all values containing a quote character.</td>
<td>write</td>
</tr>
<tr>
<td><code>comment</code></td>
<td></td>
<td>Sets a single character used for skipping lines beginning with this character. By default, it is disabled.</td>
<td>read</td>
</tr>
<tr>
<td><code>header</code></td>
<td>false</td>
<td>For reading, uses the first line as names of columns. For writing, writes the names of columns as the first line. Note that if the given path is a RDD of Strings, this header option will remove all lines same with the header if exists. CSV built-in functions ignore this option.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>inferSchema</code></td>
<td>false</td>
<td>Infers the input schema automatically from data. It requires one extra pass over the data. CSV built-in functions ignore this option.</td>
<td>read</td>
</tr>
<tr>
<td><code>enforceSchema</code></td>
<td>true</td>
<td>If it is set to <code>true</code>, the specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored. If the option is set to <code>false</code>, the schema will be validated against all headers in CSV files in the case when the <code>header</code> option is set to <code>true</code>. Field names in the schema and column names in CSV headers are checked by their positions taking into account <code>spark.sql.caseSensitive</code>. Though the default value is true, it is recommended to disable the <code>enforceSchema</code> option to avoid incorrect results. CSV built-in functions ignore this option.</td>
<td>read</td>
</tr>
<tr>
<td><code>ignoreLeadingWhiteSpace</code></td>
<td><code>false</code> (for reading), <code>true</code> (for writing)</td>
<td>A flag indicating whether or not leading whitespaces from values being read&#x2F;written should be skipped.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>ignoreTrailingWhiteSpace</code></td>
<td><code>false</code> (for reading), <code>true</code> (for writing)</td>
<td>A flag indicating whether or not trailing whitespaces from values being read&#x2F;written should be skipped.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>nullValue</code></td>
<td></td>
<td>Sets the string representation of a null value. Since 2.0.1, this <code>nullValue</code> param applies to all supported types including the string type.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>nanValue</code></td>
<td>NaN</td>
<td>Sets the string representation of a non-number value.</td>
<td>read</td>
</tr>
<tr>
<td><code>positiveInf</code></td>
<td>Inf</td>
<td>Sets the string representation of a positive infinity value.</td>
<td>read</td>
</tr>
<tr>
<td><code>negativeInf</code></td>
<td>-Inf</td>
<td>Sets the string representation of a negative infinity value.</td>
<td>read</td>
</tr>
<tr>
<td><code>dateFormat</code></td>
<td>yyyy-MM-dd</td>
<td>Sets the string that indicates a date format. Custom date formats follow the formats at<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html">Datetime Patterns</a>. This applies to date type.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>timestampFormat</code></td>
<td>yyyy-MM-dd’T’HH:mm:ss[.SSS][XXX]</td>
<td>Sets the string that indicates a timestamp format. Custom date formats follow the formats at<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html">Datetime Patterns</a>. This applies to timestamp type.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>timestampNTZFormat</code></td>
<td>yyyy-MM-dd’T’HH:mm:ss[.SSS]</td>
<td>Sets the string that indicates a timestamp without timezone format. Custom date formats follow the formats at<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html">Datetime Patterns</a>. This applies to timestamp without timezone type, note that zone-offset and time-zone components are not supported when writing or reading this data type.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>maxColumns</code></td>
<td>20480</td>
<td>Defines a hard limit of how many columns a record can have.</td>
<td>read</td>
</tr>
<tr>
<td><code>maxCharsPerColumn</code></td>
<td>-1</td>
<td>Defines the maximum number of characters allowed for any given value being read. By default, it is -1 meaning unlimited length</td>
<td>read</td>
</tr>
<tr>
<td><code>mode</code></td>
<td>PERMISSIVE</td>
<td>Allows a mode for dealing with corrupt records during parsing. It supports the following case-insensitive modes. Note that Spark tries to parse only required columns in CSV under column pruning. Therefore, corrupt records can be different based on required set of fields. This behavior can be controlled by <code>spark.sql.csv.parser.columnPruning.enabled</code> (enabled by default).&#96;&#96;* <code>PERMISSIVE</code>: when it meets a corrupted record, puts the malformed string into a field configured by <code>columnNameOfCorruptRecord</code>, and sets malformed fields to <code>null</code>. To keep corrupt records, an user can set a string type field named <code>columnNameOfCorruptRecord</code> in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. A record with less&#x2F;more tokens than schema is not a corrupted record to CSV. When it meets a record having fewer tokens than the length of the schema, sets <code>null</code> to extra fields. When the record has more tokens than the length of the schema, it drops extra tokens.* <code>DROPMALFORMED</code>: ignores the whole corrupted records. This mode is unsupported in the CSV built-in functions.* <code>FAILFAST</code>: throws an exception when it meets corrupted records.</td>
<td>read</td>
</tr>
<tr>
<td><code>columnNameOfCorruptRecord</code></td>
<td>(value of <code>spark.sql.columnNameOfCorruptRecord</code> configuration)</td>
<td>Allows renaming the new field having malformed string created by <code>PERMISSIVE</code> mode. This overrides <code>spark.sql.columnNameOfCorruptRecord</code>.</td>
<td>read</td>
</tr>
<tr>
<td><code>multiLine</code></td>
<td>false</td>
<td>Parse one record, which may span multiple lines, per file. CSV built-in functions ignore this option.</td>
<td>read</td>
</tr>
<tr>
<td><code>charToEscapeQuoteEscaping</code></td>
<td><code>escape</code> or <code>\0</code></td>
<td>Sets a single character used for escaping the escape for the quote character. The default value is escape character when escape and quote characters are different,<code>\0</code> otherwise.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>samplingRatio</code></td>
<td>1.0</td>
<td>Defines fraction of rows used for schema inferring. CSV built-in functions ignore this option.</td>
<td>read</td>
</tr>
<tr>
<td><code>emptyValue</code></td>
<td>(for reading),<code>&quot;&quot;</code> (for writing)</td>
<td>Sets the string representation of an empty value.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>locale</code></td>
<td>en-US</td>
<td>Sets a locale as language tag in IETF BCP 47 format. For instance, this is used while parsing dates and timestamps.</td>
<td>read</td>
</tr>
<tr>
<td><code>lineSep</code></td>
<td><code>\r</code>, <code>\r\n</code> and <code>\n</code> (for reading), <code>\n</code> (for writing)</td>
<td>Defines the line separator that should be used for parsing&#x2F;writing. Maximum length is 1 character. CSV built-in functions ignore this option.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>unescapedQuoteHandling</code></td>
<td>STOP_AT_DELIMITER</td>
<td>Defines how the CsvParser will handle values with unescaped quotes.&#96;&#96;* <code>STOP_AT_CLOSING_QUOTE</code>: If unescaped quotes are found in the input, accumulate the quote character and proceed parsing the value as a quoted value, until a closing quote is found.* <code>BACK_TO_DELIMITER</code>: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters of the current parsed value until the delimiter is found. If no delimiter is found in the value, the parser will continue accumulating characters from the input until a delimiter or line ending is found.* <code>STOP_AT_DELIMITER</code>: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters until the delimiter or a line ending is found in the input.* <code>SKIP_VALUE</code>: If unescaped quotes are found in the input, the content parsed for the given value will be skipped and the value set in nullValue will be produced instead.* <code>RAISE_ERROR</code>: If unescaped quotes are found in the input, a TextParsingException will be thrown.</td>
<td>read</td>
</tr>
<tr>
<td><code>compression</code></td>
<td>(none)</td>
<td>Compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (<code>none</code>, <code>bzip2</code>, <code>gzip</code>, <code>lz4</code>, <code>snappy</code> and <code>deflate</code>). CSV built-in functions ignore this option.</td>
<td>write</td>
</tr>
</tbody></table>
<h3 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h3><table>
<thead>
<tr>
<th><strong>Property Name</strong></th>
<th><strong>Default</strong></th>
<th><strong>Meaning</strong></th>
<th><strong>Scope</strong></th>
</tr>
</thead>
<tbody><tr>
<td><code>url</code></td>
<td>(none)</td>
<td>The JDBC URL of the form <code>jdbc:subprotocol:subname</code> to connect to. The source-specific connection properties may be specified in the URL. e.g., <code>jdbc:postgresql://localhost/test?user=fred&amp;password=secret</code></td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>dbtable</code></td>
<td>(none)</td>
<td>The JDBC table that should be read from or written into. Note that when using it in the read path anything that is valid in a <code>FROM</code> clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses. It is not allowed to specify <code>dbtable</code> and <code>query</code> options at the same time.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>query</code></td>
<td>(none)</td>
<td>A query that will be used to read data into Spark. The specified query will be parenthesized and used as a subquery in the <code>FROM</code> clause. Spark will also assign an alias to the subquery clause. As an example, spark will issue a query of the following form to the JDBC Source.<code>SELECT &lt;columns&gt; FROM (&lt;user_specified_query&gt;) spark_gen_alias</code>Below are a couple of restrictions while using this option.<code>1. It is not allowed to specify `dbtable` and `query` options at the same time.1. It is not allowed to specify `query` and `partitionColumn` options at the same time. When specifying `partitionColumn` option is required, the subquery can be specified using `dbtable` option instead and partition columns can be qualified using the subquery alias provided as part of `dbtable`.</code>Example:<code>spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, jdbcUrl).option(&quot;query&quot;, &quot;select c1, c2 from t1&quot;).load()</code></td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>driver</code></td>
<td>(none)</td>
<td>The class name of the JDBC driver to use to connect to this URL.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>partitionColumn, lowerBound, upperBound</code></td>
<td>(none)</td>
<td>These options must all be specified if any of them is specified. In addition,<code>numPartitions</code> must be specified. They describe how to partition the table when reading in parallel from multiple workers. <code>partitionColumn</code> must be a numeric, date, or timestamp column from the table in question. Notice that <code>lowerBound</code> and <code>upperBound</code> are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading.</td>
<td>read</td>
</tr>
<tr>
<td><code>numPartitions</code></td>
<td>(none)</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit, we decrease it to this limit by calling <code>coalesce(numPartitions)</code> before writing.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>queryTimeout</code></td>
<td><code>0</code></td>
<td>The number of seconds the driver will wait for a Statement object to execute to the given number of seconds. Zero means there is no limit. In the write path, this option depends on how JDBC drivers implement the API <code>setQueryTimeout</code>, e.g., the h2 JDBC driver checks the timeout of each query instead of an entire JDBC batch.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>fetchsize</code></td>
<td><code>0</code></td>
<td>The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (e.g. Oracle with 10 rows).</td>
<td>read</td>
</tr>
<tr>
<td><code>batchsize</code></td>
<td><code>1000</code></td>
<td>The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing.</td>
<td>write</td>
</tr>
<tr>
<td><code>isolationLevel</code></td>
<td><code>READ_UNCOMMITTED</code></td>
<td>The transaction isolation level, which applies to current connection. It can be one of <code>NONE</code>, <code>READ_COMMITTED</code>, <code>READ_UNCOMMITTED</code>, <code>REPEATABLE_READ</code>, or <code>SERIALIZABLE</code>, corresponding to standard transaction isolation levels defined by JDBC’s Connection object, with default of <code>READ_UNCOMMITTED</code>. Please refer the documentation in <code>java.sql.Connection</code>.</td>
<td>write</td>
</tr>
<tr>
<td><code>sessionInitStatement</code></td>
<td>(none)</td>
<td>After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL&#x2F;SQL block). Use this to implement session initialization code. Example:<code>option(&quot;sessionInitStatement&quot;, &quot;&quot;&quot;BEGIN execute immediate &#39;alter session set &quot;_serial_direct_read&quot;=true&#39;; END;&quot;&quot;&quot;)</code></td>
<td>read</td>
</tr>
<tr>
<td><code>truncate</code></td>
<td><code>false</code></td>
<td>This is a JDBC writer related option. When <code>SaveMode.Overwrite</code> is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. In case of failures, users should turn off <code>truncate</code> option to use <code>DROP TABLE</code> again. Also, due to the different behavior of <code>TRUNCATE TABLE</code> among DBMS, it’s not always safe to use this. MySQLDialect, DB2Dialect, MsSqlServerDialect, DerbyDialect, and OracleDialect supports this while PostgresDialect and default JDBCDirect doesn’t. For unknown and unsupported JDBCDirect, the user option <code>truncate</code> is ignored.</td>
<td>write</td>
</tr>
<tr>
<td><code>cascadeTruncate</code></td>
<td>the default cascading truncate behaviour of the JDBC database in question, specified in the <code>isCascadeTruncate</code> in each JDBCDialect</td>
<td>This is a JDBC writer related option. If enabled and supported by the JDBC database (PostgreSQL and Oracle at the moment), this options allows execution of a <code>TRUNCATE TABLE t CASCADE</code> (in the case of PostgreSQL a <code>TRUNCATE TABLE ONLY t CASCADE</code> is executed to prevent inadvertently truncating descendant tables). This will affect other tables, and thus should be used with care.</td>
<td>write</td>
</tr>
<tr>
<td><code>createTableOptions</code></td>
<td></td>
<td>This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g.,<code>CREATE TABLE t (name string) ENGINE=InnoDB.</code>).</td>
<td>write</td>
</tr>
<tr>
<td><code>createTableColumnTypes</code></td>
<td>(none)</td>
<td>The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g:<code>&quot;name CHAR(64), comments VARCHAR(1024)&quot;)</code>. The specified types should be valid spark sql data types.</td>
<td>write</td>
</tr>
<tr>
<td><code>customSchema</code></td>
<td>(none)</td>
<td>The custom schema to use for reading data from JDBC connectors. For example,<code>&quot;id DECIMAL(38, 0), name STRING&quot;</code>. You can also specify partial fields, and the others use the default type mapping. For example, <code>&quot;id DECIMAL(38, 0)&quot;</code>. The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults.</td>
<td>read</td>
</tr>
<tr>
<td><code>pushDownPredicate</code></td>
<td><code>true</code></td>
<td>The option to enable or disable predicate push-down into the JDBC data source. The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible. Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark. Predicate push-down is usually turned off when the predicate filtering is performed faster by Spark than by the JDBC data source.</td>
<td>read</td>
</tr>
<tr>
<td><code>pushDownAggregate</code></td>
<td><code>false</code></td>
<td>The option to enable or disable aggregate push-down in V2 JDBC data source. The default value is false, in which case Spark will not push down aggregates to the JDBC data source. Otherwise, if sets to true, aggregates will be pushed down to the JDBC data source. Aggregate push-down is usually turned off when the aggregate is performed faster by Spark than by the JDBC data source. Please note that aggregates can be pushed down if and only if all the aggregate functions and the related filters can be pushed down. If <code>numPartitions</code> equals to 1 or the group by key is the same as <code>partitionColumn</code>, Spark will push down aggregate to data source completely and not apply a final aggregate over the data source output. Otherwise, Spark will apply a final aggregate over the data source output.</td>
<td>read</td>
</tr>
<tr>
<td><code>pushDownLimit</code></td>
<td><code>false</code></td>
<td>The option to enable or disable LIMIT push-down into V2 JDBC data source. The LIMIT push-down also includes LIMIT + SORT , a.k.a. the Top N operator. The default value is false, in which case Spark does not push down LIMIT or LIMIT with SORT to the JDBC data source. Otherwise, if sets to true, LIMIT or LIMIT with SORT is pushed down to the JDBC data source. If <code>numPartitions</code> is greater than 1, SPARK still applies LIMIT or LIMIT with SORT on the result from data source even if LIMIT or LIMIT with SORT is pushed down. Otherwise, if LIMIT or LIMIT with SORT is pushed down and <code>numPartitions</code> equals to 1, SPARK will not apply LIMIT or LIMIT with SORT on the result from data source.</td>
<td>read</td>
</tr>
<tr>
<td><code>pushDownTableSample</code></td>
<td><code>false</code></td>
<td>The option to enable or disable TABLESAMPLE push-down into V2 JDBC data source. The default value is false, in which case Spark does not push down TABLESAMPLE to the JDBC data source. Otherwise, if value sets to true, TABLESAMPLE is pushed down to the JDBC data source.</td>
<td>read</td>
</tr>
<tr>
<td><code>keytab</code></td>
<td>(none)</td>
<td>Location of the kerberos keytab file (which must be pre-uploaded to all nodes either by <code>--files</code> option of spark-submit or manually) for the JDBC client. When path information found then Spark considers the keytab distributed manually, otherwise <code>--files</code> assumed. If both <code>keytab</code> and <code>principal</code> are defined then Spark tries to do kerberos authentication.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>principal</code></td>
<td>(none)</td>
<td>Specifies kerberos principal name for the JDBC client. If both <code>keytab</code> and <code>principal</code> are defined then Spark tries to do kerberos authentication.</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>refreshKrb5Config</code></td>
<td><code>false</code></td>
<td>This option controls whether the kerberos configuration is to be refreshed or not for the JDBC client before establishing a new connection. Set to true if you want to refresh the configuration, otherwise set to false. The default value is false. Note that if you set this option to true and try to establish multiple connections, a race condition can occur. One possble situation would be like as follows.1. refreshKrb5Config flag is set with security context 11. A JDBC connection provider is used for the corresponding DBMS1. The krb5.conf is modified but the JVM not yet realized that it must be reloaded1. Spark authenticates successfully for security context 11. The JVM loads security context 2 from the modified krb5.conf1. Spark restores the previously saved security context 11. The modified krb5.conf content just gone</td>
<td>read&#x2F;write</td>
</tr>
<tr>
<td><code>connectionProvider</code></td>
<td>(none)</td>
<td>The name of the JDBC connection provider to use to connect to this URL, e.g.<code>db2</code>, <code>mssql</code>. Must be one of the providers loaded with the JDBC data source. Used to disambiguate when more than one provider can handle the specified driver and options. The selected provider must not be disabled by <code>spark.sql.sources.disabledJdbcConnProviderList</code>.</td>
<td>read&#x2F;write</td>
</tr>
</tbody></table>
<h3 id="excel"><a href="#excel" class="headerlink" title="excel"></a>excel</h3><p>暂时没找到，找到再补</p>
<h2 id="读数据"><a href="#读数据" class="headerlink" title="读数据"></a>读数据</h2><h3 id="TEXT-1"><a href="#TEXT-1" class="headerlink" title="TEXT"></a>TEXT</h3><p>官方简介</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Spark SQL provides spark.<span class="built_in">read</span>().<span class="keyword">text</span>(<span class="string">&quot;file_name&quot;</span>) <span class="built_in">to</span> <span class="built_in">read</span> <span class="keyword">a</span> <span class="built_in">file</span> <span class="keyword">or</span> <span class="built_in">directory</span> <span class="keyword">of</span> <span class="keyword">text</span> <span class="built_in">files</span> <span class="keyword">into</span> <span class="keyword">a</span> Spark DataFrame, <span class="keyword">and</span> dataframe.<span class="built_in">write</span>().<span class="keyword">text</span>(<span class="string">&quot;path&quot;</span>) <span class="built_in">to</span> <span class="built_in">write</span> <span class="built_in">to</span> <span class="keyword">a</span> <span class="keyword">text</span> <span class="built_in">file</span>. When reading <span class="keyword">a</span> <span class="keyword">text</span> <span class="built_in">file</span>, <span class="keyword">each</span> <span class="built_in">line</span> becomes <span class="keyword">each</span> row that has <span class="keyword">string</span> “<span class="built_in">value</span>” column <span class="keyword">by</span> default. The <span class="built_in">line</span> separator can be changed <span class="keyword">as</span> shown <span class="keyword">in</span> <span class="keyword">the</span> example below. The option() <span class="function"><span class="keyword">function</span> <span class="title">can</span> <span class="title">be</span> <span class="title">used</span> <span class="title">to</span> <span class="title">customize</span> <span class="title">the</span> <span class="title">behavior</span> <span class="title">of</span> <span class="title">reading</span> <span class="title">or</span> <span class="title">writing</span>, <span class="title">such</span> <span class="title">as</span> <span class="title">controlling</span> <span class="title">behavior</span> <span class="title">of</span> <span class="title">the</span> <span class="title">line</span> <span class="title">separator</span>, <span class="title">compression</span>, <span class="title">and</span> <span class="title">so</span> <span class="title">on</span>.</span></span><br></pre></td></tr></table></figure>

<p>读的时候我们不用设置压缩格式，它和mr一样会自动解压</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">package sparkfirst</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object sparksql2 &#123;</span><br><span class="line"><span class="code">  def main(args: Array[String]): Unit = &#123;</span></span><br><span class="line"><span class="code">    val spark = SparkSession.builder().appName(&quot;Sparksql01&quot;).master(&quot;local[4]&quot;).getOrCreate()</span></span><br><span class="line"><span class="code">    val df = spark.read.text(&quot;file:///D:\\test.txt&quot;) // 返回值是DF</span></span><br><span class="line"><span class="code">    df.show()</span></span><br><span class="line"><span class="code">    df.printSchema()</span></span><br><span class="line"><span class="code">  </span></span><br><span class="line"><span class="code">  </span></span><br><span class="line"><span class="code">    var result =</span></span><br><span class="line"><span class="section">      &quot;&quot;&quot;</span></span><br><span class="line"><span class="section">+--------------------+</span></span><br><span class="line"><span class="section">|               value|</span></span><br><span class="line"><span class="section">+--------------------+</span></span><br><span class="line">|as,s,ed,f,,,qq,eq...|</span><br><span class="line"><span class="section">|,w,wq,e,w,ewq,we,...|</span></span><br><span class="line"><span class="section">+--------------------+</span></span><br><span class="line"><span class="code">      &quot;&quot;&quot;</span></span><br><span class="line"><span class="code">  </span></span><br><span class="line"><span class="code">    //text所带有的schame信息是他自己给我们添加上的，所以有时候我们用这个会不方便</span></span><br><span class="line"><span class="code">    val df1 = spark.read.textFile(&quot;file:///D:\\test.txt&quot;) // 返回值是dataset</span></span><br><span class="line"><span class="code">    df1.printSchema()</span></span><br><span class="line"><span class="code">    //--------------------------------------------------------------------</span></span><br><span class="line"><span class="code">    //使用lineSep改变分隔符如下</span></span><br><span class="line"><span class="code">    val df2 = spark.read.option(&quot;lineSep&quot;,&quot;,&quot;).text(&quot;file:///D:\\test.txt&quot;)</span></span><br><span class="line"><span class="code">    df2.show()</span></span><br><span class="line"><span class="code">    result =</span></span><br><span class="line"><span class="section">      &quot;&quot;&quot;</span></span><br><span class="line"><span class="section">+---------+</span></span><br><span class="line"><span class="section">|    value|</span></span><br><span class="line"><span class="section">+---------+</span></span><br><span class="line">|       as|</span><br><span class="line">|        s|</span><br><span class="line">|       ed|</span><br><span class="line">|        f|</span><br><span class="line">|         |</span><br><span class="line">|         |</span><br><span class="line">|       qq|</span><br><span class="line">|eqedqwe\n|</span><br><span class="line">|        w|</span><br><span class="line">|       wq|</span><br><span class="line">|        e|</span><br><span class="line">|        w|</span><br><span class="line">|      ewq|</span><br><span class="line">|       we|</span><br><span class="line">|        q|</span><br><span class="line">|        e|</span><br><span class="line">|wewqeqwel|</span><br><span class="line">|       qe|</span><br><span class="line">| lqeweqwl|</span><br><span class="line"><span class="section">|     qw\n|</span></span><br><span class="line"><span class="section">+---------+</span></span><br><span class="line"><span class="code">      &quot;&quot;&quot;</span></span><br><span class="line"><span class="code">    //---------------------------------------------------------------------</span></span><br><span class="line"><span class="code">    //使用wholetext把一整个文件当作一行来接受</span></span><br><span class="line"><span class="code">    val df3 = spark.read.option(&quot;wholetext&quot;,true).text(&quot;file:///D:\\test.txt&quot;)</span></span><br><span class="line"><span class="code">    df3.show()</span></span><br><span class="line"><span class="code">    var result =</span></span><br><span class="line"><span class="section">      &quot;&quot;&quot;</span></span><br><span class="line"><span class="section">+--------------------+</span></span><br><span class="line"><span class="section">|               value|</span></span><br><span class="line"><span class="section">+--------------------+</span></span><br><span class="line"><span class="section">|as,s,ed,f,,,qq,eq...|</span></span><br><span class="line"><span class="section">+--------------------+</span></span><br><span class="line"><span class="code">      &quot;&quot;&quot;</span></span><br><span class="line"><span class="code">    //这里的text和textFile是可以相互用的</span></span><br><span class="line"><span class="code">    val df4 = spark.read.option(&quot;wholetext&quot;,true).textFile(&quot;file:///D:\\test.txt&quot;)</span></span><br><span class="line"><span class="code">    df4.show()</span></span><br><span class="line"><span class="code">    var result =</span></span><br><span class="line"><span class="section">      &quot;&quot;&quot;</span></span><br><span class="line"><span class="section">+--------------------+</span></span><br><span class="line"><span class="section">|               value|</span></span><br><span class="line"><span class="section">+--------------------+</span></span><br><span class="line"><span class="section">|as,s,ed,f,,,qq,eq...|</span></span><br><span class="line"><span class="section">+--------------------+</span></span><br><span class="line"><span class="code">      &quot;&quot;&quot;</span></span><br><span class="line"><span class="code">    val df5 = spark.read.option(&quot;wholetext&quot;,true).format(&quot;text&quot;).load(&quot;file:///D:\\test.txt&quot;)</span></span><br><span class="line"><span class="code">    df5.show()</span></span><br><span class="line"><span class="code">    var result =</span></span><br><span class="line"><span class="section">      &quot;&quot;&quot;</span></span><br><span class="line"><span class="section">+--------------------+</span></span><br><span class="line"><span class="section">|               value|</span></span><br><span class="line"><span class="section">+--------------------+</span></span><br><span class="line"><span class="section">|as,s,ed,f,,,qq,eq...|</span></span><br><span class="line"><span class="section">+--------------------+</span></span><br><span class="line"><span class="code">      &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="code">  &#125;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>我们点进去源码发现text的底层是</p>
<p><code>def text(paths: String*): DataFrame = format(&quot;text&quot;).load(paths : _*)</code></p>
<p>所以我们使用的时候可以</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df5=spark.<span class="keyword">read</span>.<span class="keyword">option</span>(&quot;wholetext&quot;,<span class="keyword">true</span>).format(&quot;text&quot;).<span class="keyword">load</span>(&quot;file:///D:\\test.txt&quot;)</span><br></pre></td></tr></table></figure>

<h3 id="json-1"><a href="#json-1" class="headerlink" title="json"></a>json</h3><p>简介</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Spark <span class="keyword">SQL</span> can automatically infer the <span class="keyword">schema</span> <span class="keyword">of</span> a <span class="type">JSON</span> dataset <span class="keyword">and</span> <span class="keyword">load</span> it <span class="keyword">as</span> a Dataset[<span class="keyword">Row</span>]. This <span class="keyword">conversion</span> can be done <span class="keyword">using</span> SparkSession.<span class="keyword">read</span>.json() <span class="keyword">on</span> either a Dataset[String], <span class="keyword">or</span> a <span class="type">JSON</span> file.</span><br><span class="line"></span><br><span class="line">Note that the file that <span class="keyword">is</span> offered <span class="keyword">as</span> a <span class="type">json</span> file <span class="keyword">is</span> <span class="keyword">not</span> a typical <span class="type">JSON</span> file. <span class="keyword">Each</span> <span class="type">line</span> must contain a separate, self-contained <span class="keyword">valid</span> <span class="type">JSON</span> <span class="keyword">object</span>. <span class="keyword">For</span> more information, please see <span class="type">JSON</span> Lines <span class="type">text</span> <span class="keyword">format</span>, <span class="keyword">also</span> <span class="keyword">called</span> newline-delimited <span class="type">JSON</span>.</span><br><span class="line"></span><br><span class="line"><span class="keyword">For</span> a regular multi-<span class="type">line</span> <span class="type">JSON</span> file, <span class="keyword">set</span> the multiLine <span class="keyword">option</span> <span class="keyword">to</span> <span class="keyword">true</span>.</span><br></pre></td></tr></table></figure>

<p>json分为简单json和嵌套json</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">val spark = SparkSession.builder().appName(<span class="string">&quot;Sparksql01&quot;</span>).master(<span class="string">&quot;local[4]&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="regexp">//</span>-------------------------普通json</span><br><span class="line">val df = spark.read.json(<span class="string">&quot;file:///C:\\Users\\dell\\Desktop\\dept.json&quot;</span>)</span><br><span class="line">df.show()</span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="regexp">//</span>--------------------------嵌套json如果嵌套的是STruct =&gt; 打点 / ARRAY 类型 先炸开 再打点</span><br><span class="line">var df1 = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;file:///C:\\Users\\dell\\Desktop\\Skills.json&quot;</span>)</span><br><span class="line">df1.printSchema()</span><br><span class="line"><span class="regexp">//</span>--------------------------api</span><br><span class="line"><span class="regexp">//</span>-withColumn可以增加一个字段，或者把一个字段重命名 =》 提出字段</span><br><span class="line">df1=df1.withColumn(<span class="string">&quot;critical&quot;</span>,col(<span class="string">&quot;damage.critical&quot;</span>))</span><br><span class="line">df1=df1.withColumn(<span class="string">&quot;elementId&quot;</span>,explode(col(<span class="string">&quot;damage.elementId&quot;</span>)))</span><br><span class="line">df1.printSchema()</span><br><span class="line"><span class="regexp">//</span>------------------------删除字段</span><br><span class="line">df1=df1.drop(<span class="string">&quot;damage.critical&quot;</span>,<span class="string">&quot;damage.elementId&quot;</span>)</span><br><span class="line"><span class="regexp">//</span>-------------------------sql</span><br><span class="line"><span class="regexp">//</span>------------------------对比hivesql</span><br><span class="line">df1.createOrReplaceTempView(<span class="string">&quot;test&quot;</span>)</span><br><span class="line"><span class="regexp">//</span>spark.sql(<span class="string">&quot;SELECT get_json_object(&#x27;&#123;\&quot;a\&quot;:\&quot;b\&quot;&#125;&#x27;, &#x27;$.a&#x27;);&quot;</span>).show()</span><br><span class="line"><span class="regexp">//</span> struct可以用下面打点的方法</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    |select</span></span><br><span class="line"><span class="string">    |effects.ddd,</span></span><br><span class="line"><span class="string">    |damage.ddddds</span></span><br><span class="line"><span class="string">    |from</span></span><br><span class="line"><span class="string">    |test</span></span><br><span class="line"><span class="string">    |&quot;&quot;&quot;</span>.stripMargin).show()</span><br><span class="line"><span class="regexp">//</span>或者用爆炸加侧写进行 ：array元素,嵌套json</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    |select</span></span><br><span class="line"><span class="string">    |effects.ddd,</span></span><br><span class="line"><span class="string">    |damage.ddddds</span></span><br><span class="line"><span class="string">    |from</span></span><br><span class="line"><span class="string">    |test</span></span><br><span class="line"><span class="string">    |lateral view explode(store.fruit) as fruit</span></span><br><span class="line"><span class="string">    |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="csv-1"><a href="#csv-1" class="headerlink" title="csv"></a>csv</h3><p>简介</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Spark SQL provides spark.<span class="built_in">read</span>().csv(<span class="string">&quot;file_name&quot;</span>) <span class="built_in">to</span> <span class="built_in">read</span> <span class="keyword">a</span> <span class="built_in">file</span> <span class="keyword">or</span> <span class="built_in">directory</span> <span class="keyword">of</span> <span class="built_in">files</span> <span class="keyword">in</span> CSV <span class="built_in">format</span> <span class="keyword">into</span> Spark DataFrame, <span class="keyword">and</span> dataframe.<span class="built_in">write</span>().csv(<span class="string">&quot;path&quot;</span>) <span class="built_in">to</span> <span class="built_in">write</span> <span class="built_in">to</span> <span class="keyword">a</span> CSV <span class="built_in">file</span>. Function option() can be used <span class="built_in">to</span> customize <span class="keyword">the</span> behavior <span class="keyword">of</span> reading <span class="keyword">or</span> writing, such <span class="keyword">as</span> controlling behavior <span class="keyword">of</span> <span class="keyword">the</span> header, delimiter <span class="keyword">character</span>, <span class="keyword">character</span> <span class="built_in">set</span>, <span class="keyword">and</span> so on.</span><br></pre></td></tr></table></figure>

<p>csv文件默认的分隔符是，但是可以更改</p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line">val spark = SparkSession.builder().appName(<span class="string">&quot;Sparksql01&quot;</span>).master(<span class="string">&quot;local[4]&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">val df = spark.read.format(<span class="string">&quot;csv&quot;</span>).load(<span class="string">&quot;file:///C:\\Users\\dell\\Desktop\\user_profile.csv&quot;</span>)</span><br><span class="line">df.show()</span><br><span class="line">var result=</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">|+---+---------+------+----+----------+---+--------------------+------------+----------+</span></span><br><span class="line">    <span class="string">||_c0|      _c1|   _c2| _c3|       _c4|_c5|                 _c6|         _c7|       _c8|</span></span><br><span class="line">    <span class="string">|+---+---------+------+----+----------+---+--------------------+------------+----------+</span></span><br><span class="line">    <span class="string">|| id|device_id|gender| age|university|gpa|active_days_withi...|question_cnt|answer_cnt|</span></span><br><span class="line">    <span class="string">||  1|     2138|  male|  21|  北京大学|3.4|                   7|           2|        12|</span></span><br><span class="line">    <span class="string">||  2|     3214|  male|null|  复旦大学|  4|                  15|           5|        25|</span></span><br><span class="line">    <span class="string">||  3|     6543|female|  20|  北京大学|3.2|                  12|           3|        30|</span></span><br><span class="line">    <span class="string">||  4|     2315|female|  23|  浙江大学|3.6|                   5|           1|         2|</span></span><br><span class="line">    <span class="string">||  5|     5432|  male|  25|  山东大学|3.8|                  20|          15|        70|</span></span><br><span class="line">    <span class="string">||  6|     2131|  male|  28|  山东大学|3.3|                  15|           7|        13|</span></span><br><span class="line">    <span class="string">||  7|     4321|  male|  28|  复旦大学|3.6|                   9|           6|        52|</span></span><br><span class="line">    <span class="string">|+---+---------+------+----+----------+---+--------------------+------------+----------+</span></span><br><span class="line">    <span class="string">|&quot;&quot;&quot;</span>.stripMargin</span><br><span class="line"><span class="comment">// 这样默认是通过，进行分割的</span></span><br><span class="line"><span class="comment">//可以通过delimiter来设置分割参数，sep和他一样</span></span><br><span class="line">val df1 = spark.read.option(<span class="string">&quot;delimiter&quot;</span>,<span class="string">&quot;;&quot;</span>).csv(<span class="string">&quot;file:///C:\\Users\\dell\\Desktop\\user_profile.csv&quot;</span>)</span><br><span class="line">df1.show()</span><br><span class="line">result =</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">|+------------------------+</span></span><br><span class="line">    <span class="string">||                     _c0|</span></span><br><span class="line">    <span class="string">|+------------------------+</span></span><br><span class="line">    <span class="string">||    &quot;</span>id<span class="string">&quot;,&quot;</span>device_id<span class="string">&quot;,...|</span></span><br><span class="line">    <span class="string">||  1,2138,male,21,北京...|</span></span><br><span class="line">    <span class="string">||2,3214,male,,复旦大学...|</span></span><br><span class="line">    <span class="string">||    3,6543,female,20,...|</span></span><br><span class="line">    <span class="string">||    4,2315,female,23,...|</span></span><br><span class="line">    <span class="string">||  5,5432,male,25,山东...|</span></span><br><span class="line">    <span class="string">||  6,2131,male,28,山东...|</span></span><br><span class="line">    <span class="string">||  7,4321,male,28,复旦...|</span></span><br><span class="line">    <span class="string">|+------------------------+</span></span><br><span class="line">    <span class="string">|&quot;&quot;&quot;</span>.stripMargin</span><br><span class="line">val df4 = spark.read.option(<span class="string">&quot;sep&quot;</span>,<span class="string">&quot;;&quot;</span>).csv(<span class="string">&quot;file:///C:\\Users\\dell\\Desktop\\user_profile.csv&quot;</span>)</span><br><span class="line">df4.show()</span><br><span class="line">result =</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">|+------------------------+</span></span><br><span class="line">    <span class="string">||                     _c0|</span></span><br><span class="line">    <span class="string">|+------------------------+</span></span><br><span class="line">    <span class="string">||    &quot;</span>id<span class="string">&quot;,&quot;</span>device_id<span class="string">&quot;,...|</span></span><br><span class="line">    <span class="string">||  1,2138,male,21,北京...|</span></span><br><span class="line">    <span class="string">||2,3214,male,,复旦大学...|</span></span><br><span class="line">    <span class="string">||    3,6543,female,20,...|</span></span><br><span class="line">    <span class="string">||    4,2315,female,23,...|</span></span><br><span class="line">    <span class="string">||  5,5432,male,25,山东...|</span></span><br><span class="line">    <span class="string">||  6,2131,male,28,山东...|</span></span><br><span class="line">    <span class="string">||  7,4321,male,28,复旦...|</span></span><br><span class="line">    <span class="string">|+------------------------+</span></span><br><span class="line">    <span class="string">|&quot;&quot;&quot;</span>.stripMargin</span><br><span class="line"><span class="comment">//还可以从csv里加载表头</span></span><br><span class="line">val df2 = spark.read.option(<span class="string">&quot;delimiter&quot;</span>,<span class="string">&quot;,&quot;</span>).option(<span class="string">&quot;header&quot;</span>,<span class="string">&quot;true&quot;</span>).format(<span class="string">&quot;csv&quot;</span>).load(<span class="string">&quot;file:///C:\\Users\\dell\\Desktop\\user_profile.csv&quot;</span>)</span><br><span class="line">df2.show()</span><br><span class="line">result=</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">|+---+---------+------+----+----------+---+---------------------+------------+----------+</span></span><br><span class="line">    <span class="string">|| id|device_id|gender| age|university|gpa|active_days_within_30|question_cnt|answer_cnt|</span></span><br><span class="line">    <span class="string">|+---+---------+------+----+----------+---+---------------------+------------+----------+</span></span><br><span class="line">    <span class="string">||  1|     2138|  male|  21|  北京大学|3.4|                    7|           2|        12|</span></span><br><span class="line">    <span class="string">||  2|     3214|  male|null|  复旦大学|  4|                   15|           5|        25|</span></span><br><span class="line">    <span class="string">||  3|     6543|female|  20|  北京大学|3.2|                   12|           3|        30|</span></span><br><span class="line">    <span class="string">||  4|     2315|female|  23|  浙江大学|3.6|                    5|           1|         2|</span></span><br><span class="line">    <span class="string">||  5|     5432|  male|  25|  山东大学|3.8|                   20|          15|        70|</span></span><br><span class="line">    <span class="string">||  6|     2131|  male|  28|  山东大学|3.3|                   15|           7|        13|</span></span><br><span class="line">    <span class="string">||  7|     4321|  male|  28|  复旦大学|3.6|                    9|           6|        52|</span></span><br><span class="line">    <span class="string">|+---+---------+------+----+----------+---+---------------------+------------+----------+</span></span><br><span class="line">    <span class="string">|&quot;&quot;&quot;</span>.stripMargin</span><br><span class="line"><span class="comment">//还可以把上面两个option合并</span></span><br><span class="line">val df3 = spark.read.options(Map(<span class="string">&quot;delimiter&quot;</span> -&gt; <span class="string">&quot;,&quot;</span> ,<span class="string">&quot;header&quot;</span> -&gt; <span class="string">&quot;true&quot;</span>)).csv(<span class="string">&quot;file:///C:\\Users\\dell\\Desktop\\user_profile.csv&quot;</span>)</span><br><span class="line">df3.show()</span><br><span class="line">result=</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">|+---+---------+------+----+----------+---+---------------------+------------+----------+</span></span><br><span class="line">    <span class="string">|| id|device_id|gender| age|university|gpa|active_days_within_30|question_cnt|answer_cnt|</span></span><br><span class="line">    <span class="string">|+---+---------+------+----+----------+---+---------------------+------------+----------+</span></span><br><span class="line">    <span class="string">||  1|     2138|  male|  21|  北京大学|3.4|                    7|           2|        12|</span></span><br><span class="line">    <span class="string">||  2|     3214|  male|null|  复旦大学|  4|                   15|           5|        25|</span></span><br><span class="line">    <span class="string">||  3|     6543|female|  20|  北京大学|3.2|                   12|           3|        30|</span></span><br><span class="line">    <span class="string">||  4|     2315|female|  23|  浙江大学|3.6|                    5|           1|         2|</span></span><br><span class="line">    <span class="string">||  5|     5432|  male|  25|  山东大学|3.8|                   20|          15|        70|</span></span><br><span class="line">    <span class="string">||  6|     2131|  male|  28|  山东大学|3.3|                   15|           7|        13|</span></span><br><span class="line">    <span class="string">||  7|     4321|  male|  28|  复旦大学|3.6|                    9|           6|        52|</span></span><br><span class="line">    <span class="string">|+---+---------+------+----+----------+---+---------------------+------------+----------+</span></span><br><span class="line">    <span class="string">|&quot;&quot;&quot;</span>.stripMargin</span><br><span class="line"><span class="comment">//还可以加上自动推断类型，如果不加，它就会默认是字符串类型inferSchema</span></span><br><span class="line">val df5 = spark.read.options(Map(<span class="string">&quot;sep&quot;</span>-&gt;<span class="string">&quot;,&quot;</span>,<span class="string">&quot;header&quot;</span>-&gt;<span class="string">&quot;true&quot;</span>,<span class="string">&quot;inferSchema&quot;</span>-&gt;<span class="string">&quot;true&quot;</span>,<span class="string">&quot;encoding&quot;</span>-&gt;<span class="string">&quot;UTF8&quot;</span>)).csv(<span class="string">&quot;file:///C:\\Users\\dell\\Desktop\\user_profile.csv&quot;</span>)</span><br><span class="line">df5.show()</span><br><span class="line">result=</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">|+---+---------+------+----+----------+---+---------------------+------------+----------+</span></span><br><span class="line">    <span class="string">|| id|device_id|gender| age|university|gpa|active_days_within_30|question_cnt|answer_cnt|</span></span><br><span class="line">    <span class="string">|+---+---------+------+----+----------+---+---------------------+------------+----------+</span></span><br><span class="line">    <span class="string">||  1|     2138|  male|  21|  北京大学|3.4|                    7|           2|        12|</span></span><br><span class="line">    <span class="string">||  2|     3214|  male|null|  复旦大学|4.0|                   15|           5|        25|</span></span><br><span class="line">    <span class="string">||  3|     6543|female|  20|  北京大学|3.2|                   12|           3|        30|</span></span><br><span class="line">    <span class="string">||  4|     2315|female|  23|  浙江大学|3.6|                    5|           1|         2|</span></span><br><span class="line">    <span class="string">||  5|     5432|  male|  25|  山东大学|3.8|                   20|          15|        70|</span></span><br><span class="line">    <span class="string">||  6|     2131|  male|  28|  山东大学|3.3|                   15|           7|        13|</span></span><br><span class="line">    <span class="string">||  7|     4321|  male|  28|  复旦大学|3.6|                    9|           6|        52|</span></span><br><span class="line">    <span class="string">|+---+---------+------+----+----------+---+---------------------+------------+----------+</span></span><br><span class="line">    <span class="string">|&quot;&quot;&quot;</span>.stripMargin</span><br><span class="line">df5.printSchema()</span><br><span class="line">result=</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">|root</span></span><br><span class="line">    <span class="string">| |-- id: integer (nullable = true)</span></span><br><span class="line">    <span class="string">| |-- device_id: integer (nullable = true)</span></span><br><span class="line">    <span class="string">| |-- gender: string (nullable = true)</span></span><br><span class="line">    <span class="string">| |-- age: integer (nullable = true)</span></span><br><span class="line">    <span class="string">| |-- university: string (nullable = true)</span></span><br><span class="line">    <span class="string">| |-- gpa: double (nullable = true)</span></span><br><span class="line">    <span class="string">| |-- active_days_within_30: integer (nullable = true)</span></span><br><span class="line">    <span class="string">| |-- question_cnt: integer (nullable = true)</span></span><br><span class="line">    <span class="string">| |-- answer_cnt: integer (nullable = true)</span></span><br><span class="line">    <span class="string">|&quot;&quot;&quot;</span>.stripMargin</span><br><span class="line"><span class="comment">//等 剩下的请看api简介</span></span><br><span class="line"></span><br><span class="line">df5.createOrReplaceTempView(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">|select</span></span><br><span class="line">    <span class="string">|gender,</span></span><br><span class="line">    <span class="string">|device_id,</span></span><br><span class="line">    <span class="string">|active_days_within_30,</span></span><br><span class="line">    <span class="string">|university</span></span><br><span class="line">    <span class="string">|from</span></span><br><span class="line">    <span class="string">|csv</span></span><br><span class="line">    <span class="string">|where university = &#x27;北京大学&#x27;</span></span><br><span class="line">    <span class="string">|&quot;&quot;&quot;</span>.stripMargin).show()</span><br><span class="line"></span><br><span class="line">result=</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">|+------+---------+---------------------+----------+</span></span><br><span class="line">    <span class="string">||gender|device_id|active_days_within_30|university|</span></span><br><span class="line">    <span class="string">|+------+---------+---------------------+----------+</span></span><br><span class="line">    <span class="string">||  male|     2138|                    7|  北京大学|</span></span><br><span class="line">    <span class="string">||female|     6543|                   12|  北京大学|</span></span><br><span class="line">    <span class="string">|+------+---------+---------------------+----------+</span></span><br><span class="line">    <span class="string">|&quot;&quot;&quot;</span>.stripMargin</span><br></pre></td></tr></table></figure>

<h3 id="jdbc-1"><a href="#jdbc-1" class="headerlink" title="jdbc"></a>jdbc</h3><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">val spark = SparkSession.builder().appName(<span class="string">&quot;Sparksql01&quot;</span>).master(<span class="string">&quot;local[4]&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">//用代码创建</span></span><br><span class="line">val df = spark.read.format(<span class="string">&quot;JDBC&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>,<span class="string">&quot;jdbc:mysql://bigdata2:3306/try&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;emp&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;liuzihan010616&quot;</span>)</span><br><span class="line">  .load()</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// 但是这样传入是把整个表直接传进来，但是有时候我们只要其中一一部分可以这样相当于谓词下压</span></span><br><span class="line">    val sal =</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">|select</span></span><br><span class="line">    <span class="string">|*</span></span><br><span class="line">    <span class="string">|from</span></span><br><span class="line">    <span class="string">|emp where sal &gt; 1500</span></span><br><span class="line">    <span class="string">|&quot;&quot;&quot;</span>.stripMargin</span><br><span class="line">val df1 = spark.read.format(<span class="string">&quot;JDBC&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>,<span class="string">&quot;jdbc:mysql://bigdata2:3306/try&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, s<span class="string">&quot;($sal) as tmp&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;liuzihan010616&quot;</span>)</span><br><span class="line">  .load()</span><br><span class="line">df1.show()</span><br><span class="line"> result =</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">|+-----+------+---------+----+-------------------+-------+-------+------+</span></span><br><span class="line">    <span class="string">||empno| ename|      job| mgr|           hiredate|    sal|   comm|deptno|</span></span><br><span class="line">    <span class="string">|+-----+------+---------+----+-------------------+-------+-------+------+</span></span><br><span class="line">    <span class="string">|| 7499| ALLEN| SALESMAN|7698|1981-02-20 00:00:00|1600.00| 300.00|    30|</span></span><br><span class="line">    <span class="string">|| 7566| JONES|  MANAGER|7839|1981-04-02 00:00:00|2975.00|   null|    20|</span></span><br><span class="line">    <span class="string">|| 7698| BLAKE|  MANAGER|7839|1981-05-01 00:00:00|2850.00|   null|    30|</span></span><br><span class="line">    <span class="string">|| 7782| CLARK|  MANAGER|7839|1981-06-09 00:00:00|2450.00|   null|    10|</span></span><br><span class="line">    <span class="string">|| 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|</span></span><br><span class="line">    <span class="string">|| 7839|  KING|PRESIDENT|null|1981-11-17 00:00:00|5000.00|   null|    10|</span></span><br><span class="line">    <span class="string">|| 7902|  FORD|  ANALYST|7566|1981-12-03 00:00:00|3000.00|   null|    20|</span></span><br><span class="line">    <span class="string">|| 7839|  KING|PRESIDENT|null|1981-11-17 00:00:00|5000.00|   null|    10|</span></span><br><span class="line">    <span class="string">|| 7654|MARTIN| SALESMAN|7698|1981-09-28 00:00:00|3200.00|1400.00|    30|</span></span><br><span class="line">    <span class="string">|| 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|</span></span><br><span class="line">    <span class="string">|| 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|</span></span><br><span class="line">    <span class="string">|| 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|</span></span><br><span class="line">    <span class="string">|| 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|</span></span><br><span class="line">    <span class="string">|| 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|</span></span><br><span class="line">    <span class="string">|| 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|</span></span><br><span class="line">    <span class="string">|| 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|</span></span><br><span class="line">    <span class="string">|| 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|</span></span><br><span class="line">    <span class="string">|| 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|</span></span><br><span class="line">    <span class="string">|| 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|</span></span><br><span class="line">    <span class="string">|| 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|</span></span><br><span class="line">    <span class="string">|+-----+------+---------+----+-------------------+-------+-------+------+</span></span><br><span class="line">    <span class="string">|only showing top 20 rows</span></span><br><span class="line">    <span class="string">|</span></span><br><span class="line">    <span class="string">|</span></span><br><span class="line">    <span class="string">|Process finished with exit code 0</span></span><br><span class="line">    <span class="string">|</span></span><br><span class="line">    <span class="string">|&quot;&quot;&quot;</span>.stripMargin</span><br><span class="line"><span class="comment">//用Properties传入</span></span><br><span class="line">val connectionProperties = new Properties()</span><br><span class="line">connectionProperties.put(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">connectionProperties.put(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;liuzihan010616&quot;</span>)</span><br><span class="line">val jdbcDF2 = spark.read</span><br><span class="line">  .jdbc(<span class="string">&quot;jdbc:mysql://bigdata2:3306/try&quot;</span>, <span class="string">&quot;try.emp&quot;</span>, connectionProperties)</span><br></pre></td></tr></table></figure>

<h3 id="excel-1"><a href="#excel-1" class="headerlink" title="excel"></a>excel</h3><p>在idea里要先导入spark-excel的pom：这里的版本要和scala对应上</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">&lt;dependency&gt;</span></span><br><span class="line">   <span class="section">&lt;groupId&gt;</span><span class="attribute">com</span>.crealytics&lt;/groupId&gt;</span><br><span class="line">   <span class="section">&lt;artifactId&gt;</span><span class="attribute">spark</span>-excel_2.<span class="number">12</span>&lt;/artifactId&gt;</span><br><span class="line">   <span class="section">&lt;version&gt;</span><span class="attribute">0</span>.<span class="number">14</span>.<span class="number">0</span>&lt;/version&gt;</span><br><span class="line"> <span class="section">&lt;/dependency&gt;</span></span><br></pre></td></tr></table></figure>

<p>然后代码</p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">package sparkfirst</span><br><span class="line"></span><br><span class="line">import java.util.Properties</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions._</span><br><span class="line">import com.crealytics.spark.excel._</span><br><span class="line">object sparksql2 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder().appName(<span class="string">&quot;Sparksql01&quot;</span>).master(<span class="string">&quot;local[4]&quot;</span>).getOrCreate()</span><br><span class="line">    val df = spark.read.excel(header = true,inferSchema = true).load(<span class="string">&quot;file:////C:\\Users\\dell\\Desktop\\2023届毕业设计题目-计算机-选题志愿表.xlsx&quot;</span>)</span><br><span class="line">    df.show()</span><br><span class="line">    val result =</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line">        <span class="string">|+--------------------------------------+--------+-------------------------------------+------+--------+--------+--------+</span></span><br><span class="line">        <span class="string">||2023届计算机科学与技术专业毕业设计选题|     _c1|                                  _c2|   _c3|     _c4|     _c5|     _c6|</span></span><br><span class="line">        <span class="string">|+--------------------------------------+--------+-------------------------------------+------+--------+--------+--------+</span></span><br><span class="line">        <span class="string">||                                  序号|指导老师|                                 题目|学生数|第一志愿|第二志愿|第三志愿|</span></span><br><span class="line">        <span class="string">||                                     1|  王海涛|          基于android的房产中介app...|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                     2|  王海涛|        基于android的酒店预约入住a...|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                     3|  王海涛|          基于android的有声书app的...|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                     4|  王海涛|          基于android的掌上医院app...|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                     5|  王海涛|    基于web的考试管理系统的设计与实现|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                     6|    王琢|           电商平台产品评论爬虫的设计|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                     7|    王琢|     基于Django的智能水务系统前端开发|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                     8|    王琢|           个人账本管理微信小程序开发|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                     9|    王琢|       智能水务系统远程监控模块的开发|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                    10|  张文波|基于安卓系统的硕士研究生招生预报名...|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                    11|  张文波|面向工业互联网的联网设备故障检测技...|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                    12|  张文波|面向工业互联网的联网设备运行维护系...|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                    13|    曹烨|     疫情防控管理信息系统的设计与开发|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                    14|    曹烨|             多线程下载器的设计与开发|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                    15|    曹烨|             坦克对战游戏的设计与开发|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                    16|    曹烨|           五子棋游戏大厅的设计与开发|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                    17|    杜焱|       疫情封闭人员及物资管理系统开发|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                    18|    杜焱|                   志愿者服务系统开发|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">||                                    19|    杜焱|         高校教师工作绩效管理系统开发|     1|    null|    null|    null|</span></span><br><span class="line">        <span class="string">|+--------------------------------------+--------+-------------------------------------+------+--------+--------+--------+</span></span><br><span class="line">        <span class="string">|only showing top 20 rows</span></span><br><span class="line">        <span class="string">|</span></span><br><span class="line">        <span class="string">|</span></span><br><span class="line">        <span class="string">|Process finished with exit code 0</span></span><br><span class="line">        <span class="string">|</span></span><br><span class="line">        <span class="string">|&quot;&quot;&quot;</span>.stripMargin</span><br><span class="line"></span><br></pre></td></tr></table></figure>




<h2 id="写数据"><a href="#写数据" class="headerlink" title="写数据"></a>写数据</h2><p>写数据的时候一般会伴随crc文件</p>
<h3 id="TEXT-2"><a href="#TEXT-2" class="headerlink" title="TEXT"></a>TEXT</h3><p>注意text仅仅支持一列的数据进行输出，不支持多列，因为我们的resource文件夹里有配置文件所以它走我们的配置文件压缩格式为bz2，不过可以自己指定格式，一般不指定且无配置文件是不压缩的</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">package sparkfirst</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="keyword">sql</span>.SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">object</span> sparksql2 &#123;</span><br><span class="line">  def main(args: <span class="keyword">Array</span>[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;Sparksql01&quot;).master(&quot;local[4]&quot;).getOrCreate()</span><br><span class="line">    val df2 = spark.<span class="keyword">read</span>.<span class="keyword">option</span>(&quot;lineSep&quot;,&quot;,&quot;).text(&quot;file:///D:\\test.txt&quot;)</span><br><span class="line">    df2.<span class="keyword">show</span>()</span><br><span class="line">    //<span class="comment">-----------------------------------------写数据</span></span><br><span class="line">    df2.<span class="keyword">write</span>.text(&quot;file:///D:\\test1.txt&quot;)</span><br><span class="line">    //<span class="comment">-------------------------------------------加压缩</span></span><br><span class="line">    df2.<span class="keyword">write</span>.<span class="keyword">option</span>(&quot;compression&quot;, &quot;gzip&quot;).text(&quot;file:///D:\\test2.txt&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如果想解决，要自己定义外部数据源，相当于自己修改源码</p>
<p>或者把dataframe变成rdd进行输出 因为saveasTextFile是可以多列输出的</p>
<p><code>df2.rdd.saveAsTextFile(&quot;file:///D:\\test3.txt&quot;)</code></p>
<p><img src="https://pic.imgdb.cn/item/63be1a9cbe43e0d30e184271.jpg"></p>
<p>查看文件格式</p>
<p><img src="https://pic.imgdb.cn/item/63be1ad7be43e0d30e18aa99.jpg"></p>
<h3 id="json-2"><a href="#json-2" class="headerlink" title="json"></a>json</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">//</span>常用的输出方式追加append，或者覆盖(overwrite),或者忽略（ignore），错误等（error）</span><br><span class="line"><span class="regexp">//</span>df.write.mode(saveMode = <span class="string">&quot;overwrite&quot;</span>).json(<span class="string">&quot;hdfs://bigdata3:9000/spark&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="csv-2"><a href="#csv-2" class="headerlink" title="csv"></a>csv</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//写出可以用sep进行设置导出的分隔符，mode设置是不是覆盖，compression设置压缩</span></span><br><span class="line">  df5<span class="selector-class">.write</span><span class="selector-class">.options</span>(<span class="built_in">Map</span>(<span class="string">&quot;sep&quot;</span>-&gt;<span class="string">&quot;;&quot;</span>,<span class="string">&quot;compression&quot;</span>-&gt;<span class="string">&quot;gzip&quot;</span>))<span class="selector-class">.mode</span>(<span class="string">&quot;overwrite&quot;</span>)<span class="selector-class">.format</span>(<span class="string">&quot;csv&quot;</span>)<span class="selector-class">.save</span>(<span class="string">&quot;file:///C:\\Users\\dell\\Desktop\\user_profile1.csv&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>结果如下</p>
<p><img src="https://pic.imgdb.cn/item/63be621fbe43e0d30e93423c.jpg"></p>
<h3 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//写出 --------------------------代码</span></span><br><span class="line">    <span class="comment">//如果用overwrite，会把之前的表删掉，然后重新建一个，表的数据结构会发生改变</span></span><br><span class="line">    df<span class="selector-class">.write</span><span class="selector-class">.mode</span>(<span class="string">&quot;append&quot;</span>)</span><br><span class="line">      <span class="selector-class">.format</span>(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">      <span class="selector-class">.option</span>(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://bigdata2:3306/try&quot;</span>)</span><br><span class="line">      <span class="selector-class">.option</span>(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;emp1&quot;</span>)</span><br><span class="line">      <span class="selector-class">.option</span>(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">      <span class="selector-class">.option</span>(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;liuzihan010616&quot;</span>)</span><br><span class="line">      <span class="selector-class">.save</span>()</span><br><span class="line">      <span class="comment">// --------------------------Properties</span></span><br><span class="line">    df<span class="selector-class">.write</span><span class="selector-class">.mode</span>(<span class="string">&quot;append&quot;</span>)</span><br><span class="line">      <span class="selector-class">.jdbc</span>(<span class="string">&quot;jdbc:mysql://bigdata2:3306/try&quot;</span>, <span class="string">&quot;emp1&quot;</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 可以在写的时候多创建列</span></span><br><span class="line">    df<span class="selector-class">.write</span><span class="selector-class">.mode</span>(<span class="string">&quot;append&quot;</span>)</span><br><span class="line">      <span class="selector-class">.option</span>(<span class="string">&quot;createTableColumnTypes&quot;</span>, <span class="string">&quot;name CHAR(64), comments VARCHAR(1024)&quot;</span>)</span><br><span class="line">      <span class="selector-class">.jdbc</span>(<span class="string">&quot;jdbc:mysql://bigdata2:3306/try&quot;</span>, <span class="string">&quot;try.emp&quot;</span>, connectionProperties)</span><br></pre></td></tr></table></figure>

<h3 id="excel-2"><a href="#excel-2" class="headerlink" title="excel"></a>excel</h3><p>如下</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df<span class="selector-class">.write</span><span class="selector-class">.mode</span>(<span class="string">&quot;overwrite&quot;</span>)<span class="selector-class">.excel</span>(<span class="selector-tag">header</span> = true,<span class="string">&quot;A1&quot;</span>)<span class="selector-class">.save</span>(<span class="string">&quot;file:///C:\\Users\\dell\\Desktop\\2023届毕业设计题目-计算机-选题志愿表1.xlsx&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      
   <div>
     <div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

   </div>
     
        <div class="reward-container">
  <div>你们的鼓励是对我最大的支持</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="liu zihang 微信支付">
        <p>微信支付</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>liu zihang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://zihang.fun/2023/01/11/1-11/" title="sparksql-2">http://zihang.fun/2023/01/11/1-11/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/images/wechat_channel.jpg">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>

     
    
      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/spark/" rel="tag"># spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/01/10/1-10/" rel="prev" title="sparksql">
      <i class="fa fa-chevron-left"></i> sparksql
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81NzQzNy8zMzkwMQ=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9E%84%E5%BB%BAdf"><span class="nav-number">1.</span> <span class="nav-text">构建df</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="nav-number">2.</span> <span class="nav-text">加载外部数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#api%E7%AE%80%E4%BB%8B"><span class="nav-number">2.1.</span> <span class="nav-text">api简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TEXT"><span class="nav-number">2.1.1.</span> <span class="nav-text">TEXT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#json"><span class="nav-number">2.1.2.</span> <span class="nav-text">json</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#csv"><span class="nav-number">2.1.3.</span> <span class="nav-text">csv</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#jdbc"><span class="nav-number">2.1.4.</span> <span class="nav-text">jdbc</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#excel"><span class="nav-number">2.1.5.</span> <span class="nav-text">excel</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%BB%E6%95%B0%E6%8D%AE"><span class="nav-number">2.2.</span> <span class="nav-text">读数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TEXT-1"><span class="nav-number">2.2.1.</span> <span class="nav-text">TEXT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#json-1"><span class="nav-number">2.2.2.</span> <span class="nav-text">json</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#csv-1"><span class="nav-number">2.2.3.</span> <span class="nav-text">csv</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#jdbc-1"><span class="nav-number">2.2.4.</span> <span class="nav-text">jdbc</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#excel-1"><span class="nav-number">2.2.5.</span> <span class="nav-text">excel</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%99%E6%95%B0%E6%8D%AE"><span class="nav-number">2.3.</span> <span class="nav-text">写数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TEXT-2"><span class="nav-number">2.3.1.</span> <span class="nav-text">TEXT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#json-2"><span class="nav-number">2.3.2.</span> <span class="nav-text">json</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#csv-2"><span class="nav-number">2.3.3.</span> <span class="nav-text">csv</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JDBC"><span class="nav-number">2.3.4.</span> <span class="nav-text">JDBC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#excel-2"><span class="nav-number">2.3.5.</span> <span class="nav-text">excel</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="liu zihang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">liu zihang</p>
  <div class="site-description" itemprop="description">只有努力不会辜负你</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">56</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      链接网站
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://baidu.com/" title="https:&#x2F;&#x2F;baidu.com" rel="noopener" target="_blank">百度</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://fishc.com.cn/" title="https:&#x2F;&#x2F;fishc.com.cn" rel="noopener" target="_blank">鱼C论坛</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">liu zihang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共181.2k字</span>
</div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'http://zihang.fun/2023/01/11/1-11/',]
      });
      });
  </script>

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
