<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/%E6%A0%91%E5%8F%B6_sleaves%20(1).png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/%E6%A0%91%E5%8F%B6_sleaves.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-flash.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zihang.fun","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":15,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="sparkstreaming用于实时计算的模块 &#x3D;》 sparkstreaming，structuredstreaming 流处理 ： 实时 实时 来一套数据处理一条 storm，flink 数据叫event 近实时 来一批数据处理 mini-batch sparkstreaming 数据会源源不断地来  批处理 ： 离线 代码或者程序处理一个批次的数据  例子：数据放在hdfs上，我们">
<meta property="og:type" content="article">
<meta property="og:title" content="sparksql-4">
<meta property="og:url" content="http://zihang.fun/2023/01/16/1-16/index.html">
<meta property="og:site_name" content="枫叶冢">
<meta property="og:description" content="sparkstreaming用于实时计算的模块 &#x3D;》 sparkstreaming，structuredstreaming 流处理 ： 实时 实时 来一套数据处理一条 storm，flink 数据叫event 近实时 来一批数据处理 mini-batch sparkstreaming 数据会源源不断地来  批处理 ： 离线 代码或者程序处理一个批次的数据  例子：数据放在hdfs上，我们">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic.imgdb.cn/item/63c4b657be43e0d30e191256.jpg">
<meta property="og:image" content="https://pic.imgdb.cn/item/63c4fa26be43e0d30e9044af.jpg">
<meta property="article:published_time" content="2023-01-16T00:39:10.373Z">
<meta property="article:modified_time" content="2023-01-25T07:28:48.059Z">
<meta property="article:author" content="liu zihang">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.imgdb.cn/item/63c4b657be43e0d30e191256.jpg">

<link rel="canonical" href="http://zihang.fun/2023/01/16/1-16/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>sparksql-4 | 枫叶冢</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss2.xml" title="枫叶冢" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
    <a target="_blank" rel="noopener" href="https://github.com/han2044953296" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#fff; color:#151513; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">枫叶冢</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zihang.fun/2023/01/16/1-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="liu zihang">
      <meta itemprop="description" content="只有努力不会辜负你">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="枫叶冢">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          sparksql-4
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-01-16 08:39:10" itemprop="dateCreated datePublished" datetime="2023-01-16T08:39:10+08:00">2023-01-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-25 15:28:48" itemprop="dateModified" datetime="2023-01-25T15:28:48+08:00">2023-01-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%97%A5%E5%BF%97/" itemprop="url" rel="index"><span itemprop="name">日志</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>25k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>22 分钟</span>
            </span>

        </div>
      </header>

    
    
    

   

    <div class="post-body" itemprop="articleBody">

      
        <h1 id="sparkstreaming"><a href="#sparkstreaming" class="headerlink" title="sparkstreaming"></a>sparkstreaming</h1><p>用于实时计算的模块 &#x3D;》 sparkstreaming，structuredstreaming</p>
<h2 id="流处理-：-实时"><a href="#流处理-：-实时" class="headerlink" title="流处理 ： 实时"></a>流处理 ： 实时</h2><ul>
<li>实时 来一套数据处理一条 storm，flink 数据叫event</li>
<li>近实时 来一批数据处理 mini-batch sparkstreaming</li>
<li>数据会源源不断地来</li>
</ul>
<h2 id="批处理-：-离线"><a href="#批处理-：-离线" class="headerlink" title="批处理 ： 离线"></a>批处理 ： 离线</h2><ul>
<li><p>代码或者程序处理一个批次的数据</p>
<ul>
<li>例子：数据放在hdfs上，我们对他进行处理 &#x3D;》 ok</li>
</ul>
</li>
</ul>
<h2 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h2><p>生产上：</p>
<ul>
<li>sparkstreaming，structuredstreaming 10%</li>
<li>flink 90%</li>
<li>storm 2%</li>
</ul>
<p>开发角度：</p>
<ul>
<li>code &#x3D;》 flink &gt; sparkstreaming</li>
<li>sql &#x3D;&gt; flink &gt; spark streaming</li>
</ul>
<p>业务：</p>
<ul>
<li>实施指标 ：都差不多</li>
<li>实时数仓：<ul>
<li>代码 ： 差不多</li>
<li>sql文件 ： flinksql维护实时数仓 &#x3D;》 ok</li>
</ul>
</li>
</ul>
<h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><p>容易使用 &#x3D;》 客观看</p>
<p>批流一体的处理方法 &#x3D;》 sparksql &lt;&#x3D;&gt; 流处理</p>
<p>低延迟高消费</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul>
<li>sparkstreaming开发是spark-core的一个扩展</li>
<li>接收数据的渠道多</li>
<li>还可以对数据进行流处理的可以机器学习等</li>
</ul>
<p>一般来说流式处理会比批处理负载小，但不绝对</p>
<h3 id="数据源-："><a href="#数据源-：" class="headerlink" title="数据源 ："></a>数据源 ：</h3><ul>
<li>kafka ****** 流式引擎重要的数据源 -》 通过topic进行数据缓冲，它会根据sp的吞吐量来进行处理，两个引擎之间会有联系</li>
<li>flume **** 可以使用但是一般不用 flume 没有数据缓冲 致命 -》 直接把数据弄到sp里，如果数据量特别多，会让sp程序挂掉，因为如果sa程序的吞吐量比较小，则会崩掉，和sp无联系</li>
<li>hdfs</li>
</ul>
<h3 id="数据积压：kafka数据太大，导致sp程序一直处理不过来，一个出不来报表-x3D-gt-解决方法"><a href="#数据积压：kafka数据太大，导致sp程序一直处理不过来，一个出不来报表-x3D-gt-解决方法" class="headerlink" title="数据积压：kafka数据太大，导致sp程序一直处理不过来，一个出不来报表 &#x3D;&gt;解决方法"></a>数据积压：kafka数据太大，导致sp程序一直处理不过来，一个出不来报表 &#x3D;&gt;解决方法</h3><ul>
<li>吞吐量提高</li>
<li>数据量减少</li>
</ul>
<h3 id="sparkstreaming运行机制"><a href="#sparkstreaming运行机制" class="headerlink" title="sparkstreaming运行机制"></a>sparkstreaming运行机制</h3><ul>
<li>接收数据</li>
<li>拆分成batches</li>
</ul>
<h3 id="sparkstreaming-gt-kafka-："><a href="#sparkstreaming-gt-kafka-：" class="headerlink" title="sparkstreaming -&gt; kafka ："></a>sparkstreaming -&gt; kafka ：</h3><ul>
<li>5s处理数据</li>
<li>每5s会切分成一次batch</li>
<li>交给sparkngine处理</li>
<li>处理完的也是一个batch</li>
</ul>
<h3 id="sparkstreaming编程模型：Dstream"><a href="#sparkstreaming编程模型：Dstream" class="headerlink" title="sparkstreaming编程模型：Dstream"></a>sparkstreaming编程模型：Dstream</h3><ul>
<li>外部数据源</li>
<li>高级算子</li>
<li>类似RDD</li>
</ul>
<h3 id="idea开发先配置pom文件"><a href="#idea开发先配置pom文件" class="headerlink" title="idea开发先配置pom文件"></a>idea开发先配置pom文件</h3><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">&lt;dependency&gt;</span></span><br><span class="line">  <span class="section">&lt;groupId&gt;</span><span class="attribute">org</span>.apache.spark&lt;/groupId&gt;</span><br><span class="line">  <span class="section">&lt;artifactId&gt;</span><span class="attribute">spark</span>-streaming_2.<span class="number">12</span>&lt;/artifactId&gt;</span><br><span class="line">  <span class="section">&lt;version&gt;</span><span class="attribute">3</span>.<span class="number">2</span>.<span class="number">1</span>&lt;/version&gt;</span><br><span class="line"><span class="section">&lt;/dependency&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="idea代码"><a href="#idea代码" class="headerlink" title="idea代码"></a>idea代码</h3><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">package sparkstreaming</span><br><span class="line">import org.apache.spark._</span><br><span class="line">import org.apache.spark.streaming._</span><br><span class="line">import org.apache.spark.streaming.<span class="module-access"><span class="module"><span class="identifier">StreamingContext</span>.</span><span class="module"><span class="identifier">_</span></span></span></span><br><span class="line"><span class="module"><span class="module-access">i</span></span>mport org.apache.spark.SparkConf</span><br><span class="line"></span><br><span class="line"><span class="keyword">object</span> sparkstreaming1 &#123;</span><br><span class="line">  def main(args: Array<span class="literal">[S<span class="identifier">tring</span>]</span>): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="constructor">SparkConf()</span>.set<span class="constructor">Master(<span class="string">&quot;local[4]&quot;</span>)</span>.set<span class="constructor">AppName(<span class="string">&quot;NetworkWordCount&quot;</span>)</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="constructor">StreamingContext(<span class="params">conf</span>, Seconds(5)</span>)</span><br><span class="line">    <span class="comment">// 或者通过sparkcontext进行创建</span></span><br><span class="line">    <span class="comment">//val ssc = new StreamingContext(sc, Seconds(1))</span></span><br><span class="line">    <span class="comment">// 数据源</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socket<span class="constructor">TextStream(<span class="string">&quot;bigdata5&quot;</span>, 9999)</span></span><br><span class="line">    <span class="comment">// 处理数据</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flat<span class="constructor">Map(<span class="params">_</span>.<span class="params">split</span>(<span class="string">&quot; &quot;</span>)</span>)</span><br><span class="line">    <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> wordCounts = pairs.reduce<span class="constructor">ByKey(<span class="params">_</span> + <span class="params">_</span>)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印数据当前批次</span></span><br><span class="line">    wordCounts.print<span class="literal">()</span></span><br><span class="line">    ssc.start<span class="literal">()</span>             <span class="comment">// Start</span></span><br><span class="line">    ssc.await<span class="constructor">Termination()</span>  <span class="comment">// Wait</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置数据源在目标机器上执行nc -lk 9999 然后输入数据就ok了</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>还可以在webui上查看</p>
<p>如下：</p>
<p><img src="https://pic.imgdb.cn/item/63c4b657be43e0d30e191256.jpg"></p>
<p>他的打印数据是处理当前批次的数据，不是累积批次的数据</p>
<h3 id="双流join"><a href="#双流join" class="headerlink" title="双流join"></a>双流join</h3><p>api :</p>
<ul>
<li>flink -》调用api</li>
<li>sparkstreaming code 很多 -》 api join stste</li>
</ul>
<p> 延迟数据</p>
<ul>
<li>processtime + udf</li>
<li>eventime + watermaker<ul>
<li>数据和离线对不上（容易）</li>
</ul>
</li>
</ul>
<p>如何构建DStream</p>
<ul>
<li>从inputstream的方式 生产上</li>
<li>receiver 测试用 为面试准备</li>
</ul>
<h2 id="构建Dstream"><a href="#构建Dstream" class="headerlink" title="构建Dstream"></a>构建Dstream</h2><h3 id="inputstrteam"><a href="#inputstrteam" class="headerlink" title="inputstrteam"></a>inputstrteam</h3><p>比如卡夫卡</p>
<h3 id="receiver"><a href="#receiver" class="headerlink" title="receiver"></a>receiver</h3><p>用receiver接受的时候如果是本地则要大于1 -&gt; local[2+]</p>
<p>因为sparkstreaming最少是有两部分切分以及处理，如果只给1则会没有资源进行处理</p>
<p>所以针对于receiver一个要大于等于</p>
<p>上面仅仅是针对receiver</p>
<p>例子 ：</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lines = ssc.socket<span class="constructor">TextStream(<span class="string">&quot;bigdata5&quot;</span>, 9999)</span></span><br></pre></td></tr></table></figure>

<p>因为他底层源码是</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def <span class="title function_">socketTextStream</span>(</span><br><span class="line">    <span class="attr">hostname</span>: <span class="title class_">String</span>,</span><br><span class="line">    <span class="attr">port</span>: <span class="title class_">Int</span>,</span><br><span class="line">    <span class="attr">storageLevel</span>: <span class="title class_">StorageLevel</span> = <span class="title class_">StorageLevel</span>.<span class="property">MEMORY_AND_DISK_SER_2</span></span><br><span class="line">  ): <span class="title class_">ReceiverInputDStream</span>[<span class="title class_">String</span>] = <span class="title function_">withNamedScope</span>(<span class="params"><span class="string">&quot;socket text stream&quot;</span></span>) &#123;</span><br><span class="line">  socketStream[<span class="title class_">String</span>](hostname, port, <span class="title class_">SocketReceiver</span>.<span class="property">bytesToLines</span>, storageLevel)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Dstream算子"><a href="#Dstream算子" class="headerlink" title="Dstream算子"></a>Dstream算子</h2><p>转换操作：</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Similar</span> <span class="keyword">to</span> that <span class="keyword">of</span> RDDs, transformations allow the data <span class="keyword">from</span> the <span class="keyword">input</span> DStream <span class="keyword">to</span> be modified. DStreams support many <span class="keyword">of</span> the transformations available <span class="keyword">on</span> normal Spark RDD’s. <span class="keyword">Some</span> <span class="keyword">of</span> the common ones are <span class="keyword">as</span> follows.</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>Transformation</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td><strong>map</strong> ( <em>func</em> )</td>
<td>Return a new DStream by passing each element of the source DStream through a function<em>func</em> .</td>
</tr>
<tr>
<td><strong>flatMap</strong> ( <em>func</em> )</td>
<td>Similar to map, but each input item can be mapped to 0 or more output items.</td>
</tr>
<tr>
<td><strong>filter</strong> ( <em>func</em> )</td>
<td>Return a new DStream by selecting only the records of the source DStream on which<em>func</em> returns true.</td>
</tr>
<tr>
<td><strong>repartition</strong> ( <em>numPartitions</em> )</td>
<td>Changes the level of parallelism in this DStream by creating more or fewer partitions.</td>
</tr>
<tr>
<td><strong>union</strong> ( <em>otherStream</em> )</td>
<td>Return a new DStream that contains the union of the elements in the source DStream and<em>otherDStream</em> .</td>
</tr>
<tr>
<td><strong>count</strong> ()</td>
<td>Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.</td>
</tr>
<tr>
<td><strong>reduce</strong> ( <em>func</em> )</td>
<td>Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function<em>func</em> (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.</td>
</tr>
<tr>
<td><strong>countByValue</strong> ()</td>
<td>When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.</td>
</tr>
<tr>
<td><strong>reduceByKey</strong> ( <em>func</em> , [ <em>numTasks</em> ])</td>
<td>When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function.<strong>Note:</strong> By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property <code>spark.default.parallelism</code>) to do the grouping. You can pass an optional <code>numTasks</code> argument to set a different number of tasks.</td>
</tr>
<tr>
<td><strong>join</strong> ( <em>otherStream</em> , [ <em>numTasks</em> ])</td>
<td>When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.</td>
</tr>
<tr>
<td><strong>cogroup</strong> ( <em>otherStream</em> , [ <em>numTasks</em> ])</td>
<td>When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.</td>
</tr>
<tr>
<td><strong>transform</strong> ( <em>func</em> )</td>
<td>Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary RDD operations on the DStream.</td>
</tr>
<tr>
<td><strong>updateStateByKey</strong> ( <em>func</em> )</td>
<td>Return a new “state” DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.</td>
</tr>
</tbody></table>
<p>输出操作：</p>
<table>
<thead>
<tr>
<th>Output Operation</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td><strong>print</strong> ()</td>
<td>Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging.&#96;&#96;Python API This is called<strong>pprint()</strong> in the Python API.</td>
</tr>
<tr>
<td><strong>saveAsTextFiles</strong> ( <em>prefix</em> , [ <em>suffix</em> ])</td>
<td>Save this DStream’s contents as text files. The file name at each batch interval is generated based on<em>prefix</em> and  <em>suffix</em> :  <em>“prefix-TIME_IN_MS[.suffix]”</em> .</td>
</tr>
<tr>
<td><strong>saveAsObjectFiles</strong> ( <em>prefix</em> , [ <em>suffix</em> ])</td>
<td>Save this DStream’s contents as <code>SequenceFiles</code> of serialized Java objects. The file name at each batch interval is generated based on <em>prefix</em> and  <em>suffix</em> :  <em>“prefix-TIME_IN_MS[.suffix]”</em> .&#96;&#96;Python API This is not available in the Python API.</td>
</tr>
<tr>
<td><strong>saveAsHadoopFiles</strong> ( <em>prefix</em> , [ <em>suffix</em> ])</td>
<td>Save this DStream’s contents as Hadoop files. The file name at each batch interval is generated based on<em>prefix</em> and  <em>suffix</em> :  <em>“prefix-TIME_IN_MS[.suffix]”</em> .&#96;&#96;Python API This is not available in the Python API.</td>
</tr>
<tr>
<td><strong>foreachRDD</strong> ( <em>func</em> )</td>
<td>The most generic output operator that applies a function,<em>func</em> , to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function <em>func</em> is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs.</td>
</tr>
</tbody></table>
<p>我们之前的计算代码只是计算当前批次的数据，也是sparkstreaming默认的</p>
<p>基于上面官方提出了状态</p>
<h2 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h2><ul>
<li>有状态 前后批次有联系</li>
<li>无状态 前后批次无联系</li>
</ul>
<p>用于解决统计类问题</p>
<p><strong>updateStateByKey</strong> ( <em>func</em> )：这个算子</p>
<figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def updateFunction(<span class="keyword">new</span><span class="type">Values</span>: Seq[<span class="keyword">Int</span>], runningCount: <span class="type">Option</span>[<span class="keyword">Int</span>]): <span class="type">Option</span>[<span class="keyword">Int</span>] = &#123;</span><br><span class="line">    val <span class="keyword">new</span><span class="type">Count</span> = ...  <span class="comment">// add the new values with the previous running count to get the new count</span></span><br><span class="line">    Some(<span class="keyword">new</span><span class="type">Count</span>)</span><br><span class="line">&#125;</span><br><span class="line">val runningCounts = pairs.updateStateByKey[<span class="keyword">Int</span>](updateFunction <span class="literal">_</span>)</span><br></pre></td></tr></table></figure>

<p>代码如下：</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">package sparkstreaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.StreamingContext._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf</span><br><span class="line"><span class="keyword">import</span> tool.streamingcontext</span><br><span class="line"><span class="keyword">object</span> sparkstreaming1 &#123;</span><br><span class="line">  private val streamingcontext = <span class="built_in">new</span> streamingcontext</span><br><span class="line">  def main(args: <span class="keyword">Array</span>[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = streamingcontext.getstreamcotext()</span><br><span class="line">    // 或者通过sparkcontext进行创建</span><br><span class="line">    //val ssc = <span class="built_in">new</span> StreamingContext(sc, Seconds(<span class="number">1</span>))</span><br><span class="line">    // 数据源</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;bigdata5&quot;, <span class="number">9999</span>)</span><br><span class="line">    // 处理数据</span><br><span class="line">    val words = lines.flatMap(_.split(&quot; &quot;))</span><br><span class="line">    val pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">    val wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line">    // 要指定<span class="keyword">checkpoint</span>目录</span><br><span class="line">    ssc.<span class="keyword">checkpoint</span>(&quot;file:///D:\\checkpoint&quot;)</span><br><span class="line">    val totalwc = pairs.updateStateByKey(updateFunction _)</span><br><span class="line">    //wordCounts.updateStateByKey()</span><br><span class="line">    // 打印数据当前批次</span><br><span class="line">    wordCounts.print()</span><br><span class="line">    totalwc.print()</span><br><span class="line">    ssc.<span class="keyword">start</span>()             // <span class="keyword">Start</span></span><br><span class="line">    ssc.awaitTermination()  // Wait</span><br><span class="line"></span><br><span class="line">    // 配置数据源在目标机器上执行nc -lk <span class="number">9999</span> 然后输入数据就ok了</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def updateFunction(newValues: Seq[<span class="type">Int</span>], runningCount: <span class="keyword">Option</span>[<span class="type">Int</span>]): <span class="keyword">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">      // <span class="keyword">add</span> the <span class="built_in">new</span> <span class="keyword">values</span> <span class="keyword">with</span> the previous running count <span class="keyword">to</span> <span class="keyword">get</span> the <span class="built_in">new</span> count</span><br><span class="line">    val sum = newValues.sum</span><br><span class="line">    val i = runningCount.getOrElse(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">Some</span>(sum+i)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>但是这样也产生了个新问题</p>
<p>我们观察checkpoint文件夹</p>
<p><img src="https://pic.imgdb.cn/item/63c4fa26be43e0d30e9044af.jpg"></p>
<p>生成很多个小文件</p>
<p>我们该如何解决</p>
<p>生产上我们不用</p>
<p>但是必备的知识还是要的</p>
<p>为了容错，恢复作业，和kafka里的一样</p>
<h2 id="checkpoint的存储东西"><a href="#checkpoint的存储东西" class="headerlink" title="checkpoint的存储东西"></a>checkpoint的存储东西</h2><p>matestore 元数据</p>
<ul>
<li>conf 作业里的配置信息</li>
<li>算子操作</li>
<li>未完成的批次</li>
</ul>
<p>Data</p>
<ul>
<li>就是批次的数据</li>
</ul>
<p>使用场景</p>
<ul>
<li>作业失败的时候回复的时候用</li>
<li>转换算子的时候</li>
</ul>
<p>但是注意生产上用不了</p>
<p>如何使用</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Checkpointing can be enabled <span class="keyword">by</span> setting a directory <span class="keyword">in</span> a fault-tolerant, reliable file <span class="keyword">system</span> (e.g., HDFS, S3, etc.) <span class="keyword">to</span> which the <span class="keyword">checkpoint</span> information will be saved. This <span class="keyword">is</span> done <span class="keyword">by</span> <span class="keyword">using</span> streamingContext.<span class="keyword">checkpoint</span>(checkpointDirectory). This will allow you <span class="keyword">to</span> use the aforementioned stateful transformations. Additionally, <span class="keyword">if</span> you want <span class="keyword">to</span> make the application recover <span class="keyword">from</span> driver failures, you should rewrite your streaming application <span class="keyword">to</span> have the <span class="keyword">following</span> behavior.</span><br><span class="line"></span><br><span class="line"><span class="keyword">When</span> the program <span class="keyword">is</span> being started <span class="keyword">for</span> the first <span class="type">time</span>, it will <span class="keyword">create</span> a <span class="built_in">new</span> StreamingContext, <span class="keyword">set</span> up <span class="keyword">all</span> the streams <span class="keyword">and</span> <span class="keyword">then</span> <span class="keyword">call</span> <span class="keyword">start</span>().</span><br><span class="line"><span class="keyword">When</span> the program <span class="keyword">is</span> being restarted <span class="keyword">after</span> failure, it will re-<span class="keyword">create</span> a StreamingContext <span class="keyword">from</span> the <span class="keyword">checkpoint</span> data <span class="keyword">in</span> the <span class="keyword">checkpoint</span> directory.</span><br></pre></td></tr></table></figure>

<p>idea代码</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Function to create and setup a new StreamingContext</span></span><br><span class="line">def <span class="keyword">function</span><span class="constructor">ToCreateContext()</span>: StreamingContext = &#123;</span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="constructor">StreamingContext(<span class="operator">...</span>)</span>   <span class="comment">// new context</span></span><br><span class="line">  <span class="keyword">val</span> lines = ssc.socket<span class="constructor">TextStream(<span class="operator">...</span>)</span> <span class="comment">// create DStreams</span><span class="operator"></span></span><br><span class="line"><span class="operator">  ...</span></span><br><span class="line"><span class="operator">  </span>ssc.checkpoint(checkpointDirectory)   <span class="comment">// set checkpoint directory</span></span><br><span class="line">  ssc</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get StreamingContext from checkpoint data or create a new one</span></span><br><span class="line"><span class="keyword">val</span> context = <span class="module-access"><span class="module"><span class="identifier">StreamingContext</span>.</span></span>get<span class="constructor">OrCreate(<span class="params">checkpointDirectory</span>, <span class="params">functionToCreateContext</span> <span class="params">_</span>)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Do additional setup on context that needs to be done,</span></span><br><span class="line"><span class="comment">// irrespective of whether it is being started or restarted</span></span><br><span class="line">context.<span class="operator"> ...</span></span><br><span class="line"><span class="operator"></span></span><br><span class="line"><span class="operator"></span><span class="comment">// Start the context</span></span><br><span class="line">context.start<span class="literal">()</span></span><br><span class="line">context.await<span class="constructor">Termination()</span></span><br></pre></td></tr></table></figure>

<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>小文件</p>
<p>修改代码就费了，就要重整</p>
<p>checkpoint用不了-》累计批次指标问题 -》 出现问题</p>
<h2 id="如何实现相同功能？"><a href="#如何实现相同功能？" class="headerlink" title="如何实现相同功能？"></a>如何实现相同功能？</h2><p>实现存储到外部，如何根据而外部文件进行累计</p>
<h2 id="使用checkpoint"><a href="#使用checkpoint" class="headerlink" title="使用checkpoint"></a>使用checkpoint</h2><p>解决checkpoint修改代码报错和小文件问题</p>
<p>所以简历上不可以出现我在生产上用过updateStateByKey，坚决不会用</p>
<h2 id="如何把处理好的数据存储到外部"><a href="#如何把处理好的数据存储到外部" class="headerlink" title="如何把处理好的数据存储到外部"></a>如何把处理好的数据存储到外部</h2><p>如下：</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dstream.foreachRDD &#123;<span class="function"> <span class="params">rdd</span> =&gt;</span></span><br><span class="line">  <span class="keyword">val</span> connection = create<span class="constructor">NewConnection()</span>  <span class="comment">// executed at the driver</span></span><br><span class="line">  rdd.foreach &#123;<span class="function"> <span class="params">record</span> =&gt;</span></span><br><span class="line">    connection.send(record) <span class="comment">// executed at the worker</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>idea</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> sparkstreaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.StreamingContext._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession</span><br><span class="line"><span class="keyword">import</span> tool.&#123;mysqlutils, streamingcontext,savefile&#125;</span><br><span class="line">object sparkstreaming1 &#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">val</span> <span class="variable">mysqlutils</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">mysqlutils</span></span><br><span class="line">  <span class="keyword">private</span> <span class="type">val</span> <span class="variable">streamingcontext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">streamingcontext</span></span><br><span class="line">  <span class="keyword">private</span> <span class="type">val</span> <span class="variable">savefile</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">savefile</span></span><br><span class="line">  def <span class="title function_">main</span><span class="params">(args: Array[String])</span>: Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="type">val</span> <span class="variable">ssc</span> <span class="operator">=</span> streamingcontext.getstreamcotext()</span><br><span class="line">    <span class="comment">// 或者通过sparkcontext进行创建</span></span><br><span class="line">    <span class="comment">//val ssc = new StreamingContext(sc, Seconds(1))</span></span><br><span class="line">    <span class="comment">// 数据源</span></span><br><span class="line">    <span class="type">val</span> <span class="variable">lines</span> <span class="operator">=</span> ssc.socketTextStream(<span class="string">&quot;bigdata5&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="comment">// 处理数据</span></span><br><span class="line">    <span class="type">val</span> <span class="variable">words</span> <span class="operator">=</span> lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    <span class="type">val</span> <span class="variable">pairs</span> <span class="operator">=</span> words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">    <span class="type">val</span> <span class="variable">wordCounts</span> <span class="operator">=</span> pairs.reduceByKey(_ + _)</span><br><span class="line">    <span class="comment">//val totalwc = pairs.updateStateByKey(updateFunction _)</span></span><br><span class="line">    <span class="comment">//wordCounts.updateStateByKey()</span></span><br><span class="line">    <span class="comment">// 打印数据当前批次</span></span><br><span class="line">    wordCounts.print()</span><br><span class="line">    <span class="comment">//totalwc.print()</span></span><br><span class="line">    <span class="comment">// 把结果输入到mysql里 先在mysql里创建完表了</span></span><br><span class="line">    <span class="comment">// 下面会报错-&gt; mysql链接没有进行序列化 ，我们不能加除非更改底层源码</span></span><br><span class="line">    <span class="comment">// closure 闭包 -&gt; 方法内使用了方法外的变量 比如下述的connect</span></span><br><span class="line">    wordCounts.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">      <span class="type">val</span> <span class="variable">connection</span> <span class="operator">=</span> mysqlutils.getconnect(<span class="string">&quot;jdbc:mysql://bigdata2:3306/bigdata&quot;</span>, <span class="string">&quot;root&quot;</span>, <span class="string">&quot;liuzihan010616&quot;</span>)</span><br><span class="line">      rdd.foreach &#123; record =&gt;</span><br><span class="line">        <span class="type">val</span> <span class="variable">sql</span> <span class="operator">=</span> s<span class="string">&quot;insert into wc values(&#x27;$&#123;record._1&#125;&#x27;,&#x27;$&#123;record._2&#125;&#x27;)&quot;</span></span><br><span class="line">        connection.createStatement.execute(sql)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="comment">// --------------------------------------------------------</span></span><br><span class="line">    <span class="comment">//对上述进行修改之后</span></span><br><span class="line">    <span class="comment">//这样是可以的但是性能不高</span></span><br><span class="line">    <span class="comment">//因为会一直拿链接，会造成性能下降</span></span><br><span class="line">        wordCounts.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">          rdd.foreach &#123; record =&gt;</span><br><span class="line">            <span class="type">val</span> <span class="variable">sql</span> <span class="operator">=</span> s<span class="string">&quot;insert into wc values(&#x27;$&#123;record._1&#125;&#x27;,&#x27;$&#123;record._2&#125;&#x27;)&quot;</span></span><br><span class="line">            <span class="type">val</span> <span class="variable">connection</span> <span class="operator">=</span> mysqlutils.getconnect(<span class="string">&quot;jdbc:mysql://bigdata2:3306/bigdata&quot;</span>, <span class="string">&quot;root&quot;</span>, <span class="string">&quot;liuzihan010616&quot;</span>)</span><br><span class="line">            connection.createStatement.execute(sql)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    <span class="comment">//优化性能</span></span><br><span class="line">    wordCounts.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">      rdd.foreachPartition(record=&gt;&#123;</span><br><span class="line">        <span class="type">val</span> <span class="variable">connection</span> <span class="operator">=</span> mysqlutils.getconnect(<span class="string">&quot;jdbc:mysql://bigdata2:3306/bigdata&quot;</span>, <span class="string">&quot;root&quot;</span>, <span class="string">&quot;liuzihan010616&quot;</span>)</span><br><span class="line">        record.foreach(pari =&gt; &#123;</span><br><span class="line">          <span class="type">val</span> <span class="variable">sql</span> <span class="operator">=</span> s<span class="string">&quot;insert into wc values(&#x27;$&#123;pari._1&#125;&#x27;,&#x27;$&#123;pari._2&#125;&#x27;)&quot;</span></span><br><span class="line">          connection.createStatement.execute(sql)</span><br><span class="line">        &#125;)</span><br><span class="line">        mysqlutils.closeconnect(connection)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">// 再次进行优化 原因 -》 partition的数量过高</span></span><br><span class="line">    <span class="comment">// 通过连接池来进行</span></span><br><span class="line">    <span class="comment">// 或者通过coalse来控制这个分区数量</span></span><br><span class="line">    dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">      rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">        <span class="comment">// ConnectionPool is a static, lazily initialized pool of connections</span></span><br><span class="line">        <span class="type">val</span> <span class="variable">connection</span> <span class="operator">=</span> ConnectionPool.getConnection()</span><br><span class="line">        partitionOfRecords.foreach(record =&gt; connection.send(record))</span><br><span class="line">        ConnectionPool.returnConnection(connection)  <span class="comment">// return to the pool for future reuse</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 利用sparksql的方式写入 最推荐</span></span><br><span class="line">    <span class="comment">// 性能也很好因为用的是spark的</span></span><br><span class="line">    wordCounts.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">      <span class="type">val</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate()</span><br><span class="line">      <span class="keyword">import</span> spark.implicits._</span><br><span class="line">      <span class="comment">// Convert RDD[String] to DataFrame</span></span><br><span class="line">      <span class="type">val</span> <span class="variable">wordsDataFrame</span> <span class="operator">=</span> rdd.toDF(<span class="string">&quot;word&quot;</span>,<span class="string">&quot;cnt&quot;</span>)</span><br><span class="line">      val srray:Array[String] = Array(<span class="string">&quot;append&quot;</span>,<span class="string">&quot;jdbc:mysql://bigdata2:3306/bigdata&quot;</span>,<span class="string">&quot;root&quot;</span>,<span class="string">&quot;liuzihan010616&quot;</span>,<span class="string">&quot;wc&quot;</span>,<span class="string">&quot;word&quot;</span>)</span><br><span class="line">      savefile.savetojdbc(spark,wordsDataFrame,srray)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()             <span class="comment">// Start</span></span><br><span class="line">    ssc.awaitTermination()  <span class="comment">// Wait</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置数据源在目标机器上执行nc -lk 9999 然后输入数据就ok了</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def <span class="title function_">updateFunction</span><span class="params">(newValues: Seq[Int], runningCount: Option[Int])</span>: Option[Int] = &#123;</span><br><span class="line">      <span class="comment">// add the new values with the previous running count to get the new count</span></span><br><span class="line">    <span class="type">val</span> <span class="variable">sum</span> <span class="operator">=</span> newValues.sum</span><br><span class="line">    <span class="type">val</span> <span class="variable">i</span> <span class="operator">=</span> runningCount.getOrElse(<span class="number">0</span>)</span><br><span class="line">    Some(sum+i)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="transform"><a href="#transform" class="headerlink" title="transform"></a>transform</h2><h3 id="DStream和RDD之间交互的算子"><a href="#DStream和RDD之间交互的算子" class="headerlink" title="DStream和RDD之间交互的算子"></a>DStream和RDD之间交互的算子</h3><p>需求 ：</p>
<ul>
<li>一个数据是来自于mysql&#x2F;文本数据 ： 量小 伪表</li>
<li>一个数据 来自kafka sss读取形成的DStream 量大 主业务线</li>
</ul>
<p>实例：弹幕过滤功能</p>
<ul>
<li>离线</li>
<li>实时</li>
</ul>
<h3 id="数据如下"><a href="#数据如下" class="headerlink" title="数据如下"></a>数据如下</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">主表：</span><br><span class="line">不好看</span><br><span class="line">垃圾</span><br><span class="line">女主真好看</span><br><span class="line">666</span><br><span class="line">过滤的弹幕：</span><br><span class="line">热巴真丑</span><br><span class="line">鸡儿真美</span><br><span class="line">王退出娱乐圈</span><br></pre></td></tr></table></figure>

<h3 id="离线"><a href="#离线" class="headerlink" title="离线:"></a>离线:</h3><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> sparkstreaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession</span><br><span class="line"><span class="keyword">import</span> tool._</span><br><span class="line"><span class="keyword">object</span> sparkstreaming2 &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> spark = SparkSession.builder().appName(<span class="string">&quot;Sparksql01&quot;</span>).master(<span class="string">&quot;local[4]&quot;</span>).enableHiveSupport().getOrCreate()</span><br><span class="line">  def main(args: Array[String]): <span class="built_in">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> mainsql = List(</span><br><span class="line">      <span class="string">&quot;不好看&quot;</span>,</span><br><span class="line">      <span class="string">&quot;垃圾&quot;</span>,</span><br><span class="line">      <span class="string">&quot;女主真好看&quot;</span>,</span><br><span class="line">      <span class="string">&quot;666&quot;</span>,</span><br><span class="line">      <span class="string">&quot;热巴真丑&quot;</span>,</span><br><span class="line">      <span class="string">&quot;鸡儿真美&quot;</span>,</span><br><span class="line">      <span class="string">&quot;王退出娱乐圈&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> maintable = spark.sparkContext.parallelize(mainsql)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> black = List(</span><br><span class="line">      <span class="string">&quot;热巴真丑&quot;</span>,</span><br><span class="line">        <span class="string">&quot;鸡儿真美&quot;</span>,</span><br><span class="line">      <span class="string">&quot;王退出娱乐圈&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> blacktable = spark.sparkContext.parallelize(black)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> value1 = maintable.map(x =&gt; &#123;</span><br><span class="line">      (x, <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">val</span> value = blacktable.map(x =&gt; &#123;</span><br><span class="line">      (x, <span class="literal">true</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">    value1.leftOuterJoin(value).filter(_._2._2.getOrElse(<span class="literal">false</span>)!=<span class="literal">true</span>).map(_._1).foreach(println(_))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="实时："><a href="#实时：" class="headerlink" title="实时："></a>实时：</h3><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">private val streamingcontext = <span class="keyword">new</span> streamingcontext</span><br><span class="line">def main(args: <span class="built_in">Array</span>[String]): Unit = &#123;</span><br><span class="line">  val ssc = streamingcontext.getstreamcotext()</span><br><span class="line">  val maintable = ssc.socketTextStream(<span class="string">&quot;bigdata5&quot;</span>, <span class="number">9099</span>)</span><br><span class="line">  <span class="keyword">var</span> black = List(</span><br><span class="line">    <span class="string">&quot;热巴真丑&quot;</span>,</span><br><span class="line">    <span class="string">&quot;鸡儿真美&quot;</span>,</span><br><span class="line">    <span class="string">&quot;王退出娱乐圈&quot;</span></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  val blacktable = ssc.sparkContext.parallelize(black)</span><br><span class="line"></span><br><span class="line">  val value = blacktable.<span class="built_in">map</span>(<span class="function"><span class="params">x</span> =&gt;</span> &#123;</span><br><span class="line">    (x, <span class="literal">true</span>)</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  val value1 = maintable.<span class="built_in">map</span>(<span class="function"><span class="params">x</span> =&gt;</span> &#123;</span><br><span class="line">    (x, <span class="number">1</span>)</span><br><span class="line">  &#125;)</span><br><span class="line">  val value2 = value1.transform(<span class="function"><span class="params">x</span> =&gt;</span> &#123;</span><br><span class="line">    x.leftOuterJoin(value).<span class="built_in">filter</span>(_._2._2.getOrElse(<span class="literal">false</span>) != <span class="literal">true</span>).<span class="built_in">map</span>(_._1)</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  value2.print()</span><br><span class="line">  </span><br><span class="line">  ssc.start()</span><br><span class="line">  ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<h2 id="sparkstreaming和kafka整合"><a href="#sparkstreaming和kafka整合" class="headerlink" title="sparkstreaming和kafka整合"></a>sparkstreaming和kafka整合</h2><p>通过receiver方式读取kafka数据</p>
<p>kafka版本我们选择的是2.2.1</p>
<p>在sparkstreaming里默认的时候是至少一次</p>
<p>spark消费kafka数据形成的DStream里的分区数量和Kafka里的topic的分区数是一一对应的</p>
<p>分区数和task数是一一对应的-》并行度一一对应</p>
<p>官网：<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration">kafkaonspark</a></p>
<p>idea里的依赖</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">&lt;dependency&gt;</span></span><br><span class="line">  <span class="section">&lt;groupId&gt;</span><span class="attribute">org</span>.apache.spark&lt;/groupId&gt;</span><br><span class="line">  <span class="section">&lt;artifactId&gt;</span><span class="attribute">spark</span>-streaming-kafka-<span class="number">0</span>-<span class="number">10</span>_2.<span class="number">12</span>&lt;/artifactId&gt;</span><br><span class="line">  <span class="section">&lt;version&gt;</span><span class="attribute">3</span>.<span class="number">2</span>.<span class="number">1</span>&lt;/version&gt;</span><br><span class="line"><span class="section">&lt;/dependency&gt;</span></span><br></pre></td></tr></table></figure>

<p>使用：</p>
<figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-------------------------------kafka消费数据</span></span><br><span class="line"><span class="title">kafka</span>-console-consumer.sh \</span><br><span class="line"><span class="comment">--bootstrap-server bigdata3:9092,bigdata4:9092,bigdata5:9092 \</span></span><br><span class="line"><span class="comment">--topic dl2262 \</span></span><br><span class="line"><span class="comment">--from-beginning </span></span><br><span class="line"><span class="comment">-----------------------------kafka创建topic</span></span><br><span class="line"><span class="title">kafka</span>-topics.sh \</span><br><span class="line"><span class="comment">--create \</span></span><br><span class="line"><span class="comment">--zookeeper bigdata3:2181,bigdata4:2181,bigdata5:2181/kafka \</span></span><br><span class="line"><span class="comment">--topic dl2262 \</span></span><br><span class="line"><span class="comment">--partitions 6 \</span></span><br><span class="line"><span class="comment">--replication-factor 3</span></span><br><span class="line"><span class="comment">-------------------------------kafka生产数据</span></span><br><span class="line"><span class="title">kafka</span>-console-producer.sh \</span><br><span class="line"><span class="comment">--broker-list bigdata3:9092,bigdata4:9092,bigdata5:9092 \</span></span><br><span class="line"><span class="comment">--topic dl2262</span></span><br><span class="line"><span class="comment">-------------------------------kafka查看topic</span></span><br><span class="line"><span class="title">kafka</span>-topics.sh \</span><br><span class="line"><span class="comment">--describe \</span></span><br><span class="line"><span class="comment">--zookeeper bigdata3:2181,bigdata4:2181,bigdata5:2181/kafka \</span></span><br><span class="line"><span class="comment">--topic dl2262 </span></span><br><span class="line"><span class="comment">-----------------------------代码</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession</span><br><span class="line"><span class="keyword">import</span> tool._</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe</span><br><span class="line"><span class="title">object</span> sparkstreaming2 &#123;</span><br><span class="line"></span><br><span class="line">  private val streamingcontext = new streamingcontext</span><br><span class="line"></span><br><span class="line">  def main(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    val kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="string">&quot;bigdata3:9092,bigdata4:9092,bigdata5:9092&quot;</span>, // kafka地址</span><br><span class="line">      <span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>], // 反序列化</span><br><span class="line">      <span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>], // 反序列化</span><br><span class="line">      <span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;dl2262-1&quot;</span>, // 指定消费者组</span><br><span class="line">      <span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;latest&quot;</span>, // 从什么地方开始消费</span><br><span class="line">      <span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (false: java.lang.<span class="type">Boolean</span>) // offset的提交 是不是自动提交</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">     val example = streamingcontext.getstreamcotext()</span><br><span class="line">    val topics = <span class="type">Array</span>(<span class="string">&quot;dl2262&quot;</span>)</span><br><span class="line">    val stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      example,</span><br><span class="line">      <span class="type">PreferConsistent</span>, // 数据存储策略 <span class="type">Kafka</span>数据均匀分在各个exector上，一共有三种</span><br><span class="line">      <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams) // 固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.map(record =&gt; (record.value)).print()</span><br><span class="line"></span><br><span class="line">    example.start()</span><br><span class="line">    example.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述用的是新版本的kafka的api</p>
<p>消费kafka数据做wc 将结果-》mysql</p>
<figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">package sparkstreaming</span><br><span class="line"><span class="keyword">import</span> org.<span class="property">apache</span>.<span class="property">spark</span>.<span class="property">sql</span>.<span class="property">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> tool.<span class="property">_</span></span><br><span class="line"><span class="keyword">import</span> org.<span class="property">apache</span>.<span class="property">kafka</span>.<span class="property">clients</span>.<span class="property">consumer</span>.<span class="property">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.<span class="property">apache</span>.<span class="property">kafka</span>.<span class="property">common</span>.<span class="property">serialization</span>.<span class="property">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.<span class="property">apache</span>.<span class="property">spark</span>.<span class="property">streaming</span>.<span class="property">kafka010</span>.<span class="property">_</span></span><br><span class="line"><span class="keyword">import</span> org.<span class="property">apache</span>.<span class="property">spark</span>.<span class="property">streaming</span>.<span class="property">kafka010</span>.<span class="property">LocationStrategies</span>.<span class="property">PreferConsistent</span></span><br><span class="line"><span class="keyword">import</span> org.<span class="property">apache</span>.<span class="property">spark</span>.<span class="property">streaming</span>.<span class="property">kafka010</span>.<span class="property">ConsumerStrategies</span>.<span class="property">Subscribe</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">object</span> sparkstreaming2 &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> val streamingcontext = <span class="keyword">new</span> streamingcontext</span><br><span class="line">  <span class="keyword">private</span> val savefile = <span class="keyword">new</span> savefile</span><br><span class="line">  def <span class="title function_">main</span>(<span class="attr">args</span>: <span class="title class_">Array</span>[<span class="title class_">String</span>]): <span class="title class_">Unit</span> = &#123;</span><br><span class="line">    val kafkaParams = <span class="title class_">Map</span>[<span class="title class_">String</span>, <span class="title class_">Object</span>](</span><br><span class="line">      <span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="string">&quot;bigdata3:9092,bigdata4:9092,bigdata5:9092&quot;</span>, <span class="comment">// kafka地址</span></span><br><span class="line">      <span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="title class_">StringDeserializer</span>], <span class="comment">// 反序列化</span></span><br><span class="line">      <span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="title class_">StringDeserializer</span>], <span class="comment">// 反序列化</span></span><br><span class="line">      <span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;dl2262-1&quot;</span>, <span class="comment">// 指定消费者组</span></span><br><span class="line">      <span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;latest&quot;</span>, <span class="comment">// 从什么地方开始消费</span></span><br><span class="line">      <span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="attr">false</span>: java.<span class="property">lang</span>.<span class="property">Boolean</span>) <span class="comment">// offset的提交 是不是自动提交</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val example = streamingcontext.<span class="title function_">getstreamcotext</span>()</span><br><span class="line">    val topics = <span class="title class_">Array</span>(<span class="string">&quot;dl2262&quot;</span>)</span><br><span class="line">    val stream = <span class="title class_">KafkaUtils</span>.<span class="property">createDirectStream</span>[<span class="title class_">String</span>, <span class="title class_">String</span>](</span><br><span class="line">      example,</span><br><span class="line">      <span class="title class_">PreferConsistent</span>, <span class="comment">// 数据存储策略 Kafka数据均匀分在各个exector上，一共有三种</span></span><br><span class="line">      <span class="title class_">Subscribe</span>[<span class="title class_">String</span>, <span class="title class_">String</span>](topics, kafkaParams) <span class="comment">// 固定写法</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.<span class="title function_">map</span>(<span class="function"><span class="params">record</span> =&gt;</span> (record.<span class="property">value</span>)).<span class="title function_">print</span>()</span><br><span class="line"></span><br><span class="line">    val value = stream.<span class="title function_">map</span>(<span class="function"><span class="params">record</span> =&gt;</span> (record.<span class="property">value</span>)).<span class="title function_">flatMap</span>(<span class="function"><span class="params">x</span> =&gt;</span> &#123;</span><br><span class="line">      x.<span class="title function_">split</span>(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    &#125;).<span class="title function_">map</span>(<span class="function"><span class="params">word</span> =&gt;</span> &#123;</span><br><span class="line">      (word, <span class="number">1</span>)</span><br><span class="line">    &#125;).<span class="title function_">reduceByKey</span>(_ + _)</span><br><span class="line"></span><br><span class="line">    value.<span class="title function_">foreachRDD</span>(<span class="function"><span class="params">rdd</span>=&gt;</span>&#123;</span><br><span class="line">      val spark = <span class="title class_">SparkSession</span>.<span class="title function_">builder</span>().<span class="title function_">config</span>(rdd.<span class="property">sparkContext</span>.<span class="property">getConf</span>).<span class="title function_">getOrCreate</span>()</span><br><span class="line">      <span class="keyword">import</span> spark.<span class="property">implicits</span>.<span class="property">_</span></span><br><span class="line">      val wordsDataFrame = rdd.<span class="title function_">toDF</span>(<span class="string">&quot;word&quot;</span>,<span class="string">&quot;cnt&quot;</span>)</span><br><span class="line">      val <span class="attr">srray</span>:<span class="title class_">Array</span>[<span class="title class_">String</span>] = <span class="title class_">Array</span>(<span class="string">&quot;append&quot;</span>,<span class="string">&quot;jdbc:mysql://bigdata2:3306/bigdata&quot;</span>,<span class="string">&quot;root&quot;</span>,<span class="string">&quot;liuzihan010616&quot;</span>,<span class="string">&quot;wc&quot;</span>,<span class="string">&quot;word&quot;</span>)</span><br><span class="line">      savefile.<span class="title function_">savetojdbc</span>(spark,wordsDataFrame,srray)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    example.<span class="title function_">start</span>()</span><br><span class="line">    example.<span class="title function_">awaitTermination</span>()</span><br><span class="line">  &#125;&#125;</span><br></pre></td></tr></table></figure>

<p>消费完之后如果重启从上次挂掉的位置继续消费</p>
<p>要设置</p>
<ul>
<li>enable.auto.commit</li>
<li>auto.offset.reset</li>
</ul>
<p>才可以从断掉的位置开始</p>
<p>解决：</p>
<ul>
<li>获取kafka offset</li>
<li>提交kafka offset</li>
</ul>
<p>获取kafka的offset信息</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">stream<span class="selector-class">.foreachRDD</span> &#123; rdd =&gt;</span><br><span class="line">  val offsetRanges = rdd<span class="selector-class">.asInstanceOf</span><span class="selector-attr">[HasOffsetRanges]</span><span class="selector-class">.offsetRanges</span></span><br><span class="line">  rdd<span class="selector-class">.foreachPartition</span> &#123; iter =&gt;</span><br><span class="line">    val o: OffsetRange = <span class="built_in">offsetRanges</span>(TaskContext<span class="selector-class">.get</span>.partitionId)</span><br><span class="line">    <span class="built_in">println</span>(rdd<span class="selector-class">.partitions</span>.size)</span><br><span class="line">    <span class="built_in">println</span>(s<span class="string">&quot;$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>关于offset信息的解释：只要最后两列数据一样就代表这个topic里的数据都消费完了</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">6</span><br><span class="line">dl2262 5 19 19</span><br><span class="line">dl2262 4 18 18</span><br><span class="line">dl2262 0 19 19</span><br><span class="line">dl2262 2 77 77</span><br><span class="line">dl2262 1 19 19</span><br><span class="line"><span class="section">dl2262 3 46 47</span></span><br><span class="line"><span class="section">-------------------------------------------</span></span><br><span class="line"><span class="section">Time: 1673939535000 ms</span></span><br><span class="line"><span class="section">-------------------------------------------</span></span><br><span class="line">bidhashdas</span><br><span class="line"></span><br><span class="line">6</span><br><span class="line">dl2262 4 18 18</span><br><span class="line">dl2262 3 47 47</span><br><span class="line">dl2262 0 19 19</span><br><span class="line">dl2262 5 19 19</span><br><span class="line">dl2262 2 77 77</span><br><span class="line">dl2262 1 19 19</span><br></pre></td></tr></table></figure>

<p>注意：这些操作是获取到数据之后立刻这样做，就可以获得offset信息</p>
<p>其他对数据的进行操作，可以在这个里面做</p>
<p>如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">stream<span class="selector-class">.foreachRDD</span> &#123; rdd =&gt;</span><br><span class="line">  val offsetRanges = rdd<span class="selector-class">.asInstanceOf</span><span class="selector-attr">[HasOffsetRanges]</span><span class="selector-class">.offsetRanges</span></span><br><span class="line">  <span class="built_in">println</span>(rdd<span class="selector-class">.partitions</span>.size)</span><br><span class="line">  rdd<span class="selector-class">.foreachPartition</span> &#123; iter =&gt;</span><br><span class="line">    val o: OffsetRange = <span class="built_in">offsetRanges</span>(TaskContext<span class="selector-class">.get</span>.partitionId)</span><br><span class="line">    <span class="built_in">println</span>(s<span class="string">&quot;$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// wc</span></span><br><span class="line">  val spark = SparkSession<span class="selector-class">.builder</span>()<span class="selector-class">.config</span>(rdd<span class="selector-class">.sparkContext</span>.getConf)<span class="selector-class">.getOrCreate</span>()</span><br><span class="line">  import spark<span class="selector-class">.implicits</span>._</span><br><span class="line"></span><br><span class="line">  val wordsDataFrame = rdd<span class="selector-class">.map</span>(_.value)<span class="selector-class">.flatMap</span>(_<span class="selector-class">.split</span>(<span class="string">&quot;,&quot;</span>))<span class="selector-class">.map</span>((_, <span class="number">1</span>))<span class="selector-class">.reduceByKey</span>(_ + _)<span class="selector-class">.toDF</span>(<span class="string">&quot;word&quot;</span>,<span class="string">&quot;cnt&quot;</span>)</span><br><span class="line">  val srray:Array<span class="selector-attr">[String]</span> = <span class="built_in">Array</span>(<span class="string">&quot;append&quot;</span>,<span class="string">&quot;jdbc:mysql://bigdata2:3306/bigdata&quot;</span>,<span class="string">&quot;root&quot;</span>,<span class="string">&quot;liuzihan010616&quot;</span>,<span class="string">&quot;wc&quot;</span>,<span class="string">&quot;word&quot;</span>)</span><br><span class="line">  savefile<span class="selector-class">.savetojdbc</span>(spark,wordsDataFrame,srray)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 存储offset</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 提交offset</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>接下来，我们要进行提交offset</p>
<p>在提交offset之前</p>
<p>我们要存储offset</p>
<p>spark流处理 默认的就是至少一次</p>
<p>存储offfset</p>
<ul>
<li>checkpoints 不能用</li>
<li>kafka本身 简单高效 -》消费语义-》至少一次&#x2F;最多一次 ：但是最多一次我们不用 -》因为不支持事务 -》 无法支持精准一次</li>
<li>可以使用支持事务的存储结构进行精准一次的交付语义</li>
</ul>
<p>kafka本身 ： 他存储的offset信息是存储在kafka的一共特殊的offset里比如_customer_offsets</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">stream.asInstanceOf<span class="literal">[C<span class="identifier">anCommitOffsets</span>]</span>.commit<span class="constructor">Async(<span class="params">offsetRanges</span>)</span></span><br><span class="line">-------------------------------------------整体代码</span><br><span class="line">package sparkstreaming</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import tool._</span><br><span class="line">import org.apache.kafka.clients.consumer.ConsumerRecord</span><br><span class="line">import org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line">import org.apache.spark.TaskContext</span><br><span class="line">import org.apache.spark.streaming.kafka010._</span><br><span class="line">import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent</span><br><span class="line">import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe</span><br><span class="line"></span><br><span class="line"><span class="keyword">object</span> sparkstreaming2 &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> streamingcontext = <span class="keyword">new</span> streamingcontext</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> savefile = <span class="keyword">new</span> savefile</span><br><span class="line">  def main(args: Array<span class="literal">[S<span class="identifier">tring</span>]</span>): Unit = &#123;</span><br><span class="line">    <span class="keyword">val</span> kafkaParams = Map<span class="literal">[S<span class="identifier">tring</span>, O<span class="identifier">bject</span>]</span>(</span><br><span class="line">      <span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="string">&quot;bigdata3:9092,bigdata4:9092,bigdata5:9092&quot;</span>, <span class="comment">// kafka地址</span></span><br><span class="line">      <span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf<span class="literal">[S<span class="identifier">tringDeserializer</span>]</span>, <span class="comment">// 反序列化</span></span><br><span class="line">      <span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf<span class="literal">[S<span class="identifier">tringDeserializer</span>]</span>, <span class="comment">// 反序列化</span></span><br><span class="line">      <span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;dl2262-1&quot;</span>, <span class="comment">// 指定消费者组</span></span><br><span class="line">      <span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;latest&quot;</span>, <span class="comment">// 从什么地方开始消费</span></span><br><span class="line">      <span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="literal">false</span>: java.lang.Boolean) <span class="comment">// offset的提交 是不是自动提交</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> example = streamingcontext.getstreamcotext<span class="literal">()</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="constructor">Array(<span class="string">&quot;dl2262&quot;</span>)</span></span><br><span class="line">    <span class="keyword">val</span> stream = <span class="module-access"><span class="module"><span class="identifier">KafkaUtils</span>.</span></span>createDirectStream<span class="literal">[S<span class="identifier">tring</span>, S<span class="identifier">tring</span>]</span>(</span><br><span class="line">      example,</span><br><span class="line">      PreferConsistent, <span class="comment">// 数据存储策略 Kafka数据均匀分在各个exector上，一共有三种</span></span><br><span class="line">      Subscribe<span class="literal">[S<span class="identifier">tring</span>, S<span class="identifier">tring</span>]</span>(topics, kafkaParams) <span class="comment">// 固定写法</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// 获取offset信息</span></span><br><span class="line">    stream.foreachRDD &#123;<span class="function"> <span class="params">rdd</span> =&gt;</span></span><br><span class="line">      <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf<span class="literal">[H<span class="identifier">asOffsetRanges</span>]</span>.offsetRanges</span><br><span class="line">      println(rdd.partitions.size)</span><br><span class="line">      rdd.foreachPartition &#123;<span class="function"> <span class="params">iter</span> =&gt;</span></span><br><span class="line">        <span class="keyword">val</span> o: OffsetRange = offset<span class="constructor">Ranges(TaskContext.<span class="params">get</span>.<span class="params">partitionId</span>)</span></span><br><span class="line">        println(s<span class="string">&quot;$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// wc</span></span><br><span class="line">      <span class="keyword">val</span> spark = <span class="module-access"><span class="module"><span class="identifier">SparkSession</span>.</span></span>builder<span class="literal">()</span>.config(rdd.sparkContext.getConf).get<span class="constructor">OrCreate()</span></span><br><span class="line">      import spark.implicits._</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> wordsDataFrame = rdd.map(<span class="module-access"><span class="module"><span class="identifier">_</span>.</span></span>value).flat<span class="constructor">Map(<span class="params">_</span>.<span class="params">split</span>(<span class="string">&quot;,&quot;</span>)</span>).map((_, <span class="number">1</span>)).reduce<span class="constructor">ByKey(<span class="params">_</span> + <span class="params">_</span>)</span>.<span class="keyword">to</span><span class="constructor">DF(<span class="string">&quot;word&quot;</span>,<span class="string">&quot;cnt&quot;</span>)</span></span><br><span class="line">      <span class="keyword">val</span> srray:Array<span class="literal">[S<span class="identifier">tring</span>]</span> = <span class="constructor">Array(<span class="string">&quot;append&quot;</span>,<span class="string">&quot;jdbc:mysql://bigdata2:3306/bigdata&quot;</span>,<span class="string">&quot;root&quot;</span>,<span class="string">&quot;liuzihan010616&quot;</span>,<span class="string">&quot;wc&quot;</span>,<span class="string">&quot;word&quot;</span>)</span></span><br><span class="line">      savefile.savetojdbc(spark,wordsDataFrame,srray)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 存储offset和提交offset</span></span><br><span class="line">      stream.asInstanceOf<span class="literal">[C<span class="identifier">anCommitOffsets</span>]</span>.commit<span class="constructor">Async(<span class="params">offsetRanges</span>)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    example.start<span class="literal">()</span></span><br><span class="line">    example.await<span class="constructor">Termination()</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其他数据源：官方的</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The details depend on your data store, but the general idea looks like this</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// begin from the offsets committed to the database</span></span><br><span class="line">val fromOffsets = selectOffsetsFromYourDatabase<span class="selector-class">.map</span> &#123; resultSet =&gt;</span><br><span class="line">  new <span class="built_in">TopicPartition</span>(resultSet<span class="selector-class">.string</span>(<span class="string">&quot;topic&quot;</span>), resultSet<span class="selector-class">.int</span>(<span class="string">&quot;partition&quot;</span>)) -&gt; resultSet<span class="selector-class">.long</span>(<span class="string">&quot;offset&quot;</span>)</span><br><span class="line">&#125;<span class="selector-class">.toMap</span></span><br><span class="line"></span><br><span class="line">val stream = KafkaUtils<span class="selector-class">.createDirectStream</span><span class="selector-attr">[String, String]</span>(</span><br><span class="line">  streamingContext,</span><br><span class="line">  PreferConsistent,</span><br><span class="line">  Assign<span class="selector-attr">[String, String]</span>(fromOffsets<span class="selector-class">.keys</span><span class="selector-class">.toList</span>, kafkaParams, fromOffsets)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">stream<span class="selector-class">.foreachRDD</span> &#123; rdd =&gt;</span><br><span class="line">  val offsetRanges = rdd<span class="selector-class">.asInstanceOf</span><span class="selector-attr">[HasOffsetRanges]</span><span class="selector-class">.offsetRanges</span></span><br><span class="line"></span><br><span class="line">  val results = <span class="built_in">yourCalculation</span>(rdd)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// begin your transaction</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// update results</span></span><br><span class="line">  <span class="comment">// update offsets where the end of existing offsets matches the beginning of this batch of offsets</span></span><br><span class="line">  <span class="comment">// assert that offsets were updated correctly</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// end your transaction</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>SSL的一些配置</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val kafkaParams = Map[<span class="type">String</span>, Object](</span><br><span class="line">  <span class="comment">// the usual params, make sure to change the port in bootstrap.servers if 9092 is not TLS</span></span><br><span class="line">  <span class="string">&quot;security.protocol&quot;</span> <span class="punctuation">-&gt;</span> <span class="string">&quot;SSL&quot;</span>,</span><br><span class="line">  <span class="string">&quot;ssl.truststore.location&quot;</span> <span class="punctuation">-&gt;</span> <span class="string">&quot;/some-directory/kafka.client.truststore.jks&quot;</span>,</span><br><span class="line">  <span class="string">&quot;ssl.truststore.password&quot;</span> <span class="punctuation">-&gt;</span> <span class="string">&quot;test1234&quot;</span>,</span><br><span class="line">  <span class="string">&quot;ssl.keystore.location&quot;</span> <span class="punctuation">-&gt;</span> <span class="string">&quot;/some-directory/kafka.client.keystore.jks&quot;</span>,</span><br><span class="line">  <span class="string">&quot;ssl.keystore.password&quot;</span> <span class="punctuation">-&gt;</span> <span class="string">&quot;test1234&quot;</span>,</span><br><span class="line">  <span class="string">&quot;ssl.key.password&quot;</span> <span class="punctuation">-&gt;</span> <span class="string">&quot;test1234&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      
   <div>
     <div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

   </div>
     
        <div class="reward-container">
  <div>你们的鼓励是对我最大的支持</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="liu zihang 微信支付">
        <p>微信支付</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>liu zihang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://zihang.fun/2023/01/16/1-16/" title="sparksql-4">http://zihang.fun/2023/01/16/1-16/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/images/wechat_channel.jpg">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>

     
    
      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/spark/" rel="tag"># spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/01/13/1-13/" rel="prev" title="sparksql-3">
      <i class="fa fa-chevron-left"></i> sparksql-3
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/01/25/1-25/" rel="next" title="flink">
      flink <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81NzQzNy8zMzkwMQ=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#sparkstreaming"><span class="nav-number">1.</span> <span class="nav-text">sparkstreaming</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E5%A4%84%E7%90%86-%EF%BC%9A-%E5%AE%9E%E6%97%B6"><span class="nav-number">1.1.</span> <span class="nav-text">流处理 ： 实时</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%B9%E5%A4%84%E7%90%86-%EF%BC%9A-%E7%A6%BB%E7%BA%BF"><span class="nav-number">1.2.</span> <span class="nav-text">批处理 ： 离线</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B"><span class="nav-number">1.3.</span> <span class="nav-text">技术选型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E6%80%A7"><span class="nav-number">1.4.</span> <span class="nav-text">特性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.5.</span> <span class="nav-text">简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%BA%90-%EF%BC%9A"><span class="nav-number">1.5.1.</span> <span class="nav-text">数据源 ：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B%EF%BC%9Akafka%E6%95%B0%E6%8D%AE%E5%A4%AA%E5%A4%A7%EF%BC%8C%E5%AF%BC%E8%87%B4sp%E7%A8%8B%E5%BA%8F%E4%B8%80%E7%9B%B4%E5%A4%84%E7%90%86%E4%B8%8D%E8%BF%87%E6%9D%A5%EF%BC%8C%E4%B8%80%E4%B8%AA%E5%87%BA%E4%B8%8D%E6%9D%A5%E6%8A%A5%E8%A1%A8-x3D-gt-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="nav-number">1.5.2.</span> <span class="nav-text">数据积压：kafka数据太大，导致sp程序一直处理不过来，一个出不来报表 &#x3D;&gt;解决方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sparkstreaming%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6"><span class="nav-number">1.5.3.</span> <span class="nav-text">sparkstreaming运行机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sparkstreaming-gt-kafka-%EF%BC%9A"><span class="nav-number">1.5.4.</span> <span class="nav-text">sparkstreaming -&gt; kafka ：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sparkstreaming%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%EF%BC%9ADstream"><span class="nav-number">1.5.5.</span> <span class="nav-text">sparkstreaming编程模型：Dstream</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#idea%E5%BC%80%E5%8F%91%E5%85%88%E9%85%8D%E7%BD%AEpom%E6%96%87%E4%BB%B6"><span class="nav-number">1.5.6.</span> <span class="nav-text">idea开发先配置pom文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#idea%E4%BB%A3%E7%A0%81"><span class="nav-number">1.5.7.</span> <span class="nav-text">idea代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8C%E6%B5%81join"><span class="nav-number">1.5.8.</span> <span class="nav-text">双流join</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9E%84%E5%BB%BADstream"><span class="nav-number">1.6.</span> <span class="nav-text">构建Dstream</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#inputstrteam"><span class="nav-number">1.6.1.</span> <span class="nav-text">inputstrteam</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#receiver"><span class="nav-number">1.6.2.</span> <span class="nav-text">receiver</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dstream%E7%AE%97%E5%AD%90"><span class="nav-number">1.7.</span> <span class="nav-text">Dstream算子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8A%B6%E6%80%81"><span class="nav-number">1.8.</span> <span class="nav-text">状态</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#checkpoint%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%9C%E8%A5%BF"><span class="nav-number">1.9.</span> <span class="nav-text">checkpoint的存储东西</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9"><span class="nav-number">1.9.1.</span> <span class="nav-text">缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9B%B8%E5%90%8C%E5%8A%9F%E8%83%BD%EF%BC%9F"><span class="nav-number">1.10.</span> <span class="nav-text">如何实现相同功能？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8checkpoint"><span class="nav-number">1.11.</span> <span class="nav-text">使用checkpoint</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E6%8A%8A%E5%A4%84%E7%90%86%E5%A5%BD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%88%B0%E5%A4%96%E9%83%A8"><span class="nav-number">1.12.</span> <span class="nav-text">如何把处理好的数据存储到外部</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transform"><span class="nav-number">1.13.</span> <span class="nav-text">transform</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DStream%E5%92%8CRDD%E4%B9%8B%E9%97%B4%E4%BA%A4%E4%BA%92%E7%9A%84%E7%AE%97%E5%AD%90"><span class="nav-number">1.13.1.</span> <span class="nav-text">DStream和RDD之间交互的算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%A6%82%E4%B8%8B"><span class="nav-number">1.13.2.</span> <span class="nav-text">数据如下</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A6%BB%E7%BA%BF"><span class="nav-number">1.13.3.</span> <span class="nav-text">离线:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E6%97%B6%EF%BC%9A"><span class="nav-number">1.13.4.</span> <span class="nav-text">实时：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sparkstreaming%E5%92%8Ckafka%E6%95%B4%E5%90%88"><span class="nav-number">1.14.</span> <span class="nav-text">sparkstreaming和kafka整合</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="liu zihang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">liu zihang</p>
  <div class="site-description" itemprop="description">只有努力不会辜负你</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">66</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      链接网站
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://baidu.com/" title="https:&#x2F;&#x2F;baidu.com" rel="noopener" target="_blank">百度</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://fishc.com.cn/" title="https:&#x2F;&#x2F;fishc.com.cn" rel="noopener" target="_blank">鱼C论坛</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">liu zihang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共232.4k字</span>
</div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'http://zihang.fun/2023/01/16/1-16/',]
      });
      });
  </script>

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
